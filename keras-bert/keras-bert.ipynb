{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import tensorflow_hub as hub\n",
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "from bert.tokenization import FullTokenizer\n",
    "from tqdm import tqdm_notebook\n",
    "from tensorflow.keras import backend as K\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "\n",
    "# Initialize session\n",
    "sess = tf.Session()\n",
    "\n",
    "# Params for bert model and tokenization\n",
    "bert_path = \"https://tfhub.dev/google/bert_uncased_L-12_H-768_A-12/1\"\n",
    "max_seq_length = 256"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data\n",
    "\n",
    "First, we load the sample data IMDB data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load all files from a directory in a DataFrame.\n",
    "def load_directory_data(directory):\n",
    "    data = {}\n",
    "    data[\"sentence\"] = []\n",
    "    data[\"sentiment\"] = []\n",
    "    for file_path in os.listdir(directory):\n",
    "        with tf.gfile.GFile(os.path.join(directory, file_path), \"r\") as f:\n",
    "            data[\"sentence\"].append(f.read())\n",
    "            data[\"sentiment\"].append(re.match(\"\\d+_(\\d+)\\.txt\", file_path).group(1))\n",
    "    return pd.DataFrame.from_dict(data)\n",
    "\n",
    "# Merge positive and negative examples, add a polarity column and shuffle.\n",
    "def load_dataset(directory):\n",
    "    pos_df = load_directory_data(os.path.join(directory, \"pos\"))\n",
    "    neg_df = load_directory_data(os.path.join(directory, \"neg\"))\n",
    "    pos_df[\"polarity\"] = 1\n",
    "    neg_df[\"polarity\"] = 0\n",
    "    return pd.concat([pos_df, neg_df]).sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "# Download and process the dataset files.\n",
    "def download_and_load_datasets(force_download=False):\n",
    "    dataset = tf.keras.utils.get_file(\n",
    "        fname=\"aclImdb.tar.gz\", \n",
    "        origin=\"http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\", \n",
    "        extract=True)\n",
    "\n",
    "    train_df = load_dataset(os.path.join(os.path.dirname(dataset), \n",
    "                                       \"aclImdb\", \"train\"))\n",
    "    test_df = load_dataset(os.path.join(os.path.dirname(dataset), \n",
    "                                      \"aclImdb\", \"test\"))\n",
    "\n",
    "    return train_df, test_df\n",
    "\n",
    "# Reduce logging output.\n",
    "tf.logging.set_verbosity(tf.logging.ERROR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, test_df = download_and_load_datasets()\n",
    "train_df.head()\n",
    "# Create datasets (Only take up to max_seq_length words for memory)\n",
    "train_text = train_df['sentence'].tolist()\n",
    "train_text = [' '.join(t.split()[0:max_seq_length]) for t in train_text]\n",
    "train_text = np.array(train_text, dtype=object)[:, np.newaxis]\n",
    "train_label = train_df['polarity'].tolist()\n",
    "\n",
    "test_text = test_df['sentence'].tolist()\n",
    "test_text = [' '.join(t.split()[0:max_seq_length]) for t in test_text]\n",
    "test_text = np.array(test_text, dtype=object)[:, np.newaxis]\n",
    "test_label = test_df['polarity'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     org_dataset  id                                                org  \\\n",
      "3096   political   3  I do n't take the views that the only alternat...   \n",
      "3097   political   4  Oh , the problems are great . In Cuba , in the...   \n",
      "3098   political   5  Major policy on issues such as Cuban security ...   \n",
      "3101   political   8  He must never be rash or impulsive . That 's w...   \n",
      "3106   political  13  Talk about the situation , for example , in Cu...   \n",
      "\n",
      "     org_stance                                           response  \\\n",
      "3096    unknown  We could have tried to inject ourselves into t...   \n",
      "3097    unknown  And the young men and women , those who are st...   \n",
      "3098    unknown  I say in that connection that with all the cri...   \n",
      "3101    unknown  These people will feel they have no chance , a...   \n",
      "3106    unknown  Secondly , also before the threat reached that...   \n",
      "\n",
      "     response_stance    label topic  \n",
      "3096         unknown  support  cuba  \n",
      "3097         unknown  support  cuba  \n",
      "3098         unknown   attack  cuba  \n",
      "3101         unknown   attack  cuba  \n",
      "3106         unknown   attack  cuba  \n"
     ]
    }
   ],
   "source": [
    "def load_local_data(filename, data='node'):\n",
    "    df = pd.read_csv(filename, sep='\\t')\n",
    "    # Split in Training and Validation data\n",
    "    if data == 'node':\n",
    "        # Training data: NoDe debatepedia all versions without neutral label\n",
    "        # Validation data: NoDe procon\n",
    "        dataset = df.loc[~df['org_dataset'].isin(['political'])].loc[df['label'].isin(['attack', 'support'])]\n",
    "        data_train = dataset.loc[~dataset['org_dataset'].isin(['procon'])]\n",
    "        data_val = dataset.loc[dataset['org_dataset'].isin(['procon'])]\n",
    "    elif data == 'political':\n",
    "        dataset = df.loc[df['org_dataset'].isin(['political'])].loc[df['label'].isin(['attack', 'support'])]\n",
    "        data_train = dataset.iloc[:-200]\n",
    "        data_val = dataset.iloc[-200:]\n",
    "    else:\n",
    "        print('Invalid dataset')\n",
    "        sys.exit(-1)\n",
    "    return data_train, data_val\n",
    "\n",
    "# Load local data\n",
    "train_df, test_df = load_local_data('../complete_data.tsv', 'political')\n",
    "print(train_df.head())\n",
    "\n",
    "# Create datasets (Only take up to max_seq_length words for memory)\n",
    "convert_dict = {\"attack\": 0, \"support\": 1, \"unrelated\": 2}\n",
    "\n",
    "train_org = train_df['org'].tolist()\n",
    "train_org = [' '.join(t.split()[0:max_seq_length]) for t in train_org]\n",
    "train_org = np.array(train_org, dtype=object)[:, np.newaxis]\n",
    "train_text = train_df['response'].tolist()\n",
    "train_text = [' '.join(t.split()[0:max_seq_length]) for t in train_text]\n",
    "train_text = np.array(train_text, dtype=object)[:, np.newaxis]\n",
    "train_label =  np.array([convert_dict[label] for label in train_df['label']])\n",
    "\n",
    "test_org = test_df['org'].tolist()\n",
    "test_org = [' '.join(t.split()[0:max_seq_length]) for t in test_org]\n",
    "test_org = np.array(test_org, dtype=object)[:, np.newaxis]\n",
    "test_text = test_df['response'].tolist()\n",
    "test_text = [' '.join(t.split()[0:max_seq_length]) for t in test_text]\n",
    "test_text = np.array(test_text, dtype=object)[:, np.newaxis]\n",
    "test_label = np.array([convert_dict[label] for label in test_df['label']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenize\n",
    "\n",
    "Next, tokenize our text to create `input_ids`, `input_masks`, and `segment_ids`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PaddingInputExample(object):\n",
    "    \"\"\"Fake example so the num input examples is a multiple of the batch size.\n",
    "  When running eval/predict on the TPU, we need to pad the number of examples\n",
    "  to be a multiple of the batch size, because the TPU requires a fixed batch\n",
    "  size. The alternative is to drop the last batch, which is bad because it means\n",
    "  the entire output data won't be generated.\n",
    "  We use this class instead of `None` because treating `None` as padding\n",
    "  battches could cause silent errors.\n",
    "  \"\"\"\n",
    "\n",
    "class InputExample(object):\n",
    "    \"\"\"A single training/test example for simple sequence classification.\"\"\"\n",
    "\n",
    "    def __init__(self, guid, text_a, text_b=None, label=None):\n",
    "        \"\"\"Constructs a InputExample.\n",
    "    Args:\n",
    "      guid: Unique id for the example.\n",
    "      text_a: string. The untokenized text of the first sequence. For single\n",
    "        sequence tasks, only this sequence must be specified.\n",
    "      text_b: (Optional) string. The untokenized text of the second sequence.\n",
    "        Only must be specified for sequence pair tasks.\n",
    "      label: (Optional) string. The label of the example. This should be\n",
    "        specified for train and dev examples, but not for test examples.\n",
    "    \"\"\"\n",
    "        self.guid = guid\n",
    "        self.text_a = text_a\n",
    "        self.text_b = text_b\n",
    "        self.label = label\n",
    "\n",
    "def create_tokenizer_from_hub_module():\n",
    "    \"\"\"Get the vocab file and casing info from the Hub module.\"\"\"\n",
    "    bert_module =  hub.Module(bert_path)\n",
    "    tokenization_info = bert_module(signature=\"tokenization_info\", as_dict=True)\n",
    "    vocab_file, do_lower_case = sess.run(\n",
    "        [\n",
    "            tokenization_info[\"vocab_file\"],\n",
    "            tokenization_info[\"do_lower_case\"],\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    return FullTokenizer(vocab_file=vocab_file, do_lower_case=do_lower_case)\n",
    "\n",
    "def convert_single_example(tokenizer, example, max_seq_length=256):\n",
    "    \"\"\"Converts a single `InputExample` into a single `InputFeatures`.\"\"\"\n",
    "\n",
    "    if isinstance(example, PaddingInputExample):\n",
    "        input_ids = [0] * max_seq_length\n",
    "        input_mask = [0] * max_seq_length\n",
    "        segment_ids = [0] * max_seq_length\n",
    "        label = 0\n",
    "        return input_ids, input_mask, segment_ids, label\n",
    "\n",
    "    tokens_a = tokenizer.tokenize(example.text_a)\n",
    "    if len(tokens_a) > max_seq_length - 2:\n",
    "        tokens_a = tokens_a[0 : (max_seq_length - 2)]\n",
    "\n",
    "    tokens = []\n",
    "    segment_ids = []\n",
    "    tokens.append(\"[CLS]\")\n",
    "    segment_ids.append(0)\n",
    "    for token in tokens_a:\n",
    "        tokens.append(token)\n",
    "        segment_ids.append(0)\n",
    "    tokens.append(\"[SEP]\")\n",
    "    segment_ids.append(0)\n",
    "\n",
    "    input_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "\n",
    "    # The mask has 1 for real tokens and 0 for padding tokens. Only real\n",
    "    # tokens are attended to.\n",
    "    input_mask = [1] * len(input_ids)\n",
    "\n",
    "    # Zero-pad up to the sequence length.\n",
    "    while len(input_ids) < max_seq_length:\n",
    "        input_ids.append(0)\n",
    "        input_mask.append(0)\n",
    "        segment_ids.append(0)\n",
    "\n",
    "    assert len(input_ids) == max_seq_length\n",
    "    assert len(input_mask) == max_seq_length\n",
    "    assert len(segment_ids) == max_seq_length\n",
    "\n",
    "    return input_ids, input_mask, segment_ids, example.label\n",
    "\n",
    "def convert_examples_to_features(tokenizer, examples, max_seq_length=256):\n",
    "    \"\"\"Convert a set of `InputExample`s to a list of `InputFeatures`.\"\"\"\n",
    "\n",
    "    input_ids, input_masks, segment_ids, labels = [], [], [], []\n",
    "    for example in tqdm_notebook(examples, desc=\"Converting examples to features\"):\n",
    "        input_id, input_mask, segment_id, label = convert_single_example(\n",
    "            tokenizer, example, max_seq_length\n",
    "        )\n",
    "        input_ids.append(input_id)\n",
    "        input_masks.append(input_mask)\n",
    "        segment_ids.append(segment_id)\n",
    "        labels.append(label)\n",
    "    return (\n",
    "        np.array(input_ids),\n",
    "        np.array(input_masks),\n",
    "        np.array(segment_ids),\n",
    "        np.array(labels).reshape(-1, 1),\n",
    "    )\n",
    "\n",
    "def convert_text_to_examples(texts, labels):\n",
    "    \"\"\"Create InputExamples\"\"\"\n",
    "    InputExamples = []\n",
    "    for text, label in zip(texts, labels):\n",
    "        InputExamples.append(\n",
    "            InputExample(guid=None, text_a=\" \".join(text), text_b=None, label=label)\n",
    "        )\n",
    "    return InputExamples\n",
    "\n",
    "def convert_posts_to_examples(texts_a, texts_b, labels):\n",
    "    \"\"\"Create InputExamples\"\"\"\n",
    "    InputExamples = []\n",
    "    for text_a, text_b, label in zip(texts_a, texts_b, labels):\n",
    "        InputExamples.append(\n",
    "            InputExample(guid=None, text_a=\" \".join(text_a), text_b=\" \".join(text_b), label=label)\n",
    "        )\n",
    "    return InputExamples\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b0116a2b70f343a9a06122a74b325251",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Converting examples to features', max=534, style=ProgressStyl…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4338bb86842949708fe1a12e712950d5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Converting examples to features', max=200, style=ProgressStyl…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Instantiate tokenizer\n",
    "tokenizer = create_tokenizer_from_hub_module()\n",
    "\n",
    "# Convert data to InputExample format\n",
    "#train_examples = convert_text_to_examples(train_text, train_label)\n",
    "#test_examples = convert_text_to_examples(test_text, test_label)\n",
    "\n",
    "train_examples = convert_posts_to_examples(train_org, train_text, train_label)\n",
    "test_examples = convert_posts_to_examples(test_org, test_text, test_label)\n",
    "\n",
    "# Convert to features\n",
    "(train_input_ids, train_input_masks, train_segment_ids, train_labels \n",
    ") = convert_examples_to_features(tokenizer, train_examples, max_seq_length=max_seq_length)\n",
    "(test_input_ids, test_input_masks, test_segment_ids, test_labels\n",
    ") = convert_examples_to_features(tokenizer, test_examples, max_seq_length=max_seq_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertLayer(tf.layers.Layer):\n",
    "    def __init__(self, n_fine_tune_layers=10, **kwargs):\n",
    "        self.n_fine_tune_layers = n_fine_tune_layers\n",
    "        self.trainable = True\n",
    "        self.output_size = 768\n",
    "        super(BertLayer, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.bert = hub.Module(\n",
    "            bert_path,\n",
    "            trainable=self.trainable,\n",
    "            name=\"{}_module\".format(self.name)\n",
    "        )\n",
    "\n",
    "        trainable_vars = self.bert.variables\n",
    "\n",
    "        # Remove unused layers\n",
    "        trainable_vars = [var for var in trainable_vars if not \"/cls/\" in var.name]\n",
    "\n",
    "        # Select how many layers to fine tune\n",
    "        trainable_vars = trainable_vars[-self.n_fine_tune_layers :]\n",
    "\n",
    "        # Add to trainable weights\n",
    "        for var in trainable_vars:\n",
    "            self._trainable_weights.append(var)\n",
    "            \n",
    "        for var in self.bert.variables:\n",
    "            if var not in self._trainable_weights:\n",
    "                self._non_trainable_weights.append(var)\n",
    "\n",
    "        super(BertLayer, self).build(input_shape)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        inputs = [K.cast(x, dtype=\"int32\") for x in inputs]\n",
    "        input_ids, input_mask, segment_ids = inputs\n",
    "        bert_inputs = dict(\n",
    "            input_ids=input_ids, input_mask=input_mask, segment_ids=segment_ids\n",
    "        )\n",
    "        result = self.bert(inputs=bert_inputs, signature=\"tokens\", as_dict=True)[\n",
    "            \"pooled_output\"\n",
    "        ]\n",
    "        return result\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return (input_shape[0], self.output_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build model\n",
    "def build_model(max_seq_length): \n",
    "    in_id = tf.keras.layers.Input(shape=(max_seq_length,), name=\"input_ids\")\n",
    "    in_mask = tf.keras.layers.Input(shape=(max_seq_length,), name=\"input_masks\")\n",
    "    in_segment = tf.keras.layers.Input(shape=(max_seq_length,), name=\"segment_ids\")\n",
    "    bert_inputs = [in_id, in_mask, in_segment]\n",
    "    \n",
    "    bert_output = BertLayer(n_fine_tune_layers=10)(bert_inputs)\n",
    "    dense = tf.keras.layers.Dense(256, activation='relu')(bert_output)\n",
    "    pred = tf.keras.layers.Dense(1, activation='sigmoid')(dense)\n",
    "    \n",
    "    model = tf.keras.models.Model(inputs=bert_inputs, outputs=pred)\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    model.summary()\n",
    "    \n",
    "    return model\n",
    "\n",
    "def initialize_vars(sess):\n",
    "    sess.run(tf.local_variables_initializer())\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    sess.run(tf.tables_initializer())\n",
    "    K.set_session(sess)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_ids (InputLayer)          (None, 256)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_masks (InputLayer)        (None, 256)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "segment_ids (InputLayer)        (None, 256)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "bert_layer_18 (BertLayer)       (None, 768)          110104890   input_ids[0][0]                  \n",
      "                                                                 input_masks[0][0]                \n",
      "                                                                 segment_ids[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dense_34 (Dense)                (None, 256)          196864      bert_layer_18[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_35 (Dense)                (None, 1)            257         dense_34[0][0]                   \n",
      "==================================================================================================\n",
      "Total params: 110,302,011\n",
      "Trainable params: 6,102,273\n",
      "Non-trainable params: 104,199,738\n",
      "__________________________________________________________________________________________________\n",
      "Train on 534 samples, validate on 200 samples\n",
      "Epoch 1/10\n",
      "534/534 [==============================] - ETA: 28:23 - loss: 0.7431 - acc: 0.50 - ETA: 13:50 - loss: 0.7911 - acc: 0.50 - ETA: 8:59 - loss: 1.2205 - acc: 0.5000 - ETA: 6:34 - loss: 1.1100 - acc: 0.484 - ETA: 5:06 - loss: 1.0718 - acc: 0.500 - ETA: 4:07 - loss: 1.0475 - acc: 0.479 - ETA: 3:26 - loss: 0.9946 - acc: 0.508 - ETA: 2:54 - loss: 0.9166 - acc: 0.554 - ETA: 2:30 - loss: 0.9240 - acc: 0.562 - ETA: 2:10 - loss: 0.9119 - acc: 0.568 - ETA: 1:54 - loss: 0.8956 - acc: 0.551 - ETA: 1:40 - loss: 0.9003 - acc: 0.541 - ETA: 1:29 - loss: 0.9116 - acc: 0.533 - ETA: 1:19 - loss: 0.9079 - acc: 0.526 - ETA: 1:10 - loss: 0.8937 - acc: 0.525 - ETA: 1:02 - loss: 0.8797 - acc: 0.531 - ETA: 56s - loss: 0.8769 - acc: 0.533 - ETA: 50s - loss: 0.8760 - acc: 0.53 - ETA: 44s - loss: 0.8675 - acc: 0.53 - ETA: 39s - loss: 0.8553 - acc: 0.54 - ETA: 35s - loss: 0.8483 - acc: 0.54 - ETA: 31s - loss: 0.8440 - acc: 0.53 - ETA: 27s - loss: 0.8382 - acc: 0.52 - ETA: 23s - loss: 0.8309 - acc: 0.53 - ETA: 20s - loss: 0.8285 - acc: 0.52 - ETA: 17s - loss: 0.8223 - acc: 0.53 - ETA: 14s - loss: 0.8249 - acc: 0.52 - ETA: 11s - loss: 0.8199 - acc: 0.52 - ETA: 9s - loss: 0.8157 - acc: 0.5194 - ETA: 7s - loss: 0.8125 - acc: 0.516 - ETA: 4s - loss: 0.8084 - acc: 0.518 - ETA: 2s - loss: 0.8060 - acc: 0.513 - ETA: 0s - loss: 0.8025 - acc: 0.515 - 71s 134ms/step - loss: 0.8009 - acc: 0.5187 - val_loss: 0.8663 - val_acc: 0.2650\n",
      "Epoch 2/10\n",
      "534/534 [==============================] - ETA: 11s - loss: 0.7322 - acc: 0.50 - ETA: 10s - loss: 0.7015 - acc: 0.56 - ETA: 10s - loss: 0.7566 - acc: 0.52 - ETA: 10s - loss: 0.7343 - acc: 0.54 - ETA: 9s - loss: 0.7197 - acc: 0.5625 - ETA: 9s - loss: 0.7137 - acc: 0.562 - ETA: 9s - loss: 0.7086 - acc: 0.571 - ETA: 8s - loss: 0.7060 - acc: 0.593 - ETA: 8s - loss: 0.7042 - acc: 0.604 - ETA: 8s - loss: 0.7023 - acc: 0.600 - ETA: 7s - loss: 0.7085 - acc: 0.585 - ETA: 7s - loss: 0.7079 - acc: 0.583 - ETA: 7s - loss: 0.7041 - acc: 0.586 - ETA: 6s - loss: 0.7072 - acc: 0.575 - ETA: 6s - loss: 0.7043 - acc: 0.579 - ETA: 6s - loss: 0.7030 - acc: 0.578 - ETA: 5s - loss: 0.7021 - acc: 0.573 - ETA: 5s - loss: 0.7039 - acc: 0.559 - ETA: 5s - loss: 0.7031 - acc: 0.555 - ETA: 4s - loss: 0.7019 - acc: 0.559 - ETA: 4s - loss: 0.6995 - acc: 0.562 - ETA: 3s - loss: 0.6999 - acc: 0.562 - ETA: 3s - loss: 0.7009 - acc: 0.562 - ETA: 3s - loss: 0.6994 - acc: 0.565 - ETA: 2s - loss: 0.6993 - acc: 0.562 - ETA: 2s - loss: 0.6986 - acc: 0.557 - ETA: 2s - loss: 0.6972 - acc: 0.562 - ETA: 1s - loss: 0.7002 - acc: 0.560 - ETA: 1s - loss: 0.6990 - acc: 0.562 - ETA: 1s - loss: 0.6979 - acc: 0.564 - ETA: 0s - loss: 0.6980 - acc: 0.562 - ETA: 0s - loss: 0.6978 - acc: 0.564 - ETA: 0s - loss: 0.6967 - acc: 0.566 - 15s 28ms/step - loss: 0.7024 - acc: 0.5618 - val_loss: 0.6532 - val_acc: 0.7350\n",
      "Epoch 3/10\n",
      "534/534 [==============================] - ETA: 11s - loss: 0.7300 - acc: 0.31 - ETA: 10s - loss: 0.7044 - acc: 0.43 - ETA: 10s - loss: 0.7226 - acc: 0.47 - ETA: 10s - loss: 0.7730 - acc: 0.45 - ETA: 9s - loss: 0.7492 - acc: 0.5000 - ETA: 9s - loss: 0.7377 - acc: 0.510 - ETA: 9s - loss: 0.7275 - acc: 0.508 - ETA: 8s - loss: 0.7188 - acc: 0.507 - ETA: 8s - loss: 0.7156 - acc: 0.520 - ETA: 8s - loss: 0.6980 - acc: 0.550 - ETA: 7s - loss: 0.6945 - acc: 0.556 - ETA: 7s - loss: 0.6902 - acc: 0.567 - ETA: 7s - loss: 0.7046 - acc: 0.557 - ETA: 6s - loss: 0.7086 - acc: 0.558 - ETA: 6s - loss: 0.7128 - acc: 0.550 - ETA: 6s - loss: 0.7117 - acc: 0.554 - ETA: 5s - loss: 0.7163 - acc: 0.540 - ETA: 5s - loss: 0.7119 - acc: 0.541 - ETA: 5s - loss: 0.7087 - acc: 0.532 - ETA: 4s - loss: 0.7086 - acc: 0.528 - ETA: 4s - loss: 0.7035 - acc: 0.532 - ETA: 4s - loss: 0.7079 - acc: 0.522 - ETA: 3s - loss: 0.7080 - acc: 0.519 - ETA: 3s - loss: 0.7070 - acc: 0.523 - ETA: 3s - loss: 0.7063 - acc: 0.522 - ETA: 2s - loss: 0.7026 - acc: 0.524 - ETA: 2s - loss: 0.7003 - acc: 0.527 - ETA: 1s - loss: 0.6997 - acc: 0.529 - ETA: 1s - loss: 0.6986 - acc: 0.532 - ETA: 1s - loss: 0.7029 - acc: 0.529 - ETA: 0s - loss: 0.7001 - acc: 0.534 - ETA: 0s - loss: 0.6968 - acc: 0.539 - ETA: 0s - loss: 0.6988 - acc: 0.537 - 15s 29ms/step - loss: 0.6981 - acc: 0.5393 - val_loss: 0.7955 - val_acc: 0.3000\n",
      "Epoch 4/10\n",
      "534/534 [==============================] - ETA: 11s - loss: 0.5158 - acc: 0.87 - ETA: 11s - loss: 0.6502 - acc: 0.62 - ETA: 10s - loss: 0.6689 - acc: 0.58 - ETA: 10s - loss: 0.6597 - acc: 0.59 - ETA: 10s - loss: 0.6644 - acc: 0.58 - ETA: 9s - loss: 0.6647 - acc: 0.5417 - ETA: 9s - loss: 0.6688 - acc: 0.544 - ETA: 8s - loss: 0.6683 - acc: 0.570 - ETA: 8s - loss: 0.6755 - acc: 0.555 - ETA: 8s - loss: 0.6725 - acc: 0.562 - ETA: 7s - loss: 0.6721 - acc: 0.568 - ETA: 7s - loss: 0.6683 - acc: 0.578 - ETA: 7s - loss: 0.6615 - acc: 0.591 - ETA: 6s - loss: 0.6644 - acc: 0.589 - ETA: 6s - loss: 0.6563 - acc: 0.600 - ETA: 6s - loss: 0.6523 - acc: 0.605 - ETA: 5s - loss: 0.6529 - acc: 0.606 - ETA: 5s - loss: 0.6487 - acc: 0.611 - ETA: 5s - loss: 0.6497 - acc: 0.611 - ETA: 4s - loss: 0.6444 - acc: 0.615 - ETA: 4s - loss: 0.6462 - acc: 0.616 - ETA: 4s - loss: 0.6446 - acc: 0.619 - ETA: 3s - loss: 0.6420 - acc: 0.614 - ETA: 3s - loss: 0.6437 - acc: 0.601 - ETA: 2s - loss: 0.6497 - acc: 0.610 - ETA: 2s - loss: 0.6483 - acc: 0.615 - ETA: 2s - loss: 0.6477 - acc: 0.618 - ETA: 1s - loss: 0.6478 - acc: 0.616 - ETA: 1s - loss: 0.6479 - acc: 0.616 - ETA: 1s - loss: 0.6444 - acc: 0.618 - ETA: 0s - loss: 0.6458 - acc: 0.614 - ETA: 0s - loss: 0.6451 - acc: 0.617 - ETA: 0s - loss: 0.6548 - acc: 0.609 - 15s 29ms/step - loss: 0.6537 - acc: 0.6105 - val_loss: 0.6703 - val_acc: 0.4250\n",
      "Epoch 5/10\n",
      "534/534 [==============================] - ETA: 11s - loss: 0.5728 - acc: 0.81 - ETA: 11s - loss: 0.6187 - acc: 0.62 - ETA: 10s - loss: 0.6318 - acc: 0.62 - ETA: 10s - loss: 0.6155 - acc: 0.62 - ETA: 10s - loss: 0.6116 - acc: 0.62 - ETA: 9s - loss: 0.6327 - acc: 0.5938 - ETA: 9s - loss: 0.6398 - acc: 0.589 - ETA: 9s - loss: 0.6410 - acc: 0.585 - ETA: 8s - loss: 0.6380 - acc: 0.597 - ETA: 8s - loss: 0.6367 - acc: 0.593 - ETA: 8s - loss: 0.6420 - acc: 0.590 - ETA: 7s - loss: 0.6230 - acc: 0.614 - ETA: 7s - loss: 0.6184 - acc: 0.625 - ETA: 6s - loss: 0.6119 - acc: 0.638 - ETA: 6s - loss: 0.6149 - acc: 0.641 - ETA: 6s - loss: 0.6221 - acc: 0.640 - ETA: 5s - loss: 0.6214 - acc: 0.647 - ETA: 5s - loss: 0.6358 - acc: 0.642 - ETA: 5s - loss: 0.6371 - acc: 0.644 - ETA: 4s - loss: 0.6455 - acc: 0.640 - ETA: 4s - loss: 0.6576 - acc: 0.628 - ETA: 4s - loss: 0.6571 - acc: 0.627 - ETA: 3s - loss: 0.6546 - acc: 0.644 - ETA: 3s - loss: 0.6554 - acc: 0.645 - ETA: 3s - loss: 0.6570 - acc: 0.640 - ETA: 2s - loss: 0.6570 - acc: 0.644 - ETA: 2s - loss: 0.6552 - acc: 0.652 - ETA: 1s - loss: 0.6551 - acc: 0.654 - ETA: 1s - loss: 0.6534 - acc: 0.655 - ETA: 1s - loss: 0.6517 - acc: 0.654 - ETA: 0s - loss: 0.6534 - acc: 0.653 - ETA: 0s - loss: 0.6542 - acc: 0.652 - ETA: 0s - loss: 0.6538 - acc: 0.653 - 15s 29ms/step - loss: 0.6534 - acc: 0.6517 - val_loss: 0.5621 - val_acc: 0.7400\n",
      "Epoch 6/10\n",
      "534/534 [==============================] - ETA: 11s - loss: 0.6698 - acc: 0.50 - ETA: 11s - loss: 0.6107 - acc: 0.71 - ETA: 10s - loss: 0.6025 - acc: 0.72 - ETA: 10s - loss: 0.5797 - acc: 0.76 - ETA: 10s - loss: 0.5579 - acc: 0.77 - ETA: 9s - loss: 0.6427 - acc: 0.7292 - ETA: 9s - loss: 0.6337 - acc: 0.723 - ETA: 8s - loss: 0.6285 - acc: 0.718 - ETA: 8s - loss: 0.6356 - acc: 0.694 - ETA: 8s - loss: 0.6225 - acc: 0.712 - ETA: 7s - loss: 0.6240 - acc: 0.704 - ETA: 7s - loss: 0.6290 - acc: 0.692 - ETA: 7s - loss: 0.6350 - acc: 0.677 - ETA: 6s - loss: 0.6289 - acc: 0.683 - ETA: 6s - loss: 0.6241 - acc: 0.683 - ETA: 6s - loss: 0.6269 - acc: 0.679 - ETA: 5s - loss: 0.6294 - acc: 0.665 - ETA: 5s - loss: 0.6220 - acc: 0.673 - ETA: 5s - loss: 0.6215 - acc: 0.680 - ETA: 4s - loss: 0.6147 - acc: 0.681 - ETA: 4s - loss: 0.6157 - acc: 0.678 - ETA: 4s - loss: 0.6154 - acc: 0.676 - ETA: 3s - loss: 0.6108 - acc: 0.679 - ETA: 3s - loss: 0.6032 - acc: 0.684 - ETA: 2s - loss: 0.6104 - acc: 0.682 - ETA: 2s - loss: 0.6164 - acc: 0.685 - ETA: 2s - loss: 0.6126 - acc: 0.685 - ETA: 1s - loss: 0.6146 - acc: 0.683 - ETA: 1s - loss: 0.6227 - acc: 0.670 - ETA: 1s - loss: 0.6223 - acc: 0.668 - ETA: 0s - loss: 0.6221 - acc: 0.669 - ETA: 0s - loss: 0.6217 - acc: 0.668 - ETA: 0s - loss: 0.6161 - acc: 0.672 - 15s 29ms/step - loss: 0.6199 - acc: 0.6685 - val_loss: 0.7805 - val_acc: 0.4900\n",
      "Epoch 7/10\n",
      "534/534 [==============================] - ETA: 11s - loss: 0.4940 - acc: 0.68 - ETA: 11s - loss: 0.4922 - acc: 0.75 - ETA: 11s - loss: 0.5078 - acc: 0.75 - ETA: 10s - loss: 0.5125 - acc: 0.75 - ETA: 10s - loss: 0.5418 - acc: 0.72 - ETA: 10s - loss: 0.5281 - acc: 0.75 - ETA: 9s - loss: 0.5255 - acc: 0.7589 - ETA: 9s - loss: 0.5511 - acc: 0.726 - ETA: 8s - loss: 0.5292 - acc: 0.750 - ETA: 8s - loss: 0.5071 - acc: 0.756 - ETA: 8s - loss: 0.5264 - acc: 0.738 - ETA: 7s - loss: 0.5104 - acc: 0.755 - ETA: 7s - loss: 0.5022 - acc: 0.764 - ETA: 7s - loss: 0.5009 - acc: 0.763 - ETA: 6s - loss: 0.5254 - acc: 0.745 - ETA: 6s - loss: 0.5464 - acc: 0.734 - ETA: 5s - loss: 0.5694 - acc: 0.720 - ETA: 5s - loss: 0.5697 - acc: 0.725 - ETA: 5s - loss: 0.5646 - acc: 0.727 - ETA: 4s - loss: 0.5675 - acc: 0.728 - ETA: 4s - loss: 0.5700 - acc: 0.723 - ETA: 4s - loss: 0.5785 - acc: 0.707 - ETA: 3s - loss: 0.5805 - acc: 0.701 - ETA: 3s - loss: 0.5825 - acc: 0.690 - ETA: 2s - loss: 0.5861 - acc: 0.685 - ETA: 2s - loss: 0.5838 - acc: 0.689 - ETA: 2s - loss: 0.5854 - acc: 0.685 - ETA: 1s - loss: 0.5850 - acc: 0.687 - ETA: 1s - loss: 0.5880 - acc: 0.678 - ETA: 1s - loss: 0.5882 - acc: 0.677 - ETA: 0s - loss: 0.5858 - acc: 0.679 - ETA: 0s - loss: 0.5836 - acc: 0.681 - ETA: 0s - loss: 0.5822 - acc: 0.685 - 15s 29ms/step - loss: 0.5825 - acc: 0.6854 - val_loss: 1.3142 - val_acc: 0.3800\n",
      "Epoch 8/10\n",
      "534/534 [==============================] - ETA: 11s - loss: 0.8269 - acc: 0.56 - ETA: 10s - loss: 0.6677 - acc: 0.62 - ETA: 10s - loss: 0.5760 - acc: 0.70 - ETA: 10s - loss: 0.5680 - acc: 0.73 - ETA: 9s - loss: 0.5869 - acc: 0.7125 - ETA: 9s - loss: 0.5926 - acc: 0.697 - ETA: 9s - loss: 0.5861 - acc: 0.696 - ETA: 8s - loss: 0.5878 - acc: 0.703 - ETA: 8s - loss: 0.5740 - acc: 0.715 - ETA: 8s - loss: 0.5608 - acc: 0.725 - ETA: 7s - loss: 0.5796 - acc: 0.698 - ETA: 7s - loss: 0.5658 - acc: 0.708 - ETA: 7s - loss: 0.5703 - acc: 0.711 - ETA: 6s - loss: 0.5651 - acc: 0.718 - ETA: 6s - loss: 0.5708 - acc: 0.716 - ETA: 6s - loss: 0.5582 - acc: 0.726 - ETA: 5s - loss: 0.5545 - acc: 0.724 - ETA: 5s - loss: 0.5533 - acc: 0.718 - ETA: 5s - loss: 0.5481 - acc: 0.723 - ETA: 4s - loss: 0.5376 - acc: 0.731 - ETA: 4s - loss: 0.5238 - acc: 0.741 - ETA: 4s - loss: 0.5202 - acc: 0.741 - ETA: 3s - loss: 0.5022 - acc: 0.752 - ETA: 3s - loss: 0.5177 - acc: 0.750 - ETA: 2s - loss: 0.5086 - acc: 0.755 - ETA: 2s - loss: 0.5103 - acc: 0.754 - ETA: 2s - loss: 0.5024 - acc: 0.761 - ETA: 1s - loss: 0.4985 - acc: 0.763 - ETA: 1s - loss: 0.4965 - acc: 0.765 - ETA: 1s - loss: 0.5057 - acc: 0.764 - ETA: 0s - loss: 0.5050 - acc: 0.764 - ETA: 0s - loss: 0.5123 - acc: 0.759 - ETA: 0s - loss: 0.5067 - acc: 0.765 - 15s 29ms/step - loss: 0.5074 - acc: 0.7659 - val_loss: 1.0934 - val_acc: 0.4500\n",
      "Epoch 9/10\n",
      "534/534 [==============================] - ETA: 11s - loss: 0.3084 - acc: 0.87 - ETA: 10s - loss: 0.5467 - acc: 0.75 - ETA: 10s - loss: 0.5546 - acc: 0.75 - ETA: 10s - loss: 0.5428 - acc: 0.75 - ETA: 9s - loss: 0.5162 - acc: 0.7500 - ETA: 9s - loss: 0.5270 - acc: 0.750 - ETA: 9s - loss: 0.5044 - acc: 0.758 - ETA: 8s - loss: 0.4996 - acc: 0.757 - ETA: 8s - loss: 0.4796 - acc: 0.770 - ETA: 8s - loss: 0.4641 - acc: 0.775 - ETA: 7s - loss: 0.4600 - acc: 0.772 - ETA: 7s - loss: 0.4721 - acc: 0.765 - ETA: 7s - loss: 0.4590 - acc: 0.778 - ETA: 6s - loss: 0.4615 - acc: 0.776 - ETA: 6s - loss: 0.4510 - acc: 0.787 - ETA: 6s - loss: 0.4324 - acc: 0.796 - ETA: 5s - loss: 0.4280 - acc: 0.797 - ETA: 5s - loss: 0.4311 - acc: 0.795 - ETA: 5s - loss: 0.4287 - acc: 0.799 - ETA: 4s - loss: 0.4278 - acc: 0.800 - ETA: 4s - loss: 0.4339 - acc: 0.794 - ETA: 4s - loss: 0.4548 - acc: 0.784 - ETA: 3s - loss: 0.4587 - acc: 0.779 - ETA: 3s - loss: 0.4683 - acc: 0.778 - ETA: 2s - loss: 0.4661 - acc: 0.782 - ETA: 2s - loss: 0.4696 - acc: 0.778 - ETA: 2s - loss: 0.4757 - acc: 0.773 - ETA: 1s - loss: 0.4750 - acc: 0.774 - ETA: 1s - loss: 0.4787 - acc: 0.775 - ETA: 1s - loss: 0.4817 - acc: 0.775 - ETA: 0s - loss: 0.4835 - acc: 0.776 - ETA: 0s - loss: 0.4848 - acc: 0.777 - ETA: 0s - loss: 0.4923 - acc: 0.767 - 15s 29ms/step - loss: 0.4943 - acc: 0.7640 - val_loss: 0.7899 - val_acc: 0.4750\n",
      "Epoch 10/10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "534/534 [==============================] - ETA: 11s - loss: 0.4164 - acc: 0.81 - ETA: 10s - loss: 0.4385 - acc: 0.78 - ETA: 10s - loss: 0.4363 - acc: 0.83 - ETA: 10s - loss: 0.4228 - acc: 0.87 - ETA: 10s - loss: 0.4237 - acc: 0.87 - ETA: 9s - loss: 0.4251 - acc: 0.8646 - ETA: 9s - loss: 0.4224 - acc: 0.857 - ETA: 9s - loss: 0.3960 - acc: 0.875 - ETA: 8s - loss: 0.3778 - acc: 0.875 - ETA: 8s - loss: 0.3833 - acc: 0.862 - ETA: 8s - loss: 0.3719 - acc: 0.863 - ETA: 7s - loss: 0.3876 - acc: 0.864 - ETA: 7s - loss: 0.3911 - acc: 0.865 - ETA: 6s - loss: 0.3905 - acc: 0.866 - ETA: 6s - loss: 0.3901 - acc: 0.862 - ETA: 6s - loss: 0.3906 - acc: 0.859 - ETA: 5s - loss: 0.3808 - acc: 0.864 - ETA: 5s - loss: 0.3699 - acc: 0.868 - ETA: 5s - loss: 0.3651 - acc: 0.865 - ETA: 4s - loss: 0.3544 - acc: 0.868 - ETA: 4s - loss: 0.3566 - acc: 0.863 - ETA: 4s - loss: 0.3486 - acc: 0.866 - ETA: 3s - loss: 0.3580 - acc: 0.861 - ETA: 3s - loss: 0.3645 - acc: 0.856 - ETA: 3s - loss: 0.3699 - acc: 0.855 - ETA: 2s - loss: 0.3750 - acc: 0.851 - ETA: 2s - loss: 0.3785 - acc: 0.847 - ETA: 1s - loss: 0.3780 - acc: 0.850 - ETA: 1s - loss: 0.3778 - acc: 0.853 - ETA: 1s - loss: 0.3834 - acc: 0.850 - ETA: 0s - loss: 0.3896 - acc: 0.842 - ETA: 0s - loss: 0.3869 - acc: 0.843 - ETA: 0s - loss: 0.3956 - acc: 0.837 - 16s 29ms/step - loss: 0.3984 - acc: 0.8352 - val_loss: 0.6176 - val_acc: 0.6100\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x285ac90acc0>"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = build_model(max_seq_length)\n",
    "\n",
    "# Instantiate variables\n",
    "initialize_vars(sess)\n",
    "\n",
    "model.fit(\n",
    "    [train_input_ids, train_input_masks, train_segment_ids], \n",
    "    train_labels,\n",
    "    validation_data=([test_input_ids, test_input_masks, test_segment_ids], test_labels),\n",
    "    epochs=10,\n",
    "    batch_size=16\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_ids (InputLayer)          (None, 256)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_masks (InputLayer)        (None, 256)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "segment_ids (InputLayer)        (None, 256)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "bert_layer_5 (BertLayer)        (None, 768)          110104890   input_ids[0][0]                  \n",
      "                                                                 input_masks[0][0]                \n",
      "                                                                 segment_ids[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dense_8 (Dense)                 (None, 256)          196864      bert_layer_5[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "dense_9 (Dense)                 (None, 1)            257         dense_8[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 110,302,011\n",
      "Trainable params: 3,147,009\n",
      "Non-trainable params: 107,155,002\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.save('BertModel.h5')\n",
    "pre_save_preds = model.predict([test_input_ids, \n",
    "                                test_input_masks, \n",
    "                                test_segment_ids]\n",
    "                              ) # predictions before we clear and reload model\n",
    "\n",
    "# Clear and load model\n",
    "model = None\n",
    "model = build_model(max_seq_length)\n",
    "initialize_vars(sess)\n",
    "model.load_weights('BertModel.h5')\n",
    "\n",
    "post_save_preds = model.predict([test_input_ids, \n",
    "                                test_input_masks, \n",
    "                                test_segment_ids]\n",
    "                              ) # predictions after we clear and reload model\n",
    "all(pre_save_preds == post_save_preds) # Are they the same?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix:\n",
      "[[106  41]\n",
      " [ 37  16]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      attack       0.74      0.72      0.73       147\n",
      "     support       0.28      0.30      0.29        53\n",
      "\n",
      "   micro avg       0.61      0.61      0.61       200\n",
      "   macro avg       0.51      0.51      0.51       200\n",
      "weighted avg       0.62      0.61      0.61       200\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_pred = model.predict([test_input_ids, \n",
    "                                test_input_masks, \n",
    "                                test_segment_ids]\n",
    "                              )\n",
    "y_pred = np.where(y_pred > 0.5, 1, 0)\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_matrix(test_labels, y_pred))\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(test_labels, y_pred, target_names=[\"attack\", \"support\"]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

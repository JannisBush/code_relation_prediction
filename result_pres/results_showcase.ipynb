{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Result Showcase\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Necessary imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from IPython.display import IFrame\n",
    "from wordcloud import WordCloud, STOPWORDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read all necessary data\n",
    "df = pd.read_csv('../data/complete_data.tsv', sep='\\t')\n",
    "data_stats_org = np.load('../data/stats/data_stats_org.npy', allow_pickle=True).item()\n",
    "data_stats_resp = np.load('../data/stats/data_stats_resp.npy', allow_pickle=True).item()\n",
    "data_stats_topic = np.load('../data/stats/data_stats_topic.npy', allow_pickle=True).item()\n",
    "data_stats_sent = np.load('../data/stats/sent_stats.npy', allow_pickle=True).item()\n",
    "data_stats_author = pd.read_csv('../data/stats/data_stats_author.tsv', sep='\\t')\n",
    "data_stats_total = pd.read_csv('../data/stats/data_stats_total.tsv', sep='\\t')\n",
    "data_nix_ken = pd.read_csv('../data/stats/data_nix_ken.tsv', sep='\\t')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data overview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# General stats of all datasets\n",
    "# Number of unique arguments, number of total pairs, number of attacks/supports und unrelated pairs\n",
    "# Statistics about the total length (org+response) of the pairs\n",
    "# Important: debate_test/train is already repaired, but still not the same as in the paper\n",
    "# Length important for the seq_len parameter of BERT\n",
    "data_stats_total.loc[data_stats_total['dataset'].isin(['debate_test', 'debate_train', 'procon', 'political', 'agreement'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Debate train/test by topic\n",
    "# Topics that are not matching paper are Interentaccess and Militaryservice\n",
    "# Most topics attack/support distributions are similar to the overall distribution\n",
    "pd.concat((data_stats_topic['debate_train'], data_stats_topic['debate_test']), keys=['train', 'test'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Political by topic\n",
    "# Most topics have a similar distribution, minimum wage is an exception\n",
    "data_stats_topic['political']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Political by author\n",
    "# Same author mostly support each other\n",
    "# Different authors mostly attack each other\n",
    "# Dataset is heavily imbalanced in respect to the author, Kennedy occurs way more often\n",
    "print(data_nix_ken.groupby(\"author\").nunique())\n",
    "data_stats_author.style.background_gradient(cmap='Blues')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Political duplicates\n",
    "for data_set in ['political']:\n",
    "    print(data_set + \" Duplicates:\")\n",
    "    df_check = df[df['org_dataset'] == data_set]\n",
    "    print(df_check[df_check.duplicated(subset=['org', 'response'], keep=False)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot distribution of length of org, resp and combined over the different datasets\n",
    "# Seq_len 128/200 ~75% of debate_dataset, 250 ~75% political_dataset\n",
    "fig, (ax1,ax2) = plt.subplots(1,2, figsize=(10,4))  # 1 row, 2 columns\n",
    "\n",
    "for data_set, ax in [('debate_extended', ax1), ('political',ax2)]:\n",
    "    df_plot = df[df['org_dataset'] == data_set]\n",
    "    df_plot.boxplot(ax=ax)\n",
    "    ax.set_title(data_set)\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Attack/Support ratios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot how many arguments attack an argument (attack-ratio)\n",
    "# Most arguments are only attacked or only supported (interesting for detecting arguments likely to be attacked/supported)\n",
    "# If we disregard every argument, which is only answered to once most arguments have an attack-ratio of 0.5\n",
    "# In the case of the political dataset many arguments are unrelated, and unrelated arguments are disregarded in this plot\n",
    "fig, (ax1,ax2) = plt.subplots(2,2, figsize=(10,4))  # 2 rows, 2 columns\n",
    "for data_set, ax in [('debate_extended', ax1), ('political',ax2)]:\n",
    "    df_plot = data_stats_org[data_set].iloc[:-1].apply(\n",
    "        lambda r: pd.Series({\"Attack-ratio\": r.attacked / r.tot,\n",
    "                             \"Attack-ratio (exluding arguments only attacked/supported once)\": np.nan if r.tot == 1 else r.attacked / r.tot}),\n",
    "        axis=1)\n",
    "    # Ratio broken?\n",
    "    df_plot.hist(density=False, ax=ax)\n",
    "    ax[0].set_ylabel(data_set, rotation=0)\n",
    "    \n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Usage of arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First column shows how many answers an argument has\n",
    "# Second column shows how many outgoing links an argument has\n",
    "# Most arguments only have one ingoing link, but some have many ~10 debate, ~30 political\n",
    "# In debate (orginal) every argument only has one outgoing link, in political most have one, but some have many ~8\n",
    "fig, (ax1,ax2,ax3) = plt.subplots(3,2, figsize=(10,4))  # 3 rows, 2 columns\n",
    "\n",
    "for data_set, ax in [('debate_test', ax1), ('debate_extended', ax2), ('political',ax3)]:\n",
    "    df_plot = data_stats_org[data_set].iloc[:-1]\n",
    "    df_plot = df_plot['tot']\n",
    "    # Ratio broken?\n",
    "    df_plot.hist(density=True, ax=ax[0])\n",
    "    ax[0].set_title('{0}, org'.format(data_set))\n",
    "    ax[1].set_title('{0}, resp'.format(data_set))\n",
    "    df_plot = data_stats_resp[data_set].iloc[:-1]\n",
    "    df_plot = df_plot['tot']\n",
    "    # Ratio broken?\n",
    "    df_plot.hist(bins=np.arange(0, 10), ax=ax[1])\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualizations Debate Responses\n",
    "- Word scattertext of the responses in debate_train\n",
    "- Lime and anchor visualization of an example sentence, using **only_response** (rest default options)\n",
    "    - model has accuracy 53% (quite bad)\n",
    "    - details about LIME [here](https://github.com/marcotcr/lime)\n",
    "    - details about ANCHOR [here](https://github.com/marcotcr/anchor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scattertext of the responses in debate_train\n",
    "# No special \"attacking\" or \"supporting\" words easily recognizable\n",
    "# The words are either topic specific, e.g. China (in topic Chinaonechildpolicy there are more supports than attacks)\n",
    "# Or they seem to be there by chance (small dataset), e.g. he, does\n",
    "IFrame(src='../data/plots/scattertext_attack_supportdebate_train.html', width=950, height=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lime Visualization\n",
    "# Some of the words play an influence as expected, e.g. are and not (attack), play, and alcohol (support)\n",
    "# Others do not play the expected influence, e.g. china (attack and not support as expected)\n",
    "# Overall, all weights are really small and the removal/replacement with UNK of a single word \n",
    "# does not change the prediction\n",
    "IFrame(src='./lime.html', width=1200, height=350)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Anchor Visualization\n",
    "# Anchor did not find a way to change some words, and then to predict the other class\n",
    "IFrame(src='./anchor.html', width=1200, height=350)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualizations Political Authors\n",
    "- WordClouds authors\n",
    "- Scattertext authors\n",
    "- Lime and Anchor, **only_org** (rest default)\n",
    "    - Model acc: 70%, F1: 70%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wordclouds for kennedy and for nixon\n",
    "# Both often say the name of the other candidate, Nixon talks about Predisdent Eisenhower\n",
    "fig, (ax1,ax2) = plt.subplots(1,2, figsize=(12,10))  # 1 row, 2 columns\n",
    "\n",
    "stopwords = set(STOPWORDS)  # set(STOPWORDS)\n",
    "wordcloud = WordCloud(\n",
    "    stopwords=stopwords).generate(\n",
    "    \" \".join(text for text in data_nix_ken.loc[data_nix_ken[\"author\"] == 'Nixon', 'text']))\n",
    "ax1.imshow(wordcloud, interpolation=\"bilinear\")\n",
    "ax1.set_title(\"Nixon WordCloud\")\n",
    "ax1.set_axis_off()\n",
    "wordcloud = WordCloud(\n",
    "    stopwords=stopwords).generate(\n",
    "    \" \".join(text for text in data_nix_ken.loc[data_nix_ken[\"author\"] == 'Kennedy', 'text']))\n",
    "ax2.imshow(wordcloud, interpolation=\"bilinear\")\n",
    "ax2.set_title(\"Kennedy WordCloud\")\n",
    "ax2.set_axis_off()\n",
    "\n",
    "\n",
    "plt.tight_layout()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scattertext\n",
    "# Scattertext of the authors in political\n",
    "# The word usage of Nixon and Kennedy is quite different\n",
    "IFrame(src='../data/plots/scattertext_nixon_kennedy.html', width=950, height=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lime\n",
    "# All words have a very small impact\n",
    "IFrame(src='./lime_pol.html', width=1200, height=350)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Anchors\n",
    "# No rule found\n",
    "IFrame(src='./anchor_pol.html', width=1200, height=350)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baselines\n",
    "- TODO: for the grouped results, actually calculate the weighted average/baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Major Class\n",
    "def get_major_acc(x, classes=['unrelated', 'yes', 'no']):\n",
    "    return np.divide(x[classes].max(), np.sum(x[classes]))\n",
    "\n",
    "def get_major_class(x, classes=['unrelated', 'yes', 'no']):\n",
    "    return x[classes].astype('float64').idxmax()\n",
    "\n",
    "data_stats_total['major_acc'] = data_stats_total.apply(get_major_acc, axis=1)\n",
    "data_stats_total['major_class'] = data_stats_total.apply(get_major_class, axis=1)\n",
    "\n",
    "data_stats_total.loc[data_stats_total['dataset'].isin(['debate_test', 'political'])][['dataset', 'major_class', 'major_acc']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Major Class per Topic node\n",
    "data = data_stats_topic['debate_test']\n",
    "data['major_acc'] = data.apply(get_major_acc, axis=1)\n",
    "data['major_class'] = data.apply(get_major_class, axis=1)\n",
    "data[['topic', 'major_class', 'major_acc', 'tot']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Major Class per Topic political\n",
    "data = data_stats_topic['political']\n",
    "data['major_acc'] = data.apply(get_major_acc, axis=1)\n",
    "data['major_class'] = data.apply(get_major_class, axis=1)\n",
    "data[['topic', 'major_class', 'major_acc', 'tot']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Major Class per Topic political attack/support only\n",
    "data = data_stats_topic['political']\n",
    "data['major_acc'] = data.apply(get_major_acc, args=[['yes','no']], axis=1)\n",
    "data['major_class'] = data.apply(get_major_class, args=[['yes','no']], axis=1)\n",
    "data[['topic', 'major_class', 'major_acc', 'tot']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Major Class Author identified \n",
    "data = data_stats_author.copy()\n",
    "data['major_acc'] = data.apply(get_major_acc, axis=1)\n",
    "data['major_class'] = data.apply(get_major_class, axis=1)\n",
    "data[['author_resp', 'author_org', 'major_class', 'major_acc', 'tot']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Major Class Author identified attack/support only\n",
    "data = data_stats_author.copy()\n",
    "data['major_acc'] = data.apply(get_major_acc, args=[['yes','no']], axis=1)\n",
    "data['major_class'] = data.apply(get_major_class, args=[['yes','no']], axis=1)\n",
    "data[['author_resp', 'author_org', 'major_class', 'major_acc', 'tot']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merged to same author / different author\n",
    "# Very high accuracy possible if only detected if it is the same or a different author\n",
    "data = data_stats_author.iloc[:-1].copy()\n",
    "data['authors'] = data.apply(lambda r: 'Same' if r['author_resp'] == r['author_org'] else 'Different', axis=1)\n",
    "data = data.groupby('authors').sum()\n",
    "data = data.reset_index()\n",
    "data['major_acc'] = data.apply(get_major_acc, args=[['yes','no']], axis=1)\n",
    "data['major_class'] = data.apply(get_major_class, args=[['yes','no']], axis=1)\n",
    "\n",
    "data[['authors', 'major_class', 'major_acc', 'tot']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sentiment Analysis (nltk vader)\n",
    "\n",
    "# Only responses debate test, supporting arguments often have a positive sentiment\n",
    "# Attacking arguments have nothing special\n",
    "pd.concat((data_stats_sent['respdebate_test'],data_stats_sent['resppolitical']), keys=['node', 'political'], sort=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Both org and response\n",
    "# Attack often have different sentiment, support often have the same sentiment (node)\n",
    "# Nothing meaningful for political\n",
    "pd.concat((data_stats_sent['bothdebate_test'],data_stats_sent['bothpolitical']), keys=['node', 'political'], sort=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# .... ?\n",
    "# Major Class for every Org argument\n",
    "# Major Class for every Resp argument (only political)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Node Acc with different parameters\n",
    "# TODO: Comparison with Paper + Baselines\n",
    "\n",
    "# Fixed: input=both, seq_len=128, warmup_prop=0.1, seed=42\n",
    "# Tested: model=base-uncased,large-uncased, epochs=3,4,5, batch_size=8,12,16, lr=2e-5, 3e-5, 5e-5\n",
    "# Gradient accumulation: batch_size/4 for bert_large \n",
    "# (in principle equivalent, in practice different because of rounding errors etc.)\n",
    "eval_preds = pd.read_csv('../pytorch/node_both/eval_preds.csv')\n",
    "eval_results = pd.read_csv('../pytorch/node_both/eval_results.tsv', sep='\\t')\n",
    "\n",
    "# Some stats: mean, min, max, std\n",
    "print(eval_results['acc'].agg([np.mean, np.min, np.max, np.std]))\n",
    "# Print settings of best result\n",
    "print(eval_results.iloc[eval_results['acc'].idxmax()])\n",
    "\n",
    "# Show the table\n",
    "eval_results.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Political F1 CrossVal\n",
    "# Comparison with Paper + Baselines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Agreement F1 CrossVal\n",
    "# Comparison with Paper + Baselines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Node results with respect to topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Political results with respect to topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Political results with respect to author"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Results with respect to same org, same resp (always gets the same label or not?)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}

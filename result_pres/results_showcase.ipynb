{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Result Showcase\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Necessary imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from IPython.display import IFrame, HTML\n",
    "from wordcloud import WordCloud, STOPWORDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read all necessary data\n",
    "df = pd.read_csv('../data/complete_data.tsv', sep='\\t')\n",
    "data_stats_org = np.load('../data/stats/data_stats_org.npy', allow_pickle=True).item()\n",
    "data_stats_resp = np.load('../data/stats/data_stats_resp.npy', allow_pickle=True).item()\n",
    "data_stats_topic = np.load('../data/stats/data_stats_topic.npy', allow_pickle=True).item()\n",
    "data_stats_sent = np.load('../data/stats/sent_stats.npy', allow_pickle=True).item()\n",
    "data_stats_author = pd.read_csv('../data/stats/data_stats_author.tsv', sep='\\t')\n",
    "data_stats_total = pd.read_csv('../data/stats/data_stats_total.tsv', sep='\\t')\n",
    "data_nix_ken = pd.read_csv('../data/stats/data_nix_ken.tsv', sep='\\t')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data overview\n",
    "- NoDE\n",
    "    - Link: http://www-sop.inria.fr/NoDE/NoDE-xml.html\n",
    "    - Paper: https://pdfs.semanticscholar.org/16d1/6b8a37c5313fa8c8430fddc011f2a98d20c5.pdf\n",
    "- Political\n",
    "    - Link: https://dh.fbk.eu/resources/political-argumentation\n",
    "    - Paper: https://aaai.org/ocs/index.php/AAAI/AAAI18/paper/view/16393/16020\n",
    "- Agreement\n",
    "    - Link: https://dh.fbk.eu/resources/agreement-disagreement\n",
    "    - Paper: https://www.aclweb.org/anthology/C16-1232"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# General stats of all datasets\n",
    "# Number of unique arguments, number of total pairs, number of attacks/supports und unrelated pairs\n",
    "# Statistics about the total length (org+response) of the pairs\n",
    "# Important: debate_test/train is already repaired, but still not the same as in the paper\n",
    "# Important: agreement had many rows which could not get parsed, e.g. because resp or org was empty, they were excluded\n",
    "# And the dataset is smaller than reported in the paper\n",
    "# Important: There are two duplicates in the political dataset\n",
    "# Length important for the seq_len parameter of BERT\n",
    "data_stats_total.loc[data_stats_total['dataset'].isin(['debate_test', 'debate_train', 'procon', 'political', 'agreement'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Debate train/test by topic\n",
    "# Topics that are not matching paper are Interentaccess and Militaryservice\n",
    "# Most topics attack/support distributions are similar to the overall distribution\n",
    "pd.concat((data_stats_topic['debate_train'], data_stats_topic['debate_test']), keys=['train', 'test'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Political by topic\n",
    "# Most topics have a similar distribution, minimum wage is an exception\n",
    "data_stats_topic['political']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Political by author\n",
    "# Same author mostly support each other\n",
    "# Different authors mostly attack each other\n",
    "# Dataset is heavily imbalanced in respect to the author, Kennedy occurs way more often\n",
    "print(data_nix_ken.groupby(\"author\").nunique())\n",
    "data_stats_author.style.background_gradient(cmap='Blues')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Political duplicates\n",
    "for data_set in ['political']:\n",
    "    print(data_set + \" Duplicates:\")\n",
    "    df_check = df[df['org_dataset'] == data_set]\n",
    "    print(df_check[df_check.duplicated(subset=['org', 'response'], keep=False)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot distribution of length of org, resp and combined over the different datasets\n",
    "# Seq_len 128/200 ~75% of debate_dataset, 250 ~75% political_dataset\n",
    "fig, (ax1,ax2) = plt.subplots(1,2, figsize=(10,4))  # 1 row, 2 columns\n",
    "\n",
    "for data_set, ax in [('debate_extended', ax1), ('political',ax2)]:\n",
    "    df_plot = df[df['org_dataset'] == data_set]\n",
    "    df_plot.boxplot(ax=ax)\n",
    "    ax.set_title(data_set)\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Attack/Support ratios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot how many arguments attack an argument (attack-ratio)\n",
    "# Most arguments are only attacked or only supported (interesting for detecting arguments likely to be attacked/supported)\n",
    "# If we disregard every argument, which is only answered to once most arguments have an attack-ratio of 0.5\n",
    "# In the case of the political dataset many arguments are unrelated, and unrelated arguments are disregarded in this plot\n",
    "fig, (ax1,ax2) = plt.subplots(2,2, figsize=(10,4))  # 2 rows, 2 columns\n",
    "for data_set, ax in [('debate_extended', ax1), ('political',ax2)]:\n",
    "    df_plot = data_stats_org[data_set].iloc[:-1].apply(\n",
    "        lambda r: pd.Series({\"Attack-ratio\": r.attacked / r.tot,\n",
    "                             \"Attack-ratio (exluding arguments only attacked/supported once)\": np.nan if r.tot == 1 else r.attacked / r.tot}),\n",
    "        axis=1)\n",
    "    # Ratio broken?\n",
    "    df_plot.hist(density=False, ax=ax)\n",
    "    ax[0].set_ylabel(data_set, rotation=0)\n",
    "    \n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Usage of arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First column shows how many answers an argument has\n",
    "# Second column shows how many outgoing links an argument has\n",
    "# Most arguments only have one ingoing link, but some have many ~10 debate, ~30 political\n",
    "# In debate (orginal) every argument only has one outgoing link, in political most have one, but some have many ~8\n",
    "fig, (ax1,ax2,ax3) = plt.subplots(3,2, figsize=(10,4))  # 3 rows, 2 columns\n",
    "\n",
    "for data_set, ax in [('debate_test', ax1), ('debate_extended', ax2), ('political',ax3)]:\n",
    "    df_plot = data_stats_org[data_set].iloc[:-1]\n",
    "    df_plot = df_plot['tot']\n",
    "    # Ratio broken?\n",
    "    df_plot.hist(density=True, ax=ax[0])\n",
    "    ax[0].set_title('{0}, org'.format(data_set))\n",
    "    ax[1].set_title('{0}, resp'.format(data_set))\n",
    "    df_plot = data_stats_resp[data_set].iloc[:-1]\n",
    "    df_plot = df_plot['tot']\n",
    "    # Ratio broken?\n",
    "    df_plot.hist(bins=np.arange(0, 10), ax=ax[1])\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualizations Debate Responses\n",
    "- Word scattertext of the responses in debate_train\n",
    "- Lime and anchor visualization of an example sentence, using **only_response** (rest default options)\n",
    "    - model has accuracy 53% (quite bad)\n",
    "    - details about LIME [here](https://github.com/marcotcr/lime)\n",
    "    - details about ANCHOR [here](https://github.com/marcotcr/anchor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scattertext of the responses in debate_train\n",
    "# No special \"attacking\" or \"supporting\" words easily recognizable\n",
    "# The words are either topic specific, e.g. China (in topic Chinaonechildpolicy there are more supports than attacks)\n",
    "# Or they seem to be there by chance (small dataset), e.g. he, does\n",
    "IFrame(src='./scattertext_attack_supportdebate_train.html', width=950, height=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lime Visualization\n",
    "# Some of the words play an influence as expected, e.g. are and not (attack), play, and alcohol (support)\n",
    "# Others do not play the expected influence, e.g. china (attack and not support as expected)\n",
    "# Overall, all weights are really small and the removal/replacement with UNK of a single word \n",
    "# does not change the prediction\n",
    "HTML(filename='./lime.html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Anchor Visualization\n",
    "# Anchor did not find a way to change some words, and then to predict the other class\n",
    "HTML(filename='./anchor.html')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualizations Political Authors\n",
    "- WordClouds authors\n",
    "- Scattertext authors\n",
    "- Lime and Anchor, **only_org** (rest default), attack/support\n",
    "    - Model acc: 70%, F1: 70%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wordclouds for kennedy and for nixon\n",
    "# Both often say the name of the other candidate, Nixon talks about Predisdent Eisenhower\n",
    "fig, (ax1,ax2) = plt.subplots(1,2, figsize=(12,10))  # 1 row, 2 columns\n",
    "\n",
    "stopwords = set(STOPWORDS)  # set(STOPWORDS)\n",
    "wordcloud = WordCloud(\n",
    "    stopwords=stopwords).generate(\n",
    "    \" \".join(text for text in data_nix_ken.loc[data_nix_ken[\"author\"] == 'Nixon', 'text']))\n",
    "ax1.imshow(wordcloud, interpolation=\"bilinear\")\n",
    "ax1.set_title(\"Nixon WordCloud\")\n",
    "ax1.set_axis_off()\n",
    "wordcloud = WordCloud(\n",
    "    stopwords=stopwords).generate(\n",
    "    \" \".join(text for text in data_nix_ken.loc[data_nix_ken[\"author\"] == 'Kennedy', 'text']))\n",
    "ax2.imshow(wordcloud, interpolation=\"bilinear\")\n",
    "ax2.set_title(\"Kennedy WordCloud\")\n",
    "ax2.set_axis_off()\n",
    "\n",
    "\n",
    "plt.tight_layout()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scattertext\n",
    "# Scattertext of the authors in political\n",
    "# The word usage of Nixon and Kennedy is quite different\n",
    "IFrame(src='./scattertext_nixon_kennedy.html', width=950, height=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lime\n",
    "# All words have a very small impact\n",
    "HTML(filename='./lime_pol.html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Anchors\n",
    "# No rule found\n",
    "HTML(filename='./anchor_pol.html')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baselines\n",
    "- TODO: for the grouped results, actually calculate the weighted average/baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Major Class\n",
    "def get_major_acc(x, classes=['unrelated', 'attack/disagreement', 'support/agreement']):\n",
    "    return np.divide(x[classes].max(), np.sum(x[classes]))\n",
    "\n",
    "def get_major_class(x, classes=['unrelated', 'attack/disagreement', 'support/agreement']):\n",
    "    return x[classes].astype('float64').idxmax()\n",
    "\n",
    "data_stats_total['major_acc'] = data_stats_total.apply(get_major_acc, axis=1)\n",
    "data_stats_total['major_class'] = data_stats_total.apply(get_major_class, axis=1)\n",
    "\n",
    "data_stats_total.loc[data_stats_total['dataset'].isin(['debate_test', 'political'])][['dataset', 'major_class', 'major_acc']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Major Class per Topic node\n",
    "data = data_stats_topic['debate_test']\n",
    "data['major_acc'] = data.apply(get_major_acc, args=[['attack','support']], axis=1)\n",
    "data['major_class'] = data.apply(get_major_class, args=[['attack','support']], axis=1)\n",
    "data[['topic', 'major_class', 'major_acc', 'tot']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Major Class per Topic political\n",
    "data = data_stats_topic['political']\n",
    "data['major_acc'] = data.apply(get_major_acc, args=[['attack','support', 'unrelated']], axis=1)\n",
    "data['major_class'] = data.apply(get_major_class, args=[['attack','support', 'unrelated']], axis=1)\n",
    "data[['topic', 'major_class', 'major_acc', 'tot']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Major Class per Topic political attack/support only\n",
    "data = data_stats_topic['political']\n",
    "data['major_acc'] = data.apply(get_major_acc, args=[['attack','support']], axis=1)\n",
    "data['major_class'] = data.apply(get_major_class, args=[['attack','support']], axis=1)\n",
    "data[['topic', 'major_class', 'major_acc', 'tot']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Major Class Author identified \n",
    "data = data_stats_author.copy()\n",
    "data['major_acc'] = data.apply(get_major_acc, args=[['attack','support', 'unrelated']], axis=1)\n",
    "data['major_class'] = data.apply(get_major_class, args=[['attack','support', 'unrelated']], axis=1)\n",
    "data[['author_resp', 'author_org', 'major_class', 'major_acc', 'tot']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Major Class Author identified attack/support only\n",
    "data = data_stats_author.copy()\n",
    "data['major_acc'] = data.apply(get_major_acc, args=[['attack','support']], axis=1)\n",
    "data['major_class'] = data.apply(get_major_class, args=[['attack','support']], axis=1)\n",
    "data[['author_resp', 'author_org', 'major_class', 'major_acc', 'tot']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merged to same author / different author\n",
    "# Very high accuracy possible if only detected if it is the same or a different author\n",
    "data = data_stats_author.iloc[:-1].copy()\n",
    "data['authors'] = data.apply(lambda r: 'Same' if r['author_resp'] == r['author_org'] else 'Different', axis=1)\n",
    "data = data.groupby('authors').sum()\n",
    "data = data.reset_index()\n",
    "data['major_acc'] = data.apply(get_major_acc, args=[['attack','support']], axis=1)\n",
    "data['major_class'] = data.apply(get_major_class, args=[['attack','support']], axis=1)\n",
    "\n",
    "data[['authors', 'major_class', 'major_acc', 'tot']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sentiment Analysis (nltk vader)\n",
    "\n",
    "# Only responses debate test, supporting arguments often have a positive sentiment\n",
    "# Attacking arguments have nothing special\n",
    "pd.concat((data_stats_sent['respdebate_test'],data_stats_sent['resppolitical']), keys=['node', 'political'], sort=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Both org and response\n",
    "# Attack often have different sentiment, support often have the same sentiment (node)\n",
    "# Nothing meaningful for political\n",
    "pd.concat((data_stats_sent['bothdebate_test'],data_stats_sent['bothpolitical']), keys=['node', 'political'], sort=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# .... ?\n",
    "# Major Class for every Org argument\n",
    "# Major Class for every Resp argument (only political)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NoDE paper\n",
    "![](https://i.imgur.com/1N94Gjq.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Node Acc with different parameters\n",
    "\n",
    "# Fixed: input=both, seq_len=128, warmup_prop=0.1, seed=42\n",
    "# Tested: model=base-uncased,large-uncased, epochs=3,4,5, batch_size=8,12,16, lr=2e-5, 3e-5, 5e-5\n",
    "# Gradient accumulation: batch_size/4 for bert_large \n",
    "# (in principle equivalent, in practice different because of rounding errors etc.)\n",
    "eval_results = pd.read_csv('../pytorch/node_both/eval_results.tsv', sep='\\t')\n",
    "\n",
    "# Some stats: mean, min, max, std\n",
    "# Paper acc 0.67, best bert acc 0.74, mean (bert-base) 0.62 , baselines ~0.6\n",
    "print(eval_results['acc'].agg([np.mean, np.min, np.max, np.std]))\n",
    "# Somehow bert-large performs worse than bert-base\n",
    "print(eval_results.groupby('_bert-model')['acc'].agg([np.mean, np.min, np.max, np.std])) \n",
    "print()\n",
    "# Print settings of best result\n",
    "print(eval_results.iloc[eval_results['acc'].idxmax()])\n",
    "\n",
    "# Show the table\n",
    "eval_results.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Political Paper\n",
    "![](https://i.imgur.com/yGlTYbd.png)\n",
    "![](https://i.imgur.com/7yrDqQH.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Political F1 CrossVal\n",
    "# Fixed: input=both, seq_len=256, warmup_prop=0.1, seed=42\n",
    "# model=base-uncased, epochs=5, batch_size=12, lr=2e-5\n",
    "\n",
    "# Related/Unrelated\n",
    "eval_results = pd.read_csv('../pytorch/pol_ru/eval_results.tsv', sep='\\t')\n",
    "\n",
    "# Some stats: mean, min, max, std\n",
    "# Paper average F1 0.65, here average F1 0.68, baseline ?\n",
    "print(eval_results['f1'].agg([np.mean, np.min, np.max, np.std]))\n",
    "eval_results.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Political F1 CrossVal\n",
    "# Fixed: input=both, seq_len=256, warmup_prop=0.1, seed=42\n",
    "# model=base-uncased, epochs=5, batch_size=12, lr=2e-5\n",
    "\n",
    "# Attack/Support\n",
    "eval_results = pd.read_csv('../pytorch/pol_as/eval_results.tsv', sep='\\t')\n",
    "\n",
    "# Some stats: mean, min, max, std\n",
    "# Paper average F1 0.82, here average F1 0.73, baselines (author) ~0.85\n",
    "print(eval_results['f1'].agg([np.mean, np.min, np.max, np.std]))\n",
    "eval_results.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Political F1 CrossVal\n",
    "# Fixed: input=both, seq_len=256, warmup_prop=0.1, seed=42\n",
    "# model=base-uncased, epochs=5, batch_size=12, lr=2e-5\n",
    "\n",
    "# Attack/Support/Unrelated\n",
    "eval_results = pd.read_csv('../pytorch/pol_asu/eval_results.tsv', sep='\\t')\n",
    "\n",
    "# Some stats: mean, min, max, std\n",
    "# Paper only reported precision 0.57, here only major class classified\n",
    "# Use some tricks to coope with class imbalance!\n",
    "print(eval_results['f1'].agg([np.mean, np.min, np.max, np.std]))\n",
    "eval_results.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agreement Paper\n",
    "- Accuracy 74%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Agreement F1 CrossVal\n",
    "# Comparison with Paper + Baselines\n",
    "# Fixed: input=both, seq_len=256, warmup_prop=0.1, seed=42\n",
    "# model=base-uncased, epochs=2, batch_size=12, lr=2e-5\n",
    "\n",
    "# Agreement/Disagreement\n",
    "eval_results = pd.read_csv('../pytorch/agreement/eval_results.tsv', sep='\\t')\n",
    "# Some stats: mean, min, max, std\n",
    "# Paper average acc 0.74 , here average acc 0.61\n",
    "# TODO: non cross_val version had acc ~0.97! Probably parameters are bad, 2 Epochs might not be enough \n",
    "# (try again with higher epochs number)\n",
    "print(eval_results['acc'].agg([np.mean, np.min, np.max, np.std]))\n",
    "eval_results.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results analyzed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the train/test splits functions\n",
    "import sys \n",
    "import os\n",
    "# TODO: use a relative path or a module instead\n",
    "sys.path.append(os.path.abspath(\"/media/jannis/GeDaTS/SS19/BA/Code_BA/code_relation_prediction/pytorch\"))\n",
    "\n",
    "from run_classifier_dataset_utils import processors\n",
    "\n",
    "node_pro = processors['node']('both')\n",
    "political_as_pro = processors['political-as']('both')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Node results with respect to topic\n",
    "_, node_test_df = node_pro.get_dev_examples('../data')\n",
    "eval_preds = pd.read_csv('../pytorch/node_both/eval_preds.csv')\n",
    "\n",
    "# Only predictions from bert-base\n",
    "res = pd.concat([node_test_df.reset_index(drop=True), eval_preds.iloc[27:,:-1].transpose().reset_index(drop=True)], axis=1)\n",
    "res = res.replace({0: 'attack', 1: 'support'})\n",
    "\n",
    "# For now, only one run (run 51) used\n",
    "# There are errors in every topic, no clear trend visible that some topics are better or worse\n",
    "# More false classifications of attack than of support (support is the major class)\n",
    "# Could, also look at several runs, or average, etc.\n",
    "pd.crosstab(res['topic'], [res['label'],res[51]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take the rounded mean prediction for all bert-base runs \n",
    "res['mean_round'] = eval_preds.iloc[27:,:-1].mean().round().values\n",
    "res = res.replace({0: 'attack', 1: 'support'})\n",
    "pd.crosstab(res['topic'], [res['label'],res['mean_round']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can recreate all metrics from the available data\n",
    "# E.g. classification reports or confusion matrices \n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "print(classification_report(y_pred=eval_preds.iloc[51,:-1].replace({0: 'attack', 1: 'support'}), y_true=res['label']))\n",
    "\n",
    "print(confusion_matrix(res['label'], eval_preds.iloc[51,:-1].replace({0: 'attack', 1: 'support'})))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Political results with respect to topic\n",
    "splits_data = political_as_pro.get_splits('../data')\n",
    "\n",
    "# Get the test data and the test predictions\n",
    "pol_test_df = pd.concat(np.array(splits_data)[:,3])\n",
    "eval_preds = pd.read_csv('../pytorch/pol_as/eval_preds.csv')\n",
    "\n",
    "\n",
    "pol_test_df['preds'] =  eval_preds.iloc[:,:-1].stack().values\n",
    "pol_test_df = pol_test_df.replace({0: 'attack', 1: 'support'})\n",
    "\n",
    "\n",
    "pd.crosstab(pol_test_df['topic'], [pol_test_df['label'],pol_test_df['preds']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Political results with respect to author\n",
    "pd.crosstab(pol_test_df['preds'], [pol_test_df['org_stance'],pol_test_df['response_stance']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Complete results political (all folds \"summed\")\n",
    "print(classification_report(y_pred=pol_test_df['preds'], y_true=pol_test_df['label']))\n",
    "\n",
    "print(confusion_matrix(y_pred=pol_test_df['preds'], y_true=pol_test_df['label']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Results with respect to same org, same resp (always gets the same label or not?)\n",
    "\n",
    "# Same org\n",
    "# One org does not always get the same prediction (but often)\n",
    "pd.crosstab(res['org'], res[51])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Same org pol\n",
    "# TODO: aggregate to get some useful insights \n",
    "# (and maybe do it for every fold individually, \n",
    "# because otherwise it could be that we always predict one label for one org in one fold and another in another fold)\n",
    "pd.crosstab(pol_test_df['org'], pol_test_df['preds']).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Same resp pol\n",
    "pd.crosstab(pol_test_df['response'], pol_test_df['preds']).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results other inputs\n",
    "- TODO: test with only the orgs as input and with only the response as input\n",
    "   - Arguments likely to be attacked/supported\n",
    "   - Attackful/ing or supportful/ing arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only org"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only resp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Domain adaptation etc.\n",
    "- TODO: do some domain adaptation etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train on one dataset, evaluate on another (without finetuning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# With finetuning (reusing the classification layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# With finetuning + use a new classification layer "
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Result Showcase\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Necessary imports\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from IPython.display import IFrame, HTML\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "\n",
    "# Import the train/test splits functions\n",
    "import sys \n",
    "import os\n",
    "# TODO: use a relative path or a module instead\n",
    "sys.path.append(os.path.abspath(\"../pytorch\"))\n",
    "\n",
    "from run_classifier_dataset_utils import processors\n",
    "\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "from sklearn.metrics import classification_report\n",
    "import nltk\n",
    "nltk.download('vader_lexicon')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read all necessary data\n",
    "df = pd.read_csv('../data/complete_data.tsv', sep='\\t')\n",
    "data_stats_org = np.load('../data/stats/data_stats_org.npy', allow_pickle=True).item()\n",
    "data_stats_resp = np.load('../data/stats/data_stats_resp.npy', allow_pickle=True).item()\n",
    "data_stats_topic = np.load('../data/stats/data_stats_topic.npy', allow_pickle=True).item()\n",
    "data_stats_sent = np.load('../data/stats/sent_stats.npy', allow_pickle=True).item()\n",
    "data_stats_author = pd.read_csv('../data/stats/data_stats_author.tsv', sep='\\t')\n",
    "data_stats_total = pd.read_csv('../data/stats/data_stats_total.tsv', sep='\\t')\n",
    "data_nix_ken = pd.read_csv('../data/stats/data_nix_ken.tsv', sep='\\t')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data overview\n",
    "- NoDE\n",
    "    - Link: http://www-sop.inria.fr/NoDE/NoDE-xml.html\n",
    "    - Paper: https://pdfs.semanticscholar.org/16d1/6b8a37c5313fa8c8430fddc011f2a98d20c5.pdf\n",
    "- Political\n",
    "    - Link: https://dh.fbk.eu/resources/political-argumentation\n",
    "    - Paper: https://aaai.org/ocs/index.php/AAAI/AAAI18/paper/view/16393/16020\n",
    "- Agreement\n",
    "    - Link: https://dh.fbk.eu/resources/agreement-disagreement\n",
    "    - Paper: https://www.aclweb.org/anthology/C16-1232"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# General stats of all datasets\n",
    "# Number of unique arguments, number of total pairs, number of attacks/supports und unrelated pairs\n",
    "# Statistics about the combined length (org+response) of the pairs\n",
    "# Important: debate_test/train is already repaired, but still not the same as in the paper\n",
    "# Important: agreement had many rows which could not get parsed, e.g. because resp or org was empty, they were excluded\n",
    "# And the dataset is smaller than reported in the paper\n",
    "# Important: There are two duplicates in the political dataset\n",
    "# Length important for the seq_len parameter of BERT\n",
    "data_stats_total.loc[data_stats_total['Dataset'].isin(['debate_test', 'debate_train', 'procon', 'political', 'agreement'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Debate train/test by topic\n",
    "# Topics that are not matching paper are Interentaccess and Militaryservice\n",
    "# Most topics attack/support distributions are similar to the overall distribution\n",
    "pd.concat((data_stats_topic['debate_train'], data_stats_topic['debate_test']), keys=['train', 'test'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Political by topic\n",
    "# Most topics have a similar distribution, minimum wage is an exception\n",
    "data_stats_topic['political']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Political by author\n",
    "# Same author mostly support each other\n",
    "# Different authors mostly attack each other\n",
    "# Dataset is heavily imbalanced in respect to the author, Kennedy occurs way more often\n",
    "print(data_nix_ken.groupby(\"author\").nunique())\n",
    "data_stats_author.iloc[:,:-3].to_csv('../data/thesis/author_imbalance.csv', index=False)\n",
    "data_stats_author.iloc[:,:-3].style.background_gradient(cmap='Blues')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Political duplicates\n",
    "for data_set in ['political']:\n",
    "    print(data_set + \" Duplicates:\")\n",
    "    df_check = df[df['org_dataset'] == data_set]\n",
    "    print(df_check[df_check.duplicated(subset=['org', 'response'], keep=False)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot distribution of length of org, resp and combined over the different datasets\n",
    "# Seq_len 128/200 ~75% of debate_dataset, 250 ~75% political_dataset\n",
    "fig, (ax1,ax2) = plt.subplots(1,2, figsize=(10,4))  # 1 row, 2 columns\n",
    "\n",
    "for data_set, ax in [('debate_extended', ax1), ('political',ax2)]:\n",
    "    df_plot = df[df['org_dataset'] == data_set]\n",
    "    df_plot.boxplot(ax=ax)\n",
    "    ax.set_title(data_set)\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot for thesis\n",
    "fig, ax = plt.subplots(1,2, figsize=(10,4), sharey=True)  # 1 row, 2 columns\n",
    "\n",
    "#for data_set, ax in [('debate_train', ax1),('debate_test', ax2)]:\n",
    "df_plot = pd.concat((df[df['org_dataset'] == 'debate_train'], df[df['org_dataset'] == 'debate_test']))\n",
    "df_plot = df_plot.rename(columns={'org_len': 'Original', 'response_len': 'Response', 'complete_len': 'Combined'})\n",
    "df_plot = df_plot.replace({'debate_test': 'Test', 'debate_train': 'Train'})\n",
    "df_plot.groupby('org_dataset', sort=False).boxplot(ax=ax)\n",
    "#ax.set_title(data_set)\n",
    "ax[0].set_ylabel('Length in WordPiece tokens')\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.savefig('../data/thesis/node_length.pdf', bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot for thesis\n",
    "fig, ax = plt.subplots(1,1, figsize=(5,4), sharey=True)  # 1 row, 2 columns\n",
    "\n",
    "#for data_set, ax in [('debate_train', ax1),('debate_test', ax2)]:\n",
    "df_plot = df[df['org_dataset'] == 'political']\n",
    "df_plot = df_plot.rename(columns={'org_len': 'Original', 'response_len': 'Response', 'complete_len': 'Combined'})\n",
    "df_plot.groupby('org_dataset', sort=False).boxplot(ax=ax)\n",
    "ax.set_title(\"\")\n",
    "ax.set_ylabel('Length in WordPiece tokens')\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.savefig('../data/thesis/political_length.pdf', bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot for thesis\n",
    "fig, ax = plt.subplots(1,1, figsize=(5,4), sharey=True)  # 1 row, 2 columns\n",
    "\n",
    "#for data_set, ax in [('debate_train', ax1),('debate_test', ax2)]:\n",
    "df_plot = df[df['org_dataset'] == 'agreement']\n",
    "df_plot = df_plot.rename(columns={'org_len': 'Argument 1', 'response_len': 'Argument 2', 'complete_len': 'Combined'})\n",
    "df_plot.groupby('org_dataset', sort=False).boxplot(ax=ax)\n",
    "ax.set_title(\"\")\n",
    "ax.set_ylabel('Length in WordPiece tokens')\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.savefig('../data/thesis/agreement_length.pdf', bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenization tests\n",
    "from pytorch_pretrained_bert.tokenization import BertTokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)\n",
    "\n",
    "text = \"Violent games make youth more agressive/violent.\"\n",
    "tokenizer.tokenize(text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Attack/Support ratios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot how many arguments attack an argument (attack-ratio)\n",
    "# Most arguments are only attacked or only supported (interesting for detecting arguments likely to be attacked/supported)\n",
    "# If we disregard every argument, which is only answered to once most arguments have an attack-ratio of 0.5\n",
    "# In the case of the political dataset many arguments are unrelated, and unrelated arguments are disregarded in this plot\n",
    "fig, (ax1,ax2) = plt.subplots(2,2, figsize=(10,4))  # 2 rows, 2 columns\n",
    "for data_set, ax in [('debate_extended', ax1), ('political',ax2)]:\n",
    "    df_plot = data_stats_org[data_set].iloc[:-1].apply(\n",
    "        lambda r: pd.Series({\"Attack-ratio\": r[\"Attacked\"] / r[\"Total pairs\"],\n",
    "                             \"Attack-ratio (exluding arguments only attacked/supported once)\": np.nan if r[\"Total pairs\"] == 1 else r[\"Attacked\"] / r[\"Total pairs\"]}),\n",
    "        axis=1)\n",
    "    # Ratio broken?\n",
    "    df_plot.hist(density=False, ax=ax)\n",
    "    ax[0].set_ylabel(data_set, rotation=0)\n",
    "    \n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Usage of arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First column shows how many answers an argument has\n",
    "# Second column shows how many outgoing links an argument has\n",
    "# Most arguments only have one ingoing link, but some have many ~10 debate, ~30 political\n",
    "# In debate (orginal) every argument only has one outgoing link, in political most have one, but some have many ~8\n",
    "fig, (ax1,ax2,ax3) = plt.subplots(3,2, figsize=(10,4))  # 3 rows, 2 columns\n",
    "\n",
    "for data_set, ax in [('debate_test', ax1), ('debate_extended', ax2), ('political',ax3)]:\n",
    "    df_plot = data_stats_org[data_set].iloc[:-1]\n",
    "    df_plot = df_plot['Total pairs']\n",
    "    # Ratio broken?\n",
    "    df_plot.hist(density=True, ax=ax[0])\n",
    "    ax[0].set_title('{0}, org'.format(data_set))\n",
    "    ax[1].set_title('{0}, resp'.format(data_set))\n",
    "    df_plot = data_stats_resp[data_set].iloc[:-1]\n",
    "    df_plot = df_plot['Total pairs']\n",
    "    # Ratio broken?\n",
    "    df_plot.hist(bins=np.arange(0, 10), ax=ax[1])\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot for thesis\n",
    "fig, (ax1, ax2) = plt.subplots(1,2, figsize=(10,4), sharex=True)  # 1 row, 2 columns\n",
    "\n",
    "data_set = 'debate_test'\n",
    "df_plot = pd.concat((data_stats_org[data_set].iloc[:-1], data_stats_resp[data_set].iloc[:-1].rename(columns={'Attacks': 'Attacked', 'Supports': 'Supported'})), keys=['Original', 'Response'])\n",
    "#print(df_plot)\n",
    "df_plot.loc['Original'].hist(column=['Total pairs'], ax=ax1, bins=[1,2,3,4,5,6,7,8,9])\n",
    "df_plot.loc['Response'].hist(column=['Total pairs'], ax=ax2, bins=[1,2,3,4,5,6,7,8,9])\n",
    "\n",
    "ax1.set_title(\"Original\")\n",
    "ax2.set_title(\"Response\")\n",
    "ax1.set_ylabel(\"Count\")\n",
    "ax1.set_xlabel(\"Number of ingoing links\")\n",
    "ax2.set_xlabel(\"Number of outgoing links\")\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.savefig('../data/thesis/node_hist.pdf', bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot for thesis\n",
    "fig, (ax1, ax2) = plt.subplots(1,2, figsize=(10,4), sharex=False, sharey=True)  # 1 row, 2 columns\n",
    "\n",
    "data_set = 'political'\n",
    "df_plot = pd.concat((data_stats_org[data_set].iloc[:-1], data_stats_resp[data_set].iloc[:-1].rename(columns={'Attacks': 'Attacked', 'Supports': 'Supported'})), keys=['Original', 'Response'])\n",
    "#print(df_plot)\n",
    "df_plot.loc['Original'].hist(column=['Total pairs'], bins=[1,2,3,4,5,6,7,8,9],ax=ax1)\n",
    "df_plot.loc['Response'].hist(column=['Total pairs'], bins=[1,2,3,4,5,6,7,8,9], ax=ax2)\n",
    "\n",
    "ax1.set_title(\"Original\")\n",
    "ax2.set_title(\"Response\")\n",
    "ax1.set_ylabel(\"Count\")\n",
    "ax1.set_xlabel(\"Number of ingoing links\")\n",
    "ax2.set_xlabel(\"Number of outgoing links\")\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.savefig('../data/thesis/political_hist.pdf', bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualizations Debate Responses\n",
    "- Word scattertext of the responses in debate_train\n",
    "- Lime and anchor visualization of an example sentence, using **only_response** (rest default options)\n",
    "    - model has accuracy 53% (quite bad)\n",
    "    - details about LIME [here](https://github.com/marcotcr/lime)\n",
    "    - details about ANCHOR [here](https://github.com/marcotcr/anchor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scattertext of the responses in debate_train\n",
    "# No special \"attacking\" or \"supporting\" words easily recognizable\n",
    "# The words are either topic specific, e.g. China (in topic Chinaonechildpolicy there are more supports than attacks)\n",
    "# Or they seem to be there by chance (small dataset), e.g. he, does\n",
    "#IFrame(src='./scattertext_attack_supportdebate_train.html', width=950, height=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lime Visualization\n",
    "# Some of the words play an influence as expected, e.g. are and not (attack), play, and alcohol (support)\n",
    "# Others do not play the expected influence, e.g. china (attack and not support as expected)\n",
    "# Overall, all weights are really small and the removal/replacement with UNK of a single word \n",
    "# does not change the prediction\n",
    "HTML(filename='./lime.html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Anchor Visualization\n",
    "# Anchor did not find a way to change some words, and then to predict the other class\n",
    "#HTML(filename='./anchor.html')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualizations Political Authors\n",
    "- WordClouds authors\n",
    "- Scattertext authors\n",
    "- Lime and Anchor, **only_org** (rest default), attack/support\n",
    "    - Model acc: 70%, F1: 70%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wordclouds for kennedy and for nixon\n",
    "# Both often say the name of the other candidate, Nixon talks about Predisdent Eisenhower\n",
    "fig, (ax1,ax2) = plt.subplots(1,2, figsize=(12,10))  # 1 row, 2 columns\n",
    "\n",
    "stopwords = set(STOPWORDS)  # set(STOPWORDS)\n",
    "wordcloud = WordCloud(\n",
    "    stopwords=stopwords).generate(\n",
    "    \" \".join(text for text in data_nix_ken.loc[data_nix_ken[\"author\"] == 'Nixon', 'text']))\n",
    "ax1.imshow(wordcloud, interpolation=\"bilinear\")\n",
    "ax1.set_title(\"Nixon WordCloud\")\n",
    "ax1.set_axis_off()\n",
    "wordcloud = WordCloud(\n",
    "    stopwords=stopwords).generate(\n",
    "    \" \".join(text for text in data_nix_ken.loc[data_nix_ken[\"author\"] == 'Kennedy', 'text']))\n",
    "ax2.imshow(wordcloud, interpolation=\"bilinear\")\n",
    "ax2.set_title(\"Kennedy WordCloud\")\n",
    "ax2.set_axis_off()\n",
    "\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../data/thesis/authors_wordcloud.pdf', bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scattertext\n",
    "# Scattertext of the authors in political\n",
    "# The word usage of Nixon and Kennedy is quite different\n",
    "#IFrame(src='./scattertext_nixon_kennedy.html', width=950, height=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lime\n",
    "# All words have a very small impact\n",
    "HTML(filename='./lime_pol.html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Anchors\n",
    "# No rule found\n",
    "#HTML(filename='./anchor_pol.html')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baselines\n",
    "- TODO: for the grouped results, actually calculate the weighted average/baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Major Class\n",
    "def get_major_acc(x, classes=['Unrelated', 'Attack/Disagreement', 'Support/Agreement']):\n",
    "    return np.divide(x[classes].max(), np.sum(x[classes]))\n",
    "\n",
    "def get_major_class(x, classes=['Unrelated', 'Attack/Disagreement', 'Support/Agreement']):\n",
    "    return x[classes].astype('float64').idxmax()\n",
    "\n",
    "data_stats_total['major_acc'] = data_stats_total.apply(get_major_acc, axis=1)\n",
    "data_stats_total['major_class'] = data_stats_total.apply(get_major_class, axis=1)\n",
    "\n",
    "data_stats_total.loc[data_stats_total['Dataset'].isin(['debate_test', 'political'])][['Dataset', 'major_class', 'major_acc']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Major Class per Topic node\n",
    "data = data_stats_topic['debate_test']\n",
    "data['major_acc'] = data.apply(get_major_acc, args=[['Attack','Support']], axis=1)\n",
    "data['major_class'] = data.apply(get_major_class, args=[['Attack','Support']], axis=1)\n",
    "data[['Topic', 'major_class', 'major_acc', 'Total pairs']]\n",
    "data = data.set_index('Topic')\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "node_pro = processors['node']('both')\n",
    "\n",
    "_, node_test_df = node_pro.get_dev_examples('../data')\n",
    "\n",
    "node_test_df['major_topic_pred'] = node_test_df['topic'].apply(lambda r: data.loc[r, 'major_class']).replace({'Attack': 'attack', 'Support': 'support'})\n",
    "print(classification_report(node_test_df['label'], node_test_df['major_topic_pred']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "sid = SentimentIntensityAnalyzer()\n",
    "\n",
    "def disc_pol(x):\n",
    "    if x >= 0.00:\n",
    "        return 'positive'\n",
    "    else:\n",
    "        return 'negative'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "node_test_df['org_polarity'] = node_test_df['org'].apply(lambda r: disc_pol(sid.polarity_scores(r)['compound']))\n",
    "node_test_df['resp_polarity'] = node_test_df['response'].apply(lambda r: disc_pol(sid.polarity_scores(r)['compound']))\n",
    "node_test_df['sent_both_baseline'] = node_test_df.apply(lambda r: 'attack' if r['org_polarity'] != r['resp_polarity'] else 'support', axis=1)\n",
    "node_test_df['sent_resp_baseline'] = node_test_df.apply(lambda r: 'attack' if r['resp_polarity'] == 'negative' else 'support', axis=1)\n",
    "print(classification_report(node_test_df['label'], node_test_df['sent_both_baseline']))\n",
    "print(classification_report(node_test_df['label'], node_test_df['sent_resp_baseline']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Major Class per Topic political RU\n",
    "political_ru_pro = processors['political-ru']('both')\n",
    "splits_data = np.array(political_ru_pro.get_splits('../data'))\n",
    "\n",
    "# Get the test data and the test predictions\n",
    "pol_test_df = pd.concat(splits_data[:,3])\n",
    "\n",
    "data = data_stats_topic['political']\n",
    "data['related'] = data.apply(lambda r: r['Attack'] + r['Support'], axis=1)\n",
    "data['major_acc'] = data.apply(get_major_acc, args=[['related', 'Unrelated']], axis=1)\n",
    "data['major_class'] = data.apply(get_major_class, args=[['related', 'Unrelated']], axis=1)\n",
    "data[['Topic', 'major_class', 'major_acc', 'Total pairs']]\n",
    "data = data.set_index('Topic')\n",
    "pol_test_df['major_topic_pred'] = pol_test_df['topic'].apply(lambda r: data.loc[r, 'major_class']).replace({'Unrelated': 'unrelated'})\n",
    "print(classification_report(pol_test_df['label'], pol_test_df['major_topic_pred']))\n",
    "pol_test_df['major_class'] = 'related'\n",
    "print(classification_report(pol_test_df['label'], pol_test_df['major_class']))\n",
    "\n",
    "def count_values(x, labels):\n",
    "    return x['label'].loc[x['label'].isin(labels)].count()\n",
    "\n",
    "data = pol_test_df.groupby('org').apply(lambda r: pd.Series({'org': r['org'].iloc[0], 'related': count_values(r, ['related']), 'unrelated': count_values(r, ['unrelated'])}))\n",
    "data['major_acc'] = data.apply(get_major_acc, args=[['related', 'unrelated']], axis=1)\n",
    "data['major_class'] = data.apply(get_major_class, args=[['related', 'unrelated']], axis=1)\n",
    "data = data.set_index('org')\n",
    "data['total'] = data['related'] + data['unrelated']\n",
    "orgs_only_once = data.loc[data['total'] == 1].index.to_list()\n",
    "index = pol_test_df[pol_test_df['org'].isin(orgs_only_once)].index\n",
    "pol_test_df = pol_test_df.drop(index)\n",
    "pol_test_df['major_org_pred'] = pol_test_df['org'].apply(lambda r: data.loc[r, 'major_class'])\n",
    "print(classification_report(pol_test_df['label'], pol_test_df['major_org_pred']))\n",
    "splits_data = np.array(political_ru_pro.get_splits('../data'))\n",
    "\n",
    "# Get the test data and the test predictions\n",
    "pol_test_df = pd.concat(splits_data[:,3])\n",
    "data = pol_test_df.groupby('response').apply(lambda r: pd.Series({'resp': r['response'].iloc[0], 'related': count_values(r, ['related']), 'unrelated': count_values(r, ['unrelated'])}))\n",
    "data['major_acc'] = data.apply(get_major_acc, args=[['related', 'unrelated']], axis=1)\n",
    "data['major_class'] = data.apply(get_major_class, args=[['related', 'unrelated']], axis=1)\n",
    "data = data.set_index('resp')\n",
    "data['total'] = data['related'] + data['unrelated']\n",
    "resps_only_once = data.loc[data['total'] == 1].index.to_list()\n",
    "index = pol_test_df[pol_test_df['response'].isin(resps_only_once)].index\n",
    "pol_test_df = pol_test_df.drop(index)\n",
    "pol_test_df['major_resp_pred'] = pol_test_df['response'].apply(lambda r: data.loc[r, 'major_class'])\n",
    "print(classification_report(pol_test_df['label'], pol_test_df['major_resp_pred']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Major Class Author identified \n",
    "splits_data = np.array(political_ru_pro.get_splits('../data'))\n",
    "\n",
    "# Get the test data and the test predictions\n",
    "pol_test_df = pd.concat(splits_data[:,3])\n",
    "\n",
    "data = data_stats_author.copy()\n",
    "data['related'] = data.apply(lambda r: r['Attack'] + r['Support'], axis=1)\n",
    "data['major_acc'] = data.apply(get_major_acc, args=[['related', 'Unrelated']], axis=1)\n",
    "data['major_class'] = data.apply(get_major_class, args=[['related', 'Unrelated']], axis=1)\n",
    "data[['Author resp', 'Author org', 'major_class', 'major_acc', 'Total pairs']]\n",
    "data = data.set_index(['Author resp', 'Author org'])\n",
    "pol_test_df['major_author'] = pol_test_df.apply(lambda r: data.loc[r['response_stance'], r['org_stance']]['major_class'], axis=1).replace({'Unrelated': 'unrelated'})\n",
    "print(classification_report(pol_test_df['label'], pol_test_df['major_author']))\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Major Class per Topic political attack/support only\n",
    "political_as_pro = processors['political-as']('both')\n",
    "splits_data = np.array(political_as_pro.get_splits('../data'))\n",
    "\n",
    "# Get the test data and the test predictions\n",
    "pol_test_df = pd.concat(splits_data[:,3])\n",
    "data = data_stats_topic['political']\n",
    "data['major_acc'] = data.apply(get_major_acc, args=[['Attack','Support']], axis=1)\n",
    "data['major_class'] = data.apply(get_major_class, args=[['Attack','Support']], axis=1)\n",
    "data[['Topic', 'major_class', 'major_acc', 'Total pairs']]\n",
    "data = data.set_index('Topic')\n",
    "pol_test_df['major_topic_pred'] = pol_test_df['topic'].apply(lambda r: data.loc[r, 'major_class']).replace({'Attack': 'attack', 'Support': 'support'})\n",
    "print(classification_report(pol_test_df['label'], pol_test_df['major_topic_pred']))\n",
    "pol_test_df['major_class'] = 'attack'\n",
    "print(classification_report(pol_test_df['label'], pol_test_df['major_class']))\n",
    "pol_test_df['org_polarity'] = pol_test_df['org'].apply(lambda r: disc_pol(sid.polarity_scores(r)['compound']))\n",
    "pol_test_df['resp_polarity'] = pol_test_df['response'].apply(lambda r: disc_pol(sid.polarity_scores(r)['compound']))\n",
    "pol_test_df['sent_both_baseline'] = pol_test_df.apply(lambda r: 'attack' if r['org_polarity'] != r['resp_polarity'] else 'support', axis=1)\n",
    "pol_test_df['sent_resp_baseline'] =pol_test_df.apply(lambda r: 'attack' if r['resp_polarity'] == 'negative' else 'support', axis=1)\n",
    "print(classification_report(pol_test_df['label'], pol_test_df['sent_both_baseline']))\n",
    "print(classification_report(pol_test_df['label'], pol_test_df['sent_resp_baseline']))\n",
    "\n",
    "data = pol_test_df.groupby('org').apply(lambda r: pd.Series({'org': r['org'].iloc[0], 'attack': count_values(r, ['attack']), 'support': count_values(r, ['support'])}))\n",
    "data['major_acc'] = data.apply(get_major_acc, args=[['attack', 'support']], axis=1)\n",
    "data['major_class'] = data.apply(get_major_class, args=[['attack', 'support']], axis=1)\n",
    "data = data.set_index('org')\n",
    "data['total'] = data['attack'] + data['support']\n",
    "orgs_only_once = data.loc[data['total'] == 1].index.to_list()\n",
    "index = pol_test_df[pol_test_df['org'].isin(orgs_only_once)].index\n",
    "pol_test_df = pol_test_df.drop(index)\n",
    "pol_test_df['major_org_pred'] = pol_test_df['org'].apply(lambda r: data.loc[r, 'major_class'])\n",
    "print(classification_report(pol_test_df['label'], pol_test_df['major_org_pred']))\n",
    "\n",
    "splits_data = np.array(political_as_pro.get_splits('../data'))\n",
    "\n",
    "# Get the test data and the test predictions\n",
    "pol_test_df = pd.concat(splits_data[:,3])\n",
    "data = pol_test_df.groupby('response').apply(lambda r: pd.Series({'resp': r['response'].iloc[0], 'attack': count_values(r, ['attack']), 'support': count_values(r, ['support'])}))\n",
    "data['major_acc'] = data.apply(get_major_acc, args=[['attack', 'support']], axis=1)\n",
    "data['major_class'] = data.apply(get_major_class, args=[['attack', 'support']], axis=1)\n",
    "data = data.set_index('resp')\n",
    "data['total'] = data['attack'] + data['support']\n",
    "resps_only_once = data.loc[data['total'] == 1].index.to_list()\n",
    "index = pol_test_df[pol_test_df['response'].isin(resps_only_once)].index\n",
    "pol_test_df = pol_test_df.drop(index)\n",
    "pol_test_df['major_resp_pred'] = pol_test_df['response'].apply(lambda r: data.loc[r, 'major_class'])\n",
    "print(classification_report(pol_test_df['label'], pol_test_df['major_resp_pred']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data_stats_author.copy()\n",
    "data['major_acc'] = data.apply(get_major_acc, args=[['Attack', 'Support']], axis=1)\n",
    "data['major_class'] = data.apply(get_major_class, args=[['Attack', 'Support']], axis=1)\n",
    "data[['Author resp', 'Author org', 'major_class', 'major_acc', 'Total pairs']]\n",
    "data = data.set_index(['Author resp', 'Author org'])\n",
    "pol_test_df['major_author'] = pol_test_df.apply(lambda r: data.loc[r['response_stance'], r['org_stance']]['major_class'], axis=1).replace({'Attack': 'attack', 'Support': 'support'})\n",
    "print(classification_report(pol_test_df['label'], pol_test_df['major_author']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merged to same author / different author\n",
    "# Very high accuracy possible if only detected if it is the same or a different author\n",
    "data = data_stats_author.iloc[:-1].copy()\n",
    "data['authors'] = data.apply(lambda r: 'Same' if r['Author resp'] == r['Author org'] else 'Different', axis=1)\n",
    "data = data.groupby('authors').sum()\n",
    "data = data.reset_index()\n",
    "data['major_acc'] = data.apply(get_major_acc, args=[['Attack','Support']], axis=1)\n",
    "data['major_class'] = data.apply(get_major_class, args=[['Attack','Support']], axis=1)\n",
    "data[['authors', 'major_acc', 'Total pairs']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Both org and response\n",
    "# Attack often have different sentiment, support often have the same sentiment (node)\n",
    "# Nothing meaningful for political\n",
    "pd.concat((data_stats_sent['bothdebate_test'],data_stats_sent['bothpolitical']), keys=['node', 'political'], sort=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sentiment Analysis (nltk vader)\n",
    "\n",
    "# Only responses debate test, supporting arguments often have a positive sentiment\n",
    "# Attacking arguments have nothing special\n",
    "pd.concat((data_stats_sent['respdebate_test'],data_stats_sent['resppolitical']), keys=['node', 'political'], sort=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# .... ?\n",
    "# Major Class for every Org argument\n",
    "# Major Class for every Resp argument (only political)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Major class agreement\n",
    "data = data_stats_topic['agreement']\n",
    "data['major_acc'] = data.apply(get_major_acc, args=[['Agreement','Disagreement']], axis=1)\n",
    "data['major_class'] = data.apply(get_major_class, args=[['Agreement','Disagreement']], axis=1)\n",
    "data[['Topic', 'major_class', 'major_acc', 'Total pairs']]\n",
    "data = data.set_index('Topic')\n",
    "agreement_pro = processors['agreement']('both')\n",
    "splits_data = np.array(agreement_pro.get_splits('../data'))\n",
    "\n",
    "# Get the test data and the test predictions\n",
    "ag_test_df = pd.concat(splits_data[:,3])\n",
    "ag_test_df['major_topic_pred'] = ag_test_df['topic'].apply(lambda r: data.loc[r, 'major_class']).replace({'Agreement': 'agreement', 'Disagreement': 'disagreement'})\n",
    "print(classification_report(ag_test_df['label'], ag_test_df['major_topic_pred']))\n",
    "ag_test_df['major'] = 'disagreement'\n",
    "print(classification_report(ag_test_df['label'], ag_test_df['major']))\n",
    "\n",
    "ag_test_df['org_polarity'] = ag_test_df['org'].apply(lambda r: disc_pol(sid.polarity_scores(r)['compound']))\n",
    "ag_test_df['resp_polarity'] = ag_test_df['response'].apply(lambda r: disc_pol(sid.polarity_scores(r)['compound']))\n",
    "ag_test_df['sent_both_baseline'] = ag_test_df.apply(lambda r: 'disagreement' if r['org_polarity'] != r['resp_polarity'] else 'agreement', axis=1)\n",
    "ag_test_df['sent_resp_baseline'] = ag_test_df.apply(lambda r: 'disagreement' if r['resp_polarity'] == 'negative' else 'agreement', axis=1)\n",
    "print(classification_report(ag_test_df['label'], ag_test_df['sent_both_baseline']))\n",
    "print(classification_report(ag_test_df['label'], ag_test_df['sent_resp_baseline']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NoDE paper\n",
    "![](https://i.imgur.com/1N94Gjq.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Node Acc with different parameters\n",
    "\n",
    "# Fixed: input=both, seq_len=128, warmup_prop=0.1, seed=42\n",
    "# Tested: model=base-uncased,large-uncased, epochs=3,4,5, batch_size=8,12,16, lr=2e-5, 3e-5, 5e-5\n",
    "# Gradient accumulation: batch_size/4 for bert_large \n",
    "# (in principle equivalent, in practice different because of rounding errors etc.)\n",
    "eval_results = pd.read_csv('../pytorch/res/node_both_paper/eval_results.tsv', sep='\\t')\n",
    "\n",
    "# Some stats: mean, min, max, std\n",
    "# Paper acc 0.67, best bert acc 0.74, mean (bert-base) 0.62 , baselines ~0.6\n",
    "print(eval_results['acc'].agg([np.mean, np.min, np.max, np.std]))\n",
    "# Somehow bert-large performs worse than bert-base\n",
    "print(eval_results.groupby('_bert-model')['acc'].agg([np.mean, np.min, np.max, np.std])) \n",
    "print()\n",
    "# Print settings of best result\n",
    "print(eval_results.iloc[eval_results['acc'].idxmax()])\n",
    "\n",
    "eval_results_grouped = eval_results.groupby(['_bert-model', '_num_epochs', '_batch_size','_gradient_acc' ,'_learning_rate' ])\n",
    "results = eval_results_grouped['acc'].agg([np.mean, np.min, np.max, np.std, np.median])\n",
    "# Ste = std/sqrt(count(scores)) # every parameter setting was tested 10 times\n",
    "results['ste'] = results['std']/np.sqrt(10)\n",
    "# Confidence intervals (assumes Gaussian distribution) 0.95\n",
    "results['interval'] = results['ste'] * 1.96\n",
    "results['upper'] = results['mean'] + results['interval']\n",
    "results['lower'] = results['mean'] - results['interval']\n",
    "\n",
    "\n",
    "print(\"best mean\", results.loc[results['mean'].idxmax()]) # Best mean result\n",
    "print(\"best upper\", results.loc[results['upper'].idxmax()]) # Best upper conf bound\n",
    "print(\"best median\", results.loc[results['median'].idxmax()]) # Best median result\n",
    "print(\"best worse\", results.loc[results['amin'].idxmax()]) # Best worse result\n",
    "print(\"best run\", results.loc[results['amax'].idxmax()]) # Best overall results (one run)\n",
    "print(\"smallest std\", results.loc[results['std'].idxmin()]) # Result with smallest std\n",
    "\n",
    "\n",
    "\n",
    "# Show the table\n",
    "results = results.reset_index()\n",
    "results = results.rename(columns={element: element.replace(\"_\", \"-\") for element in results.columns.tolist()})\n",
    "results = results.rename(columns={element: re.sub(\"^-\", \"\", element) for element in results.columns.tolist()})\n",
    "results = results.replace({\"bert-base-uncased\": 'bbu', \"bert-large-uncased\": 'blu'})\n",
    "results['batch-size'] = results['batch-size'] * results['gradient-acc']\n",
    "results = results.drop(columns=[\"gradient-acc\"])\n",
    "print(results.columns)\n",
    "results.iloc[:,:9].to_csv('../data/thesis/node_all_results.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "node_pro = processors['node']('both')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, node_test_df = node_pro.get_dev_examples('../data')\n",
    "eval_preds = pd.read_csv('../pytorch/res/node_both_paper/eval_preds.csv')\n",
    "\n",
    "results = eval_results_grouped['acc'].agg([np.mean, np.min, np.max, np.std, np.median])\n",
    "bmodel, bepochs, bb, bga, blr = results.loc[results['mean'].idxmax()].name\n",
    "print(bmodel, bepochs, bb, blr)\n",
    "best_pred_ps = eval_results.loc[(eval_results['_bert-model'] == bmodel) & \n",
    "                       (eval_results['_num_epochs'] == bepochs) & (eval_results['_batch_size'] == bb) &\n",
    "                       (eval_results['_learning_rate'] == blr)].index\n",
    "print(best_pred_ps)\n",
    "\n",
    "# Only predictions from best setting \n",
    "res = pd.concat([node_test_df.reset_index(drop=True), eval_preds.iloc[best_pred_ps,:-1].transpose().reset_index(drop=True)], axis=1)\n",
    "res = res.replace({0: 'attack', 1: 'support'})\n",
    "re_pre_dict = {'Precision support' : [], 'Precision attack': [], 'Recall support': [], 'Recall attack': []}\n",
    "for ind in best_pred_ps:\n",
    "    pre, rec, _, _ = precision_recall_fscore_support(res['label'], res[ind], labels=['support', 'attack'])\n",
    "    re_pre_dict['Precision support'].append(pre[0])\n",
    "    re_pre_dict['Precision attack'].append(pre[1])\n",
    "    re_pre_dict['Recall support'].append(rec[0])\n",
    "    re_pre_dict['Recall attack'].append(rec[1])\n",
    "\n",
    "re_pre_df = pd.DataFrame(re_pre_dict)\n",
    "re_pre_df.agg([np.mean, np.min, np.max, np.std, np.median])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot distributions of acc\n",
    "# Some look gaussian, maybe 10 runs is not enough\n",
    "eval_results_grouped.hist(column='acc')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Political Paper\n",
    "![](https://i.imgur.com/yGlTYbd.png)\n",
    "![](https://i.imgur.com/7yrDqQH.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Political F1 CrossVal\n",
    "# Fixed: input=both, seq_len=256, warmup_prop=0.1, seed=42\n",
    "# model=base-uncased, epochs=5, batch_size=12, lr=2e-5\n",
    "\n",
    "# Related/Unrelated\n",
    "eval_results = pd.read_csv('../pytorch/res/pol_ru/eval_results.tsv', sep='\\t')\n",
    "\n",
    "# Some stats: mean, min, max, std\n",
    "# Paper average F1 0.65, here average F1 0.67, baseline ?\n",
    "print(eval_results.groupby(['_bert-model', '_num_epochs', '_batch_size','_gradient_acc' ,'_learning_rate', '_seed'])['f1'].agg([np.mean, np.min, np.max, np.std]))\n",
    "eval_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Political results with respect to topic\n",
    "political_ru_pro = processors['political-ru']('both')\n",
    "splits_data = np.array(political_ru_pro.get_splits('../data'))\n",
    "\n",
    "# Get the test data and the test predictions\n",
    "pol_test_df = pd.concat(splits_data[:,3])\n",
    "eval_preds = pd.read_csv('../pytorch/res/pol_ru/eval_preds.csv')\n",
    "\n",
    "\n",
    "#pol_test_df['preds'] =  eval_preds.iloc[:10,:-1].stack().values # Not working anymore, because of apending to output file\n",
    "preds = pd.Series()\n",
    "re_pre_f1_dict = {'Precision related' : [], 'Precision unrelated': [], \n",
    "                  'Recall related': [], 'Recall unrelated': [], 'F1 related': [], 'F1 unrelated': []}\n",
    "scores_df = pd.DataFrame(re_pre_f1_dict)\n",
    "for i, row in eval_preds.iterrows():\n",
    "    #print(pd.Series(row.values[~row.str.contains('bert*', na=False, regex=True)]))\n",
    "    preds_split_i = pd.Series(row.values[~row.str.contains('bert*', na=False, regex=True)]).dropna()    \n",
    "    preds_split_i = preds_split_i.replace({0: 'related', 1: 'unrelated'})\n",
    "    preds = preds.append(preds_split_i)\n",
    "    pre, rec, f1, support = precision_recall_fscore_support(splits_data[i,3]['label'], preds_split_i, labels=['related', 'unrelated'])\n",
    "    scores_df.loc[i] = np.array((pre,rec,f1)).reshape((1,-1))[0]\n",
    "    if i == 9:\n",
    "        break\n",
    "\n",
    "pol_test_df['preds'] = preds.values\n",
    "\n",
    "print(classification_report(pol_test_df['label'], pol_test_df['preds'], labels=['related', 'unrelated']))\n",
    "\n",
    "scores_df['Average Precision'] = (scores_df['Precision related']+scores_df['Precision unrelated'])/2\n",
    "scores_df['Average Recall'] = (scores_df['Recall related']+scores_df['Recall unrelated'])/2\n",
    "scores_df['Average F1'] = (scores_df['F1 related']+scores_df['F1 unrelated'])/2\n",
    "\n",
    "scores_df.agg([np.mean, np.std, np.min, np.max])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true = np.array((np.ones(50), np.zeros(50))).reshape(100)\n",
    "y_pred = np.concatenate([np.ones(48), np.zeros(2), np.ones(40), np.zeros(10)])\n",
    "print(y_pred)\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "confusion_matrix(y_true, y_pred)\n",
    "print(classification_report(y_true, y_pred))\n",
    "print(precision_recall_fscore_support(y_true, y_pred, average='micro'))\n",
    "\n",
    "# Test about the result tables in the paper\n",
    "# Result: Average Precision and Recall reported is macro/weighted, and F1 score is micro=accuracy? \n",
    "# This is also correct for table 3, \n",
    "# for table 4 this could be correct, because actually there are 3 classes (attack/support/unrelated) and not the full table is shown\n",
    "# The unrelated samples are always false, lower the accuracy, but not the other scores in the table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Political F1 CrossVal\n",
    "# Fixed: input=both, seq_len=256, warmup_prop=0.1, seed=42\n",
    "# model=base-uncased, epochs=5, batch_size=12, lr=2e-5\n",
    "\n",
    "# Attack/Support\n",
    "eval_results = pd.read_csv('../pytorch/res/pol_as/eval_results.tsv', sep='\\t')\n",
    "\n",
    "# Some stats: mean, min, max, std\n",
    "# Paper average F1 0.82, here average F1 0.73, baselines (author) ~0.85\n",
    "print(eval_results.groupby(['_bert-model', '_num_epochs', '_batch_size','_gradient_acc' ,'_learning_rate', '_seed'])['f1'].agg([np.mean, np.min, np.max, np.std]))\n",
    "eval_results.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "political_as_pro = processors['political-as']('both')\n",
    "splits_data = np.array(political_as_pro.get_splits('../data'))\n",
    "\n",
    "# Get the test data and the test predictions\n",
    "pol_test_df = pd.concat(splits_data[:,3])\n",
    "eval_preds = pd.read_csv('../pytorch/res/pol_as/eval_preds.csv')\n",
    "\n",
    "\n",
    "#pol_test_df['preds'] =  eval_preds.iloc[:10,:-1].stack().values # Not working anymore, because of apending to output file\n",
    "preds = pd.Series()\n",
    "re_pre_f1_dict = {'Precision attack' : [], 'Precision support': [], \n",
    "                  'Recall attack': [], 'Recall support': [], 'F1 attack': [], 'F1 support': []}\n",
    "scores_df = pd.DataFrame(re_pre_f1_dict)\n",
    "for i, row in eval_preds.iterrows():\n",
    "    #print(pd.Series(row.values[~row.str.contains('bert*', na=False, regex=True)]))\n",
    "    preds_split_i = pd.Series(row.values[~row.str.contains('bert*', na=False, regex=True)]).dropna()    \n",
    "    preds_split_i = preds_split_i.replace({0: 'attack', 1: 'support'})\n",
    "    preds = preds.append(preds_split_i)\n",
    "    pre, rec, f1, support = precision_recall_fscore_support(splits_data[i,3]['label'], preds_split_i, labels=['attack', 'support'])\n",
    "    print(pre, rec, f1, support)\n",
    "    scores_df.loc[i] = np.array((pre,rec,f1)).reshape((1,-1))[0]\n",
    "    if i == 9:\n",
    "        break\n",
    "\n",
    "pol_test_df['preds'] = preds.values\n",
    "\n",
    "print(classification_report(pol_test_df['label'], pol_test_df['preds'], labels=['attack', 'support']))\n",
    "\n",
    "scores_df['Average Precision'] = (scores_df['Precision attack']+scores_df['Precision support'])/2\n",
    "scores_df['Average Recall'] = (scores_df['Recall attack']+scores_df['Recall support'])/2\n",
    "scores_df['Average F1'] = (scores_df['F1 attack']+scores_df['F1 support'])/2\n",
    "\n",
    "scores_df.agg([np.mean, np.std, np.min, np.max])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Political F1 CrossVal\n",
    "# Fixed: input=both, seq_len=256, warmup_prop=0.1, seed=42\n",
    "# model=base-uncased, epochs=5, batch_size=12, lr=2e-5\n",
    "\n",
    "# Attack/Support/Unrelated\n",
    "eval_results = pd.read_csv('../pytorch/res/pol_asu/eval_results.tsv', sep='\\t')\n",
    "\n",
    "# Some stats: mean, min, max, std\n",
    "# Paper only reported precision 0.57, here average f1 0.60 \n",
    "# Use some tricks to coope with class imbalance!\n",
    "print(eval_results.groupby(['_bert-model', '_num_epochs', '_batch_size','_gradient_acc' ,'_learning_rate', '_seed'])['f1'].agg([np.mean, np.min, np.max, np.std]))\n",
    "eval_results.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "political_asu_pro = processors['political-asu']('both')\n",
    "splits_data = np.array(political_asu_pro.get_splits('../data'))\n",
    "\n",
    "# Get the test data and the test predictions\n",
    "pol_test_df = pd.concat(splits_data[:,3])\n",
    "eval_preds = pd.read_csv('../pytorch/res/pol_asu/eval_preds.csv')\n",
    "\n",
    "\n",
    "#pol_test_df['preds'] =  eval_preds.iloc[:10,:-1].stack().values # Not working anymore, because of apending to output file\n",
    "preds = pd.Series()\n",
    "re_pre_f1_dict = {'Precision attack' : [], 'Precision support': [], 'Precision unrelated': [],\n",
    "                  'Recall attack': [], 'Recall support': [], 'Recall unrelated': [],\n",
    "                  'F1 attack': [], 'F1 support': [], 'F1 unrelated': []}\n",
    "scores_df = pd.DataFrame(re_pre_f1_dict)\n",
    "for i, row in eval_preds.iterrows():\n",
    "    #print(pd.Series(row.values[~row.str.contains('bert*', na=False, regex=True)]))\n",
    "    preds_split_i = pd.Series(row.values[~row.str.contains('bert*', na=False, regex=True)]).dropna()    \n",
    "    preds_split_i = preds_split_i.replace({0: 'attack', 1: 'support', 2: 'unrelated'})\n",
    "    preds = preds.append(preds_split_i)\n",
    "    pre, rec, f1, support = precision_recall_fscore_support(splits_data[i,3]['label'], preds_split_i, labels=['attack', 'support', 'unrelated'])\n",
    "    print(pre, rec, f1, support)\n",
    "    scores_df.loc[i] = np.array((pre,rec,f1)).reshape((1,-1))[0]\n",
    "    if i == 9:\n",
    "        break\n",
    "\n",
    "pol_test_df['preds'] = preds.values\n",
    "\n",
    "print(classification_report(pol_test_df['label'], pol_test_df['preds'], labels=['attack', 'support', 'unrelated']))\n",
    "print(classification_report(pol_test_df['label'], ['unrelated']*1460, labels=['attack', 'support', 'unrelated']))\n",
    "\n",
    "\n",
    "scores_df['Average Precision'] = (scores_df['Precision attack']+scores_df['Precision support']+scores_df['Precision unrelated'])/3\n",
    "scores_df['Average Recall'] = (scores_df['Recall attack']+scores_df['Recall support']+scores_df['Recall unrelated'])/3\n",
    "scores_df['Average F1'] = (scores_df['F1 attack']+scores_df['F1 support']+scores_df['F1 unrelated'])/3\n",
    "\n",
    "scores_df.agg([np.mean, np.std, np.min, np.max])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agreement Paper\n",
    "- Accuracy 74%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Agreement F1 CrossVal\n",
    "# Comparison with Paper + Baselines\n",
    "# Fixed: input=both, seq_len=256, warmup_prop=0.1, seed=42\n",
    "# model=base-uncased, epochs=3, batch_size=12, lr=2e-5\n",
    "\n",
    "# Agreement/Disagreement\n",
    "eval_results = pd.read_csv('../pytorch/res/agreement_new/eval_results.tsv', sep='\\t')\n",
    "# Some stats: mean, min, max, std\n",
    "# Paper average acc 0.74 , here average acc 0.61\n",
    "# TODO: non cross_val version had acc ~0.97! Probably parameters are bad, 2 Epochs might not be enough \n",
    "# (try again with higher epochs number)\n",
    "print(eval_results['acc'].agg([np.mean, np.min, np.max, np.std]))\n",
    "eval_results.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agreement_pro = processors['agreement']('both')\n",
    "splits_data = np.array(agreement_pro.get_splits('../data'))\n",
    "\n",
    "# Get the test data and the test predictions\n",
    "ag_test_df = pd.concat(splits_data[:,3])\n",
    "eval_preds = pd.read_csv('../pytorch/res/agreement_new/eval_preds.csv')\n",
    "\n",
    "preds = pd.Series()\n",
    "re_pre_f1_dict = {'Precision agreement' : [], 'Precision disagreement': [], \n",
    "                  'Recall agreement': [], 'Recall disagreement': [], 'F1 agreement': [], 'F1 disagreement': []}\n",
    "scores_df = pd.DataFrame(re_pre_f1_dict)\n",
    "for i, row in eval_preds.iterrows():\n",
    "    #print(pd.Series(row.values[~row.str.contains('bert*', na=False, regex=True)]))\n",
    "    preds_split_i = pd.Series(row.values[~row.str.contains('bert*', na=False, regex=True)]).dropna()    \n",
    "    preds_split_i = preds_split_i.replace({0: 'agreement', 1: 'disagreement','0': 'agreement', '1': 'disagreement'})\n",
    "\n",
    "    preds = preds.append(preds_split_i)\n",
    "    pre, rec, f1, support = precision_recall_fscore_support(splits_data[i,3]['label'], preds_split_i, labels=['agreement', 'disagreement'])\n",
    "    scores_df.loc[i] = np.array((pre,rec,f1)).reshape((1,-1))[0]\n",
    "    if i == 9:\n",
    "        break\n",
    "\n",
    "ag_test_df['preds'] = preds.values\n",
    "scores_df.describe()\n",
    "pd.crosstab(ag_test_df['preds'], ag_test_df['label'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results analyzed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_results = pd.read_csv('../pytorch/res/node_both_paper/eval_results.tsv', sep='\\t')\n",
    "eval_results_grouped = eval_results.groupby(['_bert-model', '_num_epochs', '_batch_size','_gradient_acc' ,'_learning_rate' ])\n",
    "_, node_test_df = node_pro.get_dev_examples('../data')\n",
    "eval_preds = pd.read_csv('../pytorch/res/node_both_paper/eval_preds.csv')\n",
    "\n",
    "results = eval_results_grouped['acc'].agg([np.mean, np.min, np.max, np.std, np.median])\n",
    "bmodel, bepochs, bb, bga, blr = results.loc[results['mean'].idxmax()].name\n",
    "print(bmodel, bepochs, bb, blr)\n",
    "best_pred_ps = eval_results.loc[(eval_results['_bert-model'] == bmodel) & \n",
    "                       (eval_results['_num_epochs'] == bepochs) & (eval_results['_batch_size'] == bb) &\n",
    "                       (eval_results['_learning_rate'] == blr)].index\n",
    "print(best_pred_ps)\n",
    "\n",
    "# Only predictions from best setting \n",
    "res = pd.concat([node_test_df.reset_index(drop=True), eval_preds.iloc[best_pred_ps,:-1].transpose().reset_index(drop=True)], axis=1)\n",
    "res['Mean prediction'] = res[list(best_pred_ps)].mean(axis=1).round().values\n",
    "res = res.replace({0: 'attack', 1: 'support'})\n",
    "res = res.rename(columns={'label': 'Label'})\n",
    "\n",
    "pd.crosstab(res['topic'], [res['Label'],res['Mean prediction']]).to_csv('../data/thesis/node_topics_preds.csv', index=True)\n",
    "pd.crosstab(res['topic'], [res['Label'],res['Mean prediction']])\n",
    "preds_orgs = pd.crosstab(res['org'], [res['Label'],res['Mean prediction']])\n",
    "preds_orgs['total'] = preds_orgs.agg([np.sum], axis=1)\n",
    "# TODO: how to visualize? E.g. count number of same predictions and divide by total to show how (important) the response arguments are (exclude arguments with total 1)\n",
    "# Compare this number to the correct predictions?\n",
    "preds_orgs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Node results with respect to topic\n",
    "_, node_test_df = node_pro.get_dev_examples('../data')\n",
    "eval_preds = pd.read_csv('../pytorch/res/node_both_paper/eval_preds.csv')\n",
    "\n",
    "\n",
    "# Only predictions from bert-base\n",
    "res = pd.concat([node_test_df.reset_index(drop=True), eval_preds.iloc[27:,:-1].transpose().reset_index(drop=True)], axis=1)\n",
    "res = res.replace({0: 'attack', 1: 'support'})\n",
    "\n",
    "# For now, only one run (run 51) used\n",
    "# There are errors in every topic, no clear trend visible that some topics are better or worse\n",
    "# More false classifications of attack than of support (support is the major class)\n",
    "# Could, also look at several runs, or average, etc.\n",
    "pd.crosstab(res['topic'], [res['label'],res[51]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take the rounded mean prediction for all bert-base runs \n",
    "res['mean_round'] = eval_preds.iloc[27:,:-1].mean().round().values\n",
    "res = res.replace({0: 'attack', 1: 'support'})\n",
    "pd.crosstab(res['topic'], [res['label'],res['mean_round']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can recreate all metrics from the available data\n",
    "# E.g. classification reports or confusion matrices \n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "print(classification_report(y_pred=eval_preds.iloc[51,:-1].replace({0: 'attack', 1: 'support'}), y_true=res['label']))\n",
    "\n",
    "print(confusion_matrix(res['label'], eval_preds.iloc[51,:-1].replace({0: 'attack', 1: 'support'})))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Political results with respect to topic\n",
    "political_ru_pro = processors['political-ru']('both')\n",
    "splits_data = political_ru_pro.get_splits('../data')\n",
    "\n",
    "# Get the test data and the test predictions\n",
    "pol_test_df = pd.concat(np.array(splits_data)[:,3])\n",
    "eval_preds = pd.read_csv('../pytorch/res/pol_ru/eval_preds.csv')\n",
    "\n",
    "\n",
    "#pol_test_df['preds'] =  eval_preds.iloc[:10,:-1].stack().values # Not working anymore, because of apending to output file\n",
    "preds = pd.Series()\n",
    "for i, row in eval_preds.iterrows():\n",
    "    #print(pd.Series(row.values[~row.str.contains('bert*', na=False, regex=True)]))\n",
    "    preds = preds.append(pd.Series(row.values[~row.str.contains('bert*', na=False, regex=True)]))\n",
    "    if i == 9:\n",
    "        break\n",
    "preds = preds.dropna()\n",
    "preds = preds.astype(int)\n",
    "\n",
    "pol_test_df['preds'] = preds.values\n",
    "pol_test_df = pol_test_df.replace({0: 'related', 1: 'unrelated'})\n",
    "\n",
    "for name, group in pol_test_df.groupby('topic'):\n",
    "    print(name)\n",
    "    print(classification_report(group['label'], group['preds'], labels=['related', 'unrelated']))\n",
    "\n",
    "\n",
    "pd.crosstab(pol_test_df['topic'], [pol_test_df['label'],pol_test_df['preds']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Political results with respect to topic\n",
    "political_ru_pro = processors['political-ru-topics']('both')\n",
    "splits_data = political_ru_pro.get_splits('../data')\n",
    "\n",
    "# Get the test data and the test predictions\n",
    "pol_test_df = pd.concat(np.array(splits_data)[:,3])\n",
    "eval_preds = pd.read_csv('../pytorch/res/pol_ru_topics/eval_preds.csv', names=list(range(0,315))).iloc[1:]\n",
    "\n",
    "\n",
    "#pol_test_df['preds'] =  eval_preds.iloc[:10,:-1].stack().values # Not working anymore, because of apending to output file\n",
    "preds = pd.Series()\n",
    "for i, row in eval_preds.iterrows():\n",
    "    #print(pd.Series(row.values[~row.str.contains('bert*', na=False, regex=True)]))\n",
    "    preds = preds.append(pd.Series(row.values[~row.str.contains('bert*', na=False, regex=True)]))\n",
    "    if i == 5:\n",
    "        break\n",
    "preds = preds.dropna()\n",
    "preds = preds.astype(int)\n",
    "\n",
    "pol_test_df['preds'] = preds.values\n",
    "pol_test_df = pol_test_df.replace({0: 'related', 1: 'unrelated'})\n",
    "\n",
    "for name, group in pol_test_df.groupby('topic'):\n",
    "    print(name)\n",
    "    print(classification_report(group['label'], group['preds'], labels=['related', 'unrelated']))\n",
    "\n",
    "print(classification_report(pol_test_df['label'], pol_test_df['preds']))\n",
    "\n",
    "pd.crosstab(pol_test_df['topic'], [pol_test_df['label'],pol_test_df['preds']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Political results with respect to topic\n",
    "political_as_pro = processors['political-as']('both')\n",
    "splits_data = political_as_pro.get_splits('../data')\n",
    "\n",
    "# Get the test data and the test predictions\n",
    "pol_test_df = pd.concat(np.array(splits_data)[:,3])\n",
    "eval_preds = pd.read_csv('../pytorch/res/pol_as/eval_preds.csv')\n",
    "\n",
    "\n",
    "#pol_test_df['preds'] =  eval_preds.iloc[:10,:-1].stack().values # Not working anymore, because of apending to output file\n",
    "preds = pd.Series()\n",
    "for i, row in eval_preds.iterrows():\n",
    "    #print(pd.Series(row.values[~row.str.contains('bert*', na=False, regex=True)]))\n",
    "    preds = preds.append(pd.Series(row.values[~row.str.contains('bert*', na=False, regex=True)]))\n",
    "    if i == 9:\n",
    "        break\n",
    "preds = preds.dropna()\n",
    "preds = preds.astype(int)\n",
    "\n",
    "pol_test_df['preds'] = preds.values\n",
    "pol_test_df = pol_test_df.replace({0: 'attack', 1: 'support'})\n",
    "\n",
    "for name, group in pol_test_df.groupby('topic'):\n",
    "    print(name)\n",
    "    print(classification_report(group['label'], group['preds'], labels=['attack', 'support']))\n",
    "\n",
    "print(classification_report(pol_test_df['label'], pol_test_df['preds']))\n",
    "\n",
    "pol_test_df['authors'] = pol_test_df.apply(lambda r: 'same' if r['org_stance'] == r['response_stance'] else 'different', axis=1)\n",
    "\n",
    "for name, group in pol_test_df.groupby('authors'):\n",
    "    print(name)\n",
    "    print(classification_report(group['label'], group['preds'], labels=['attack', 'support']))\n",
    "    \n",
    "for name, group in pol_test_df.groupby(['response_stance', 'org_stance']):\n",
    "    print(name)\n",
    "    print(classification_report(group['label'], group['preds'], labels=['attack', 'support']))\n",
    "\n",
    "\n",
    "pd.crosstab(pol_test_df['topic'], [pol_test_df['label'],pol_test_df['preds']])\n",
    "pol_test_df.loc[(pol_test_df['authors'] == 'same') & (pol_test_df['label'] == 'attack')][['org', 'response', 'preds', 'id']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Political results with respect to topic\n",
    "political_as_pro = processors['political-as-topics']('both')\n",
    "splits_data = political_as_pro.get_splits('../data')\n",
    "\n",
    "# Get the test data and the test predictions\n",
    "pol_test_df = pd.concat(np.array(splits_data)[:,3])\n",
    "eval_preds = pd.read_csv('../pytorch/res/pol_as_topics/eval_preds.csv', names=list(range(0,315))).iloc[1:]\n",
    "\n",
    "\n",
    "#pol_test_df['preds'] =  eval_preds.iloc[:10,:-1].stack().values # Not working anymore, because of apending to output file\n",
    "preds = pd.Series()\n",
    "for i, row in eval_preds.iterrows():\n",
    "    #print(pd.Series(row.values[~row.str.contains('bert*', na=False, regex=True)]))\n",
    "    preds = preds.append(pd.Series(row.values[~row.str.contains('bert*', na=False, regex=True)]))\n",
    "    if i == 5:\n",
    "        break\n",
    "preds = preds.dropna()\n",
    "preds = preds.astype(int)\n",
    "\n",
    "pol_test_df['preds'] = preds.values\n",
    "pol_test_df = pol_test_df.replace({0: 'attack', 1: 'support'})\n",
    "\n",
    "for name, group in pol_test_df.groupby('topic'):\n",
    "    print(name)\n",
    "    print(classification_report(group['label'], group['preds'], labels=['attack', 'support']))\n",
    "\n",
    "print(classification_report(pol_test_df['label'], pol_test_df['preds']))\n",
    "\n",
    "pd.crosstab(pol_test_df['topic'], [pol_test_df['label'],pol_test_df['preds']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Political results with respect to author\n",
    "pd.crosstab(pol_test_df['preds'], [pol_test_df['org_stance'],pol_test_df['response_stance']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Complete results political (all folds \"summed\")\n",
    "print(classification_report(y_pred=pol_test_df['preds'], y_true=pol_test_df['label']))\n",
    "\n",
    "print(confusion_matrix(y_pred=pol_test_df['preds'], y_true=pol_test_df['label']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Results with respect to same org, same resp (always gets the same label or not?)\n",
    "\n",
    "# Same org\n",
    "# One org does not always get the same prediction (but often)\n",
    "pd.crosstab(res['org'], res[51])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Same org pol\n",
    "# TODO: aggregate to get some useful insights \n",
    "# (and maybe do it for every fold individually, \n",
    "# because otherwise it could be that we always predict one label for one org in one fold and another in another fold)\n",
    "pd.crosstab(pol_test_df['org'], pol_test_df['preds']).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Same resp pol\n",
    "pd.crosstab(pol_test_df['response'], pol_test_df['preds']).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Agreement results with respect to topic\n",
    "ag_pro = processors['agreement']('both')\n",
    "splits_data = ag_pro.get_splits('../data')\n",
    "\n",
    "# Get the test data and the test predictions\n",
    "ag_test_df = pd.concat(np.array(splits_data)[:,3])\n",
    "eval_preds = pd.read_csv('../pytorch/res/agreement_new/eval_preds.csv')\n",
    "\n",
    "\n",
    "#pol_test_df['preds'] =  eval_preds.iloc[:10,:-1].stack().values # Not working anymore, because of apending to output file\n",
    "preds = pd.Series()\n",
    "for i, row in eval_preds.iterrows():\n",
    "    #print(pd.Series(row.values[~row.str.contains('bert*', na=False, regex=True)]))\n",
    "    preds = preds.append(pd.Series(row.values[~row.str.contains('bert*', na=False, regex=True)]))\n",
    "    if i == 9:\n",
    "        break\n",
    "preds = preds.dropna()\n",
    "preds = preds.astype(int)\n",
    "\n",
    "ag_test_df['preds'] = preds.values\n",
    "ag_test_df = ag_test_df.replace({0: 'agreement', 1: 'disagreement'})\n",
    "\n",
    "print(classification_report(ag_test_df['label'], ag_test_df['preds']))\n",
    "\n",
    "results_topic = pd.crosstab(ag_test_df['topic'], [ag_test_df['label'], ag_test_df['preds']])\n",
    "results_topic['total'] = results_topic.agg([np.sum], axis=1)\n",
    "results_topic['correctness'] = results_topic.apply(lambda r: (r['agreement', 'agreement']+r['disagreement', 'disagreement'])/r['total'], axis=1)\n",
    "results = results_topic.sort_values(by='correctness')\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Agreement results with respect to topic\n",
    "ag_pro = processors['agreement-topics']('both')\n",
    "_, ag_test_df = ag_pro.get_dev_examples('../data')\n",
    "\n",
    "# Get the test data and the test predictions\n",
    "eval_preds = pd.read_csv('../pytorch/res/agreement_topics_new/eval_preds.csv')\n",
    "\n",
    "\n",
    "ag_test_df['preds'] = eval_preds.transpose().iloc[:-1].values\n",
    "ag_test_df = ag_test_df.replace({0: 'agreement', 1: 'disagreement'})\n",
    "\n",
    "print(classification_report(ag_test_df['label'], ag_test_df['preds']))\n",
    "\n",
    "results_topic = pd.crosstab(ag_test_df['topic'], [ag_test_df['label'], ag_test_df['preds']])\n",
    "print(results_topic)\n",
    "results_topic['total'] = results_topic.agg([np.sum], axis=1)\n",
    "results_topic['correctness'] = results_topic.apply(lambda r: (r['agreement', 'agreement']+r['disagreement', 'disagreement'])/r['total'], axis=1)\n",
    "results = results_topic.sort_values(by='correctness')\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results additional\n",
    "- NoDE + procon\n",
    "- Pol as/ru Topic 5-CV\n",
    "- Agreement train/test (topics indepedent + removed duplicates)\n",
    "- test with only the orgs as input and with only the response as input\n",
    "   - Arguments likely to be attacked/supported\n",
    "   - Attackful/ing or supportful/ing arguments\n",
    "- changed order org/resp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NoDE + procon\n",
    "# Fixed: input=both, seq_len=128, warmup_prop=0.1, seed=42-(42+30)\n",
    "# model=base-uncased, epochs=4-5, batch_size=12,16, lr=2e-5,3e-5\n",
    "\n",
    "# Attack/Support\n",
    "eval_results = pd.read_csv('../pytorch/res/node_both_procon/eval_results.tsv', sep='\\t')\n",
    "\n",
    "# Some stats: mean, min, max, std\n",
    "# Best setting 0.67+-0.05 (without procon 0.69+-0.02) performance decreased! (more seeds?, procon too different from debatepedia, ...)\n",
    "results = eval_results.groupby(['_bert-model', '_num_epochs', '_batch_size','_gradient_acc' ,'_learning_rate'])['acc'].agg([np.mean, np.min, np.max, np.std])\n",
    "print(\"best mean\", results.loc[results['mean'].idxmax()]) # Best mean result\n",
    "results\n",
    "\n",
    "results = results.reset_index()\n",
    "results = results.rename(columns={element: element.replace(\"_\", \"-\") for element in results.columns.tolist()})\n",
    "results = results.rename(columns={element: re.sub(\"^-\", \"\", element) for element in results.columns.tolist()})\n",
    "results = results.replace({\"bert-base-uncased\": 'bbu', \"bert-large-uncased\": 'blu'})\n",
    "results['batch-size'] = results['batch-size'] * results['gradient-acc']\n",
    "results = results.drop(columns=[\"gradient-acc\"])\n",
    "print(results.columns)\n",
    "results.iloc[:,:9].to_csv('../data/thesis/node_procon_results.csv', index=False)\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pol ru topics-cv\n",
    "# Fixed: input=both, seq_len=256, warmup_prop=0.1, seed=42\n",
    "# model=base-uncased, epochs=5, batch_size=12, lr=2e-5\n",
    "\n",
    "# Related/Unrelated\n",
    "eval_results = pd.read_csv('../pytorch/res/pol_ru_topics/eval_results.tsv', sep='\\t')\n",
    "\n",
    "# Some stats: mean, min, max, std\n",
    "# 0.59 +- 0.04 (original BERT 0.67+-0.03) -> performance decreased! task is harder, network might remember topics/args\n",
    "results = eval_results.groupby(['_bert-model', '_num_epochs', '_batch_size','_gradient_acc' ,'_learning_rate'])['f1'].agg([np.mean, np.min, np.max, np.std])\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pol as topics-cv\n",
    "# Fixed: input=both, seq_len=256, warmup_prop=0.1, seed=42\n",
    "# model=base-uncased, epochs=5, batch_size=12, lr=2e-5\n",
    "\n",
    "# Attack/Support\n",
    "eval_results = pd.read_csv('../pytorch/res/pol_as_topics/eval_results.tsv', sep='\\t')\n",
    "\n",
    "# Some stats: mean, min, max, std\n",
    "# 0.63 +- 0.07 (orginal BERT 0.76+-0.05 and paper 0.82) -> performance drastically decreased! task is harder, in normal version the network might remember a lot?\n",
    "results = eval_results.groupby(['_bert-model', '_num_epochs', '_batch_size','_gradient_acc' ,'_learning_rate'])['f1'].agg([np.mean, np.min, np.max, np.std])\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Agreement topic train/test\n",
    "# Fixed: input=both, seq_len=256, warmup_prop=0.1, seed=42\n",
    "# model=base-uncased, epochs=3, batch_size=12, lr=2e-5\n",
    "\n",
    "# Related/Unrelated\n",
    "eval_results = pd.read_csv('../pytorch/res/agreement_topics_new/eval_results.tsv', sep='\\t')\n",
    "\n",
    "# Some stats: mean, min, max, std\n",
    "# TODO: Fix numbers!\n",
    "# 0.75 (original BERT 0.97, paper: 0.74) -> performance drastically decreased! original task to easy, because of many duplicates + remembering distribution over topics\n",
    "results = eval_results.groupby(['_bert-model', '_num_epochs', '_batch_size','_gradient_acc' ,'_learning_rate'])['acc'].agg([np.mean, np.min, np.max, np.std])\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NoDE only org\n",
    "# Fixed: input=org, seq_len=128, warmup_prop=0.1, seed=42-(42+30)\n",
    "# model=base-uncased, epochs=4-5, batch_size=12,16, lr=2e-5,3e-5\n",
    "\n",
    "# Attack/Support\n",
    "eval_results = pd.read_csv('../pytorch/res/node_org/eval_results.tsv', sep='\\t')\n",
    "\n",
    "# Some stats: mean, min, max, std\n",
    "# Best setting 0.64+-0.02 (with org and resp 0.69+-0.02) performance decreased! (only org is not enough, but carries some meaning)\n",
    "results = eval_results.groupby(['_bert-model', '_num_epochs', '_batch_size','_gradient_acc' ,'_learning_rate'])['acc'].agg([np.mean, np.min, np.max, np.std])\n",
    "print(\"best mean\", results.loc[results['mean'].idxmax()]) # Best mean result\n",
    "results\n",
    "results = results.reset_index()\n",
    "results = results.rename(columns={element: element.replace(\"_\", \"-\") for element in results.columns.tolist()})\n",
    "results = results.rename(columns={element: re.sub(\"^-\", \"\", element) for element in results.columns.tolist()})\n",
    "results = results.replace({\"bert-base-uncased\": 'bbu', \"bert-large-uncased\": 'blu'})\n",
    "results['batch-size'] = results['batch-size'] * results['gradient-acc']\n",
    "results = results.drop(columns=[\"gradient-acc\"])\n",
    "print(results.columns)\n",
    "results.iloc[:,:9].to_csv('../data/thesis/node_onlyorg_results.csv', index=False)\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NoDE only resp\n",
    "# Fixed: input=resp, seq_len=128, warmup_prop=0.1, seed=42-(42+30)\n",
    "# model=base-uncased, epochs=4-5, batch_size=12,16, lr=2e-5,3e-5\n",
    "\n",
    "# Attack/Support\n",
    "eval_results = pd.read_csv('../pytorch/res/node_resp/eval_results.tsv', sep='\\t')\n",
    "\n",
    "# Some stats: mean, min, max, std\n",
    "# Best setting 0.58+-0.06 (with org and resp 0.69+-0.02) worse than just using org (unexpected!)\n",
    "results = eval_results.groupby(['_bert-model', '_num_epochs', '_batch_size','_gradient_acc' ,'_learning_rate'])['acc'].agg([np.mean, np.min, np.max, np.std])\n",
    "print(\"best mean\", results.loc[results['mean'].idxmax()]) # Best mean result\n",
    "results\n",
    "results = results.reset_index()\n",
    "results = results.rename(columns={element: element.replace(\"_\", \"-\") for element in results.columns.tolist()})\n",
    "results = results.rename(columns={element: re.sub(\"^-\", \"\", element) for element in results.columns.tolist()})\n",
    "results = results.replace({\"bert-base-uncased\": 'bbu', \"bert-large-uncased\": 'blu'})\n",
    "results['batch-size'] = results['batch-size'] * results['gradient-acc']\n",
    "results = results.drop(columns=[\"gradient-acc\"])\n",
    "print(results.columns)\n",
    "results.iloc[:,:9].to_csv('../data/thesis/node_onlyresp_results.csv', index=False)\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pol ru only org\n",
    "# Fixed: input=org, seq_len=256, warmup_prop=0.1, seed=42\n",
    "# model=base-uncased, epochs=5, batch_size=12, lr=2e-5\n",
    "\n",
    "# Related/Unrelated\n",
    "eval_results = pd.read_csv('../pytorch/res/pol_ru_org/eval_results.tsv', sep='\\t')\n",
    "\n",
    "# Some stats: mean, min, max, std\n",
    "# 0.61 +- 0.06 (original BERT 0.67+-0.03) -> better than random\n",
    "results = eval_results.groupby(['_bert-model', '_num_epochs', '_batch_size','_gradient_acc' ,'_learning_rate'])['f1'].agg([np.mean, np.min, np.max, np.std])\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pol ru only resp\n",
    "# Fixed: input=resp, seq_len=256, warmup_prop=0.1, seed=42\n",
    "# model=base-uncased, epochs=5, batch_size=12, lr=2e-5\n",
    "\n",
    "# Related/Unrelated\n",
    "eval_results = pd.read_csv('../pytorch/res/pol_ru_resp/eval_results.tsv', sep='\\t')\n",
    "\n",
    "# Some stats: mean, min, max, std\n",
    "# 0.62 +- 0.05 (original BERT 0.67+-0.03) -> better than random/no significant difference between just using org or just using response\n",
    "results = eval_results.groupby(['_bert-model', '_num_epochs', '_batch_size','_gradient_acc' ,'_learning_rate'])['f1'].agg([np.mean, np.min, np.max, np.std])\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pol as only org\n",
    "# Fixed: input=org, seq_len=256, warmup_prop=0.1, seed=42\n",
    "# model=base-uncased, epochs=5, batch_size=12, lr=2e-5\n",
    "\n",
    "# Related/Unrelated\n",
    "eval_results = pd.read_csv('../pytorch/res/pol_as_org/eval_results.tsv', sep='\\t')\n",
    "\n",
    "# Some stats: mean, min, max, std\n",
    "# 0.55 +- 0.15 (original BERT 0.76+-0.05) -> very high std!, performance quite bad\n",
    "results = eval_results.groupby(['_bert-model', '_num_epochs', '_batch_size','_gradient_acc' ,'_learning_rate'])['f1'].agg([np.mean, np.min, np.max, np.std])\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pol as only resp\n",
    "# Fixed: input=resp, seq_len=256, warmup_prop=0.1, seed=42\n",
    "# model=base-uncased, epochs=5, batch_size=12, lr=2e-5\n",
    "\n",
    "# Related/Unrelated\n",
    "eval_results = pd.read_csv('../pytorch/res/pol_as_resp/eval_results.tsv', sep='\\t')\n",
    "\n",
    "# Some stats: mean, min, max, std\n",
    "# 0.62 +- 0.06 (original BERT 0.76+-0.05) -> similar to ru? better than just using org?\n",
    "results = eval_results.groupby(['_bert-model', '_num_epochs', '_batch_size','_gradient_acc' ,'_learning_rate'])['f1'].agg([np.mean, np.min, np.max, np.std])\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NoDE input reversed\n",
    "# Fixed: input=resp-org, seq_len=128, warmup_prop=0.1, seed=42-(42+30)\n",
    "# model=base-uncased, epochs=4-5, batch_size=12,16, lr=2e-5,3e-5\n",
    "\n",
    "# Attack/Support\n",
    "eval_results = pd.read_csv('../pytorch/res/node_both_reversed/eval_results.tsv', sep='\\t')\n",
    "\n",
    "# Some stats: mean, min, max, std\n",
    "# Best setting 0.62+-0.05 (with org and resp 0.69+-0.02) order of org/resp matters! (maybe because of NSP?)\n",
    "results = eval_results.groupby(['_bert-model', '_num_epochs', '_batch_size','_gradient_acc' ,'_learning_rate'])['acc'].agg([np.mean, np.min, np.max, np.std])\n",
    "print(\"best mean\", results.loc[results['mean'].idxmax()]) # Best mean result\n",
    "results\n",
    "results = results.reset_index()\n",
    "results = results.rename(columns={element: element.replace(\"_\", \"-\") for element in results.columns.tolist()})\n",
    "results = results.rename(columns={element: re.sub(\"^-\", \"\", element) for element in results.columns.tolist()})\n",
    "results = results.replace({\"bert-base-uncased\": 'bbu', \"bert-large-uncased\": 'blu'})\n",
    "results['batch-size'] = results['batch-size'] * results['gradient-acc']\n",
    "results = results.drop(columns=[\"gradient-acc\"])\n",
    "print(results.columns)\n",
    "results.iloc[:,:9].to_csv('../data/thesis/node_resporg_results.csv', index=False)\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pol ru reversed\n",
    "# Fixed: input=resp-org, seq_len=256, warmup_prop=0.1, seed=42\n",
    "# model=base-uncased, epochs=5, batch_size=12, lr=2e-5\n",
    "\n",
    "# Related/Unrelated\n",
    "eval_results = pd.read_csv('../pytorch/res/pol_ru_resporg/eval_results.tsv', sep='\\t')\n",
    "\n",
    "# Some stats: mean, min, max, std\n",
    "# 0.68 +- 0.03(original BERT 0.67+-0.03) -> better performance than the other ordering?!\n",
    "results = eval_results.groupby(['_bert-model', '_num_epochs', '_batch_size','_gradient_acc' ,'_learning_rate'])['f1'].agg([np.mean, np.min, np.max, np.std])\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pol as reversed\n",
    "# Fixed: input=resp-org, seq_len=256, warmup_prop=0.1, seed=42\n",
    "# model=base-uncased, epochs=5, batch_size=12, lr=2e-5\n",
    "\n",
    "# Related/Unrelated\n",
    "eval_results = pd.read_csv('../pytorch/res/pol_as_resporg/eval_results.tsv', sep='\\t')\n",
    "\n",
    "# Some stats: mean, min, max, std\n",
    "# 0.79 +- 0.06(original BERT 0.76+-0.05) -> better performance than the other ordering?!\n",
    "results = eval_results.groupby(['_bert-model', '_num_epochs', '_batch_size','_gradient_acc' ,'_learning_rate'])['f1'].agg([np.mean, np.min, np.max, np.std])\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Domain adaptation etc.\n",
    "- TODO: do some domain adaptation etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train on one dataset, evaluate on another (without finetuning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# With finetuning (reusing the classification layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# With finetuning + use a new classification layer "
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}

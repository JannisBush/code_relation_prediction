{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Relation Prediction in Argument Mining With Pre-trained Deep Bidirectional Transformers\n",
    "\n",
    "This notebook generates or shows all plots and tables used in the thesis. Some additional interesting material not mentioned or shown in the thesis, can be found in the other file. To run this file, first make sure that all requirements are satisified. The README has detailed information about how to satisfy the requirements. Not all training settings have to be retrained to run this notebook, the cells belonging to training settings not done will be simply skipped. The order is the same as in the thesis.\n",
    "\n",
    "## Table of Contents:\n",
    "1. [Methods and Materials](#methods)\n",
    "2. [Comparative Experiments](#results)\n",
    "3. [Additional Experiments](#addexps)\n",
    "4. [Critical Analysis](#critana)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "The following cells import and load the necessary data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Necessary imports and setups\n",
    "# Basic python imports\n",
    "import re\n",
    "import sys \n",
    "import os\n",
    "\n",
    "# For the sentiment baselines\n",
    "import nltk\n",
    "# Download Sentiment Lexicon\n",
    "nltk.download('vader_lexicon')\n",
    "\n",
    "# Datahandling and plotting\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Statistics and other stuff\n",
    "from sklearn.metrics import precision_recall_fscore_support, accuracy_score, classification_report\n",
    "from IPython.display import IFrame, HTML\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "\n",
    "# Get the exact splits of the data\n",
    "sys.path.append(os.path.abspath(\"../pytorch\"))\n",
    "from run_classifier_dataset_utils import processors\n",
    "\n",
    "# Settings\n",
    "# Do no hide rows in pandas\n",
    "pd.set_option('display.max_rows', 999)\n",
    "pd.set_option('precision', 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read all necessary data\n",
    "df = pd.read_csv('../data/complete_data.tsv', sep='\\t').astype({\"id\": str})\n",
    "data_stats_org = np.load('../data/stats/data_stats_org.npy', allow_pickle=True).item()\n",
    "data_stats_resp = np.load('../data/stats/data_stats_resp.npy', allow_pickle=True).item()\n",
    "data_stats_topic = np.load('../data/stats/data_stats_topic.npy', allow_pickle=True).item()\n",
    "data_stats_author = pd.read_csv('../data/stats/data_stats_author.tsv', sep='\\t')\n",
    "data_stats_total = pd.read_csv('../data/stats/data_stats_total.tsv', sep='\\t')\n",
    "data_nix_ken = pd.read_csv('../data/stats/data_nix_ken.tsv', sep='\\t')\n",
    "# Init the data processors and get the correct labels\n",
    "node_pro = processors['node']('both')\n",
    "_, node_test_df = node_pro.get_dev_examples('../data')\n",
    "political_ru_pro = processors['political-ru']('both')\n",
    "pol_ru_test_df = pd.concat(np.array(political_ru_pro.get_splits('../data'))[:,3])\n",
    "political_as_pro = processors['political-as']('both')\n",
    "pol_as_test_df = pd.concat(np.array(political_as_pro.get_splits('../data'))[:,3])\n",
    "political_asu_pro = processors['political-asu']('both')\n",
    "pol_asu_test_df = pd.concat(np.array(political_asu_pro.get_splits('../data'))[:,3])\n",
    "agreement_pro = processors['agreement']('both')\n",
    "ag_test_df = pd.concat(np.array(agreement_pro.get_splits('../data'))[:,3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Methods and Materials <a name=\"methods\"></a>\n",
    "\n",
    "Three datasets were used in this thesis. For every dataset a link where the dataset is available and a link to the describing paper is provided.\n",
    "- NoDE:\n",
    "    - Link: http://www-sop.inria.fr/NoDE/NoDE-xml.html\n",
    "    - Paper: https://pdfs.semanticscholar.org/16d1/6b8a37c5313fa8c8430fddc011f2a98d20c5.pdf\n",
    "- Political\n",
    "    - Link: https://dh.fbk.eu/resources/political-argumentation\n",
    "    - Paper: https://aaai.org/ocs/index.php/AAAI/AAAI18/paper/view/16393/16020\n",
    "- Agreement\n",
    "    - Link: https://dh.fbk.eu/resources/agreement-disagreement\n",
    "    - Paper: https://www.aclweb.org/anthology/C16-1232"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NoDE (consisting of debatepedia train and test and procon)\n",
    "# General statistics for the datasets\n",
    "pd.concat((data_stats_topic['debate_train'], data_stats_topic['debate_test'], data_stats_topic['procon']), keys=['train', 'test', 'procon'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Histogram of ingoing and outgoing links for the arguments in the NoDe debatepedia test set\n",
    "fig, (ax1, ax2) = plt.subplots(1,2, figsize=(10,4), sharex=True)  # 1 row, 2 columns\n",
    "\n",
    "data_set = 'debate_test'\n",
    "df_plot = pd.concat((data_stats_org[data_set].iloc[:-1], \n",
    "                     data_stats_resp[data_set].iloc[:-1].rename(\n",
    "                         columns={'Attacks': 'Attacked', 'Supports': 'Supported'})), keys=['Original', 'Response'])\n",
    "df_plot.loc['Original'].hist(column=['Total pairs'], ax=ax1, bins=[1,2,3,4,5,6,7,8,9])\n",
    "df_plot.loc['Response'].hist(column=['Total pairs'], ax=ax2, bins=[1,2,3,4,5,6,7,8,9])\n",
    "\n",
    "ax1.set_title(\"Original\")\n",
    "ax2.set_title(\"Response\")\n",
    "ax1.set_ylabel(\"Count\")\n",
    "ax1.set_xlabel(\"Number of ingoing links\")\n",
    "ax2.set_xlabel(\"Number of outgoing links\")\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.savefig('../data/thesis/node_hist.pdf', bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Boxplot of the lengths of arguments in WordPiece-Tokens for the debatepedia train and test set\n",
    "fig, ax = plt.subplots(1,2, figsize=(10,4), sharey=True)  # 1 row, 2 columns\n",
    "\n",
    "#for data_set, ax in [('debate_train', ax1),('debate_test', ax2)]:\n",
    "df_plot = pd.concat((df[df['org_dataset'] == 'debate_train'], df[df['org_dataset'] == 'debate_test']))\n",
    "df_plot = df_plot.rename(columns={'org_len': 'Original', 'response_len': 'Response', 'complete_len': 'Combined'})\n",
    "df_plot = df_plot.replace({'debate_test': 'Test', 'debate_train': 'Train'})\n",
    "df_plot.groupby('org_dataset', sort=False).boxplot(ax=ax)\n",
    "ax[0].set_ylabel('Length in WordPiece tokens')\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.savefig('../data/thesis/node_length.pdf', bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Political\n",
    "# General statistics for the dataset\n",
    "data_stats_topic['political']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Histogram of ingoing and outgoing links for the arguments in the Political dataset\n",
    "fig, (ax1, ax2) = plt.subplots(1,2, figsize=(10,4), sharex=False, sharey=True)  # 1 row, 2 columns\n",
    "\n",
    "data_set = 'political'\n",
    "df_plot = pd.concat((data_stats_org[data_set].iloc[:-1], \n",
    "                     data_stats_resp[data_set].iloc[:-1].rename(\n",
    "                         columns={'Attacks': 'Attacked', 'Supports': 'Supported'})), keys=['Original', 'Response'])\n",
    "df_plot.loc['Original'].hist(column=['Total pairs'], bins=[1,2,3,4,5,6,7,8,9],ax=ax1)\n",
    "df_plot.loc['Response'].hist(column=['Total pairs'], bins=[1,2,3,4,5,6,7,8,9], ax=ax2)\n",
    "\n",
    "ax1.set_title(\"Original\")\n",
    "ax2.set_title(\"Response\")\n",
    "ax1.set_ylabel(\"Count\")\n",
    "ax1.set_xlabel(\"Number of ingoing links\")\n",
    "ax2.set_xlabel(\"Number of outgoing links\")\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.savefig('../data/thesis/political_hist.pdf', bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Boxplot of the lengths of arguments in WordPiece-Tokens for the Political dataset\n",
    "fig, ax = plt.subplots(1,1, figsize=(5,4), sharey=True)  # 1 row, 2 columns\n",
    "\n",
    "#for data_set, ax in [('debate_train', ax1),('debate_test', ax2)]:\n",
    "df_plot = df[df['org_dataset'] == 'political']\n",
    "df_plot = df_plot.rename(columns={'org_len': 'Original', 'response_len': 'Response', 'complete_len': 'Combined'})\n",
    "df_plot.groupby('org_dataset', sort=False).boxplot(ax=ax)\n",
    "ax.set_title(\"\")\n",
    "ax.set_ylabel('Length in WordPiece tokens')\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.savefig('../data/thesis/political_length.pdf', bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Agreement\n",
    "# General statistics\n",
    "data_stats_topic['agreement']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Boxplot of the lengths of arguments in WordPiece-Tokens for the Agreement dataset\n",
    "fig, ax = plt.subplots(1,1, figsize=(5,4), sharey=True)  # 1 row, 2 columns\n",
    "\n",
    "df_plot = df[df['org_dataset'] == 'agreement']\n",
    "df_plot = df_plot.rename(columns={'org_len': 'Argument 1', 'response_len': 'Argument 2', 'complete_len': 'Combined'})\n",
    "df_plot.groupby('org_dataset', sort=False).boxplot(ax=ax)\n",
    "ax.set_title(\"\")\n",
    "ax.set_ylabel('Length in WordPiece tokens')\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.savefig('../data/thesis/agreement_length.pdf', bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparative Experiments <a name=\"results\"></a>\n",
    "\n",
    "In the following, the results for the comparative experiments for the three datasets are presented."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The comparative results on the NoDE dataset. The network is always trained on the train dataset \n",
    "# and evaluated on the test dataset. \n",
    "\n",
    "# Parameters:\n",
    "# Fixed: input=both, seq_len=128, warmup_prop=0.1, seed=42-51\n",
    "# Tested: model=base-uncased,large-uncased, epochs=3,4,5, batch_size=8,12,16, lr=2e-5, 3e-5, 5e-5\n",
    "# Gradient accumulation: batch_size/4 for bert_large \n",
    "\n",
    "# Only run this cell, if the NoDE comparative training has been done\n",
    "if os.path.isdir(\"../pytorch/res/node_both_paper\"):\n",
    "    # Read in all testing results\n",
    "    eval_results = pd.read_csv('../pytorch/res/node_both_paper/eval_results.tsv', sep='\\t')\n",
    "    # Read in all testing predictions\n",
    "    eval_preds = pd.read_csv('../pytorch/res/node_both_paper/eval_preds.csv')\n",
    "    # Group all runs with the same settings (different seeds) together\n",
    "    eval_results_grouped = eval_results.groupby(['_bert-model', '_num_epochs', '_batch_size','_gradient_acc' ,'_learning_rate' ])\n",
    "    # Aggregate mean, min, max, std and median\n",
    "    results = eval_results_grouped['acc'].agg([np.mean, np.min, np.max, np.std, np.median])\n",
    "    \n",
    "    # Print the statistics reported in the thesis\n",
    "    print(\"Best mean result with parameters and other statistics:\\n\", results.loc[results['mean'].idxmax()]) \n",
    "    print(\"\\nOverall mean:\\n\", eval_results['acc'].mean())\n",
    "    print(\"\\nWorst (single) run:\\n\", eval_results['acc'].min())\n",
    "    print(\"\\nBest (single) run:\\n\", eval_results['acc'].max())\n",
    "    print(\"\\nResults grouped per model:\\n\", eval_results.groupby('_bert-model')['acc'].agg([np.mean])) \n",
    "    \n",
    "    # Precision and recall for the best mean setting\n",
    "    bmodel, bepochs, bb, bga, blr = results.loc[results['mean'].idxmax()].name\n",
    "    best_pred_ps = eval_results.loc[(eval_results['_bert-model'] == bmodel) & \n",
    "                           (eval_results['_num_epochs'] == bepochs) & (eval_results['_batch_size'] == bb) &\n",
    "                           (eval_results['_learning_rate'] == blr)].index\n",
    "    res = pd.concat([node_test_df.reset_index(drop=True), \n",
    "                     eval_preds.iloc[best_pred_ps,:-1].transpose().reset_index(drop=True)], axis=1)\n",
    "    res = res.replace({0: 'attack', 1: 'support'})\n",
    "    re_pre_dict = {'Precision support' : [], 'Precision attack': [], 'Recall support': [], 'Recall attack': []}\n",
    "    for ind in best_pred_ps:\n",
    "        pre, rec, _, _ = precision_recall_fscore_support(res['label'], res[ind], labels=['support', 'attack'])\n",
    "        re_pre_dict['Precision support'].append(pre[0])\n",
    "        re_pre_dict['Precision attack'].append(pre[1])\n",
    "        re_pre_dict['Recall support'].append(rec[0])\n",
    "        re_pre_dict['Recall attack'].append(rec[1])\n",
    "    \n",
    "    print(\"\\nPrecision and recall for the best mean setting:\")\n",
    "    display(pd.DataFrame(re_pre_dict).agg([np.mean, np.std]))\n",
    "    \n",
    "    # Formats and shows the complete result table\n",
    "    results = results.reset_index()\n",
    "    results = results.rename(columns={element: element.replace(\"_\", \"-\") for element in results.columns.tolist()})\n",
    "    results = results.rename(columns={element: re.sub(\"^-\", \"\", element) for element in results.columns.tolist()})\n",
    "    results = results.replace({\"bert-base-uncased\": 'bbu', \"bert-large-uncased\": 'blu'})\n",
    "    results['batch-size'] = results['batch-size'] * results['gradient-acc']\n",
    "    results = results.drop(columns=[\"gradient-acc\"])\n",
    "    results.iloc[:,:9].to_csv('../data/thesis/node_all_results.csv', index=False)\n",
    "    print(\"\\nComplete results table:\")\n",
    "    display(results.iloc[:,:9])\n",
    "else:\n",
    "    print(\"You have to first reproduce the results for the NoDE dataset.\\n\"\n",
    "          \"../code_relation_prediction/pytorch ./run_all_node.sh comp\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The comparative results on the Political dataset (Task 1: Related/Unrelated). \n",
    "# 10-Fold stratified cross-validation is used \n",
    "\n",
    "# Parameters:\n",
    "# Fixed: input=both, seq_len=256, warmup_prop=0.1, seed=42\n",
    "# model=base-uncased, epochs=5, batch_size=12, lr=2e-5\n",
    "\n",
    "# Only run this cell, if the Political Task 1 comparative training has been done\n",
    "if os.path.isdir(\"../pytorch/res/pol_ru\"):\n",
    "    # Read the results\n",
    "    eval_results = pd.read_csv('../pytorch/res/pol_ru/eval_results.tsv', sep='\\t')\n",
    "    # Read the predictions\n",
    "    eval_preds = pd.read_csv('../pytorch/res/pol_ru/eval_preds.csv')\n",
    "    \n",
    "    # Calculate precision, recall and F1 for all Folds\n",
    "    preds = pd.Series()\n",
    "    re_pre_f1_dict = {'Precision related' : [], 'Precision unrelated': [], \n",
    "                      'Recall related': [], 'Recall unrelated': [], 'F1 related': [], 'F1 unrelated': []}\n",
    "    scores_df = pd.DataFrame(re_pre_f1_dict)\n",
    "    count = 0\n",
    "    for i, row in eval_preds.iterrows():\n",
    "        preds_split_i = pd.Series(row.values[~row.str.contains('bert*', na=False, regex=True)]).dropna().astype('int8')    \n",
    "        preds_split_i = preds_split_i.replace({0: 'related', 1: 'unrelated'})\n",
    "        preds = preds.append(preds_split_i)\n",
    "        length = len(preds_split_i)\n",
    "        pre, rec, f1, support = precision_recall_fscore_support(pol_ru_test_df.iloc[count:count+length]['label'], preds_split_i, labels=['related', 'unrelated'])\n",
    "        count += length\n",
    "        scores_df.loc[i] = np.array((pre,rec,f1)).reshape((1,-1))[0]\n",
    "        if i == 9:\n",
    "            break\n",
    "    pol_ru_test_df['preds'] = preds.values\n",
    "\n",
    "    # Calculate the macro-averages\n",
    "    scores_df['Average Precision'] = (scores_df['Precision related']+scores_df['Precision unrelated'])/2\n",
    "    scores_df['Average Recall'] = (scores_df['Recall related']+scores_df['Recall unrelated'])/2\n",
    "    scores_df['Average F1'] = (scores_df['F1 related']+scores_df['F1 unrelated'])/2\n",
    "\n",
    "    print(\"Precision, Recall, F1, Macro-averages (Mean + std) for Task 1 on all folds:\")\n",
    "    display(scores_df.agg([np.mean, np.std, np.min, np.max]))\n",
    "    \n",
    "    print('Only calculate the scores on the complete predictions:')\n",
    "    print(classification_report(pol_ru_test_df['label'], pol_ru_test_df['preds'], labels=['related', 'unrelated']))\n",
    "\n",
    "else:\n",
    "    print('You have to first reproduce the results for the Polical dataset Task 1.\\n'\n",
    "          'python run_classifier_ba.py  --task_name \"political-ru\" --output_dir res/pol_ru/crossval1 --do_cross_val --do_lower_case --num_train_epochs 5 --max_seq_length 256 --train_batch_size 12 --learning_rate 2e-5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The comparative results on the Political dataset (Task 2: Attack/Support). \n",
    "# 10-Fold stratified cross-validation is used \n",
    "\n",
    "# Parameters:\n",
    "# Fixed: input=both, seq_len=256, warmup_prop=0.1, seed=42\n",
    "# model=base-uncased, epochs=5, batch_size=12, lr=2e-5\n",
    "\n",
    "# Only run this cell, if the Political Task 2 comparative training has been done\n",
    "if os.path.isdir(\"../pytorch/res/pol_as\"):\n",
    "    # Read the results\n",
    "    eval_results = pd.read_csv('../pytorch/res/pol_as/eval_results.tsv', sep='\\t')\n",
    "    # Read the predictions\n",
    "    eval_preds = pd.read_csv('../pytorch/res/pol_as/eval_preds.csv')\n",
    "    \n",
    "    # Calculate precision, recall and F1 for all Folds\n",
    "    preds = pd.Series()\n",
    "    re_pre_f1_dict = {'Precision attack' : [], 'Precision support': [], \n",
    "                  'Recall attack': [], 'Recall support': [], 'F1 attack': [], 'F1 support': []}\n",
    "    scores_df = pd.DataFrame(re_pre_f1_dict)\n",
    "    count = 0\n",
    "    for i, row in eval_preds.iterrows():\n",
    "        preds_split_i = pd.Series(row.values[~row.str.contains('bert*', na=False, regex=True)]).dropna().astype('int8')    \n",
    "        preds_split_i = preds_split_i.replace({0: 'attack', 1: 'support'})\n",
    "        preds = preds.append(preds_split_i)\n",
    "        length = len(preds_split_i)\n",
    "        pre, rec, f1, support = precision_recall_fscore_support(pol_as_test_df.iloc[count:count+length]['label'], preds_split_i, labels=['attack', 'support'])\n",
    "        count += length\n",
    "        scores_df.loc[i] = np.array((pre,rec,f1)).reshape((1,-1))[0]\n",
    "        if i == 9:\n",
    "            break\n",
    "    pol_as_test_df['preds'] = preds.values\n",
    "\n",
    "    # Calculate the macro-averages\n",
    "    scores_df['Average Precision'] = (scores_df['Precision attack']+scores_df['Precision support'])/2\n",
    "    scores_df['Average Recall'] = (scores_df['Recall attack']+scores_df['Recall support'])/2\n",
    "    scores_df['Average F1'] = (scores_df['F1 attack']+scores_df['F1 support'])/2\n",
    "\n",
    "    print(\"Precision, Recall, F1, Macro-averages (Mean + std) for Task 2 on all folds:\")\n",
    "    display(scores_df.agg([np.mean, np.std, np.min, np.max]))\n",
    "    \n",
    "    print('Only calculate the scores on the complete predictions:')\n",
    "    print(classification_report(pol_as_test_df['label'], pol_as_test_df['preds'], labels=['attack', 'support']))\n",
    "\n",
    "else:\n",
    "    print('You have to first reproduce the results for the Polical dataset Task 2.\\n'\n",
    "          'python run_classifier_ba.py  --task_name \"political-as\" --output_dir res/pol_as/crossval1 --do_cross_val --do_lower_case --num_train_epochs 5 --max_seq_length 256 --train_batch_size 12 --learning_rate 2e-5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The comparative results on the Political dataset (Task 3: Attack/Support/Unrelated). \n",
    "# 10-Fold stratified cross-validation is used \n",
    "\n",
    "# Parameters:\n",
    "# Fixed: input=both, seq_len=256, warmup_prop=0.1, seed=42\n",
    "# model=base-uncased, epochs=5, batch_size=12, lr=2e-5\n",
    "\n",
    "# Only run this cell, if the Political Task 3 comparative training has been done\n",
    "if os.path.isdir(\"../pytorch/res/pol_asu\"):\n",
    "    # Read the results\n",
    "    eval_results = pd.read_csv('../pytorch/res/pol_asu/eval_results.tsv', sep='\\t')\n",
    "    # Read the predictions\n",
    "    eval_preds = pd.read_csv('../pytorch/res/pol_asu/eval_preds.csv')\n",
    "    \n",
    "    # Calculate precision, recall and F1 for all Folds\n",
    "    preds = pd.Series()\n",
    "    re_pre_f1_dict = {'Precision attack' : [], 'Precision support': [], 'Precision unrelated': [],\n",
    "                  'Recall attack': [], 'Recall support': [], 'Recall unrelated': [],\n",
    "                  'F1 attack': [], 'F1 support': [], 'F1 unrelated': []}\n",
    "    scores_df = pd.DataFrame(re_pre_f1_dict)\n",
    "    count = 0\n",
    "    for i, row in eval_preds.iterrows():\n",
    "        preds_split_i = pd.Series(row.values[~row.str.contains('bert*', na=False, regex=True)]).dropna().astype('int8')    \n",
    "        preds_split_i = preds_split_i.replace({0: 'attack', 1: 'support', 2: 'unrelated'})\n",
    "        preds = preds.append(preds_split_i)\n",
    "        length = len(preds_split_i)\n",
    "        pre, rec, f1, support = precision_recall_fscore_support(pol_asu_test_df.iloc[count:count+length]['label'], preds_split_i, labels=['attack', 'support', 'unrelated'])\n",
    "        count += length\n",
    "        scores_df.loc[i] = np.array((pre,rec,f1)).reshape((1,-1))[0]\n",
    "        if i == 9:\n",
    "            break\n",
    "    pol_asu_test_df['preds'] = preds.values\n",
    "\n",
    "    # Calculate the macro-averages\n",
    "    scores_df['Average Precision'] = (scores_df['Precision attack']+scores_df['Precision support']+scores_df['Precision unrelated'])/3\n",
    "    scores_df['Average Recall'] = (scores_df['Recall attack']+scores_df['Recall support']+scores_df['Recall unrelated'])/3\n",
    "    scores_df['Average F1'] = (scores_df['F1 attack']+scores_df['F1 support']+scores_df['F1 unrelated'])/3\n",
    "\n",
    "    print(\"Precision, Recall, F1, Macro-averages (Mean + std) for Task 3 on all folds:\")\n",
    "    display(scores_df.agg([np.mean, np.std, np.min, np.max]))\n",
    "    \n",
    "    print('Only calculate the scores on the complete predictions:')\n",
    "    print(classification_report(pol_asu_test_df['label'], pol_asu_test_df['preds'], labels=['attack', 'support', 'unrelated']))\n",
    "    \n",
    "    print('Major class baseline:')\n",
    "    print(classification_report(pol_asu_test_df['label'], ['unrelated']*1460, labels=['attack', 'support', 'unrelated']))\n",
    "\n",
    "\n",
    "else:\n",
    "    print('You have to first reproduce the results for the Polical dataset Task 3.\\n'\n",
    "          'python run_classifier_ba.py  --task_name \"political-asu\" --output_dir res/pol_asu/crossval1 --do_cross_val --do_lower_case --num_train_epochs 5 --max_seq_length 256 --train_batch_size 12 --learning_rate 2e-5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The comparative results on the Agreement dataset. \n",
    "# 10-Fold stratified cross-validation is used \n",
    "\n",
    "# Parameters:\n",
    "# Fixed: input=both, seq_len=256, warmup_prop=0.1, seed=42\n",
    "# model=base-uncased, epochs=3, batch_size=12, lr=2e-5\n",
    "\n",
    "# Only run this cell, if the Agreement comparative training has been done\n",
    "if os.path.isdir(\"../pytorch/res/agreement_new\"):\n",
    "    # Read the results\n",
    "    eval_results = pd.read_csv('../pytorch/res/agreement_new/eval_results.tsv', sep='\\t')\n",
    "    # Read the predictions\n",
    "    eval_preds = pd.read_csv('../pytorch/res/agreement_new/eval_preds.csv')\n",
    "    \n",
    "    # Calculate precision, recall and F1 for all Folds\n",
    "    preds = pd.Series()\n",
    "    re_pre_f1_dict = {'Precision agreement' : [], 'Precision disagreement': [], \n",
    "                  'Recall agreement': [], 'Recall disagreement': [], 'F1 agreement': [], 'F1 disagreement': [], \"Accuracy\": []}\n",
    "    scores_df = pd.DataFrame(re_pre_f1_dict)\n",
    "    count = 0\n",
    "    for i, row in eval_preds.iterrows():\n",
    "        preds_split_i = pd.Series(row.values[~row.str.contains('bert*', na=False, regex=True)]).dropna().astype('int8')    \n",
    "        preds_split_i = preds_split_i.replace({0: 'agreement', 1: 'disagreement'})\n",
    "        preds = preds.append(preds_split_i)\n",
    "        length = len(preds_split_i)\n",
    "        pre, rec, f1, support = precision_recall_fscore_support(ag_test_df.iloc[count:count+length]['label'], preds_split_i, labels=['agreement', 'disagreement'])\n",
    "        acc = np.array([accuracy_score(ag_test_df.iloc[count:count+length]['label'], preds_split_i)])\n",
    "        count += length\n",
    "        scores_df.loc[i] = np.concatenate((pre,rec,f1,acc)).ravel()\n",
    "        if i == 9:\n",
    "            break\n",
    "    ag_test_df['preds'] = preds.values\n",
    "\n",
    "    print(\"Precision, Recall, F1, Accuracy (Mean + std) for Agreement on all folds:\")\n",
    "    display(scores_df.agg([np.mean, np.std, np.min, np.max]))\n",
    "\n",
    "\n",
    "else:\n",
    "    print('You have to first reproduce the results for the Agreement dataset.\\n'\n",
    "          'python run_classifier_ba.py  --task_name \"agreement\" --output_dir res/agreement_new/crossval1 --do_cross_val --do_lower_case --num_train_epochs 3 --max_seq_length 256 --train_batch_size 12 --learning_rate 2e-5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Additional Experiments <a name=\"addexps\"></a>\n",
    "\n",
    "In the following the results of the additional experiments are presented."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The additional procon results on the NoDE dataset. The network is always trained on the train dataset + procon \n",
    "# and evaluated on the test dataset. \n",
    "\n",
    "# Parameters:\n",
    "# Fixed: input=both, seq_len=128, warmup_prop=0.1, seed=42-71\n",
    "# Tested: model=base-uncased, epochs=4,5, batch_size=12,16, lr=2e-5, 3e-5\n",
    "\n",
    "# Only run this cell, if the NoDE additional procon training has been done\n",
    "if os.path.isdir(\"../pytorch/res/node_both_procon\"):\n",
    "    # Read in all testing results\n",
    "    eval_results = pd.read_csv('../pytorch/res/node_both_procon/eval_results.tsv', sep='\\t')\n",
    "    # Read in all testing predictions\n",
    "    eval_preds = pd.read_csv('../pytorch/res/node_both_procon/eval_preds.csv')\n",
    "    # Group all runs with the same settings (different seeds) together\n",
    "    eval_results_grouped = eval_results.groupby(['_bert-model', '_num_epochs', '_batch_size','_gradient_acc' ,'_learning_rate' ])\n",
    "    # Aggregate mean, min, max, std and median\n",
    "    results = eval_results_grouped['acc'].agg([np.mean, np.min, np.max, np.std, np.median])\n",
    "    \n",
    "    # Print the statistics reported in the thesis\n",
    "    print(\"Best mean result with parameters and other statistics:\\n\", results.loc[results['mean'].idxmax()]) \n",
    "    print(\"\\nOverall mean:\\n\", eval_results['acc'].mean())\n",
    "    print(\"\\nWorst (single) run:\\n\", eval_results['acc'].min())\n",
    "    print(\"\\nBest (single) run:\\n\", eval_results['acc'].max())\n",
    "    \n",
    "    # Formats and shows the complete result table\n",
    "    results = results.reset_index()\n",
    "    results = results.rename(columns={element: element.replace(\"_\", \"-\") for element in results.columns.tolist()})\n",
    "    results = results.rename(columns={element: re.sub(\"^-\", \"\", element) for element in results.columns.tolist()})\n",
    "    results = results.replace({\"bert-base-uncased\": 'bbu', \"bert-large-uncased\": 'blu'})\n",
    "    results['batch-size'] = results['batch-size'] * results['gradient-acc']\n",
    "    results = results.drop(columns=[\"gradient-acc\"])\n",
    "    results.iloc[:,:9].to_csv('../data/thesis/node_procon_results.csv', index=False)\n",
    "    print(\"\\nComplete results table:\")\n",
    "    display(results.iloc[:,:8])\n",
    "else:\n",
    "    print(\"You have to first reproduce the results for the NoDE procon dataset.\\n\"\n",
    "          \"../code_relation_prediction/pytorch ./run_all_node.sh procon\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The additional results on the Political dataset (Task 1+2). \n",
    "# 5-Fold leave-one-group-out (topics) cross-validation is used \n",
    "\n",
    "# Parameters:\n",
    "# Fixed: input=both, seq_len=256, warmup_prop=0.1, seed=42\n",
    "# model=base-uncased, epochs=5, batch_size=12, lr=2e-5\n",
    "\n",
    "# Only run this cell, if the Political Task 1 additional training has been done\n",
    "if os.path.isdir(\"../pytorch/res/pol_ru_topics\"):\n",
    "    # Load results\n",
    "    eval_results = pd.read_csv('../pytorch/res/pol_ru_topics/eval_results.tsv', sep='\\t')\n",
    "    # Aggregate and display\n",
    "    print(\"Task 1:\")\n",
    "    results = eval_results.groupby(['_bert-model', '_num_epochs', '_batch_size','_gradient_acc' ,'_learning_rate'])['f1'].agg([np.mean, np.min, np.max, np.std])\n",
    "    display(results)\n",
    "else:\n",
    "    print('You have to first reproduce the results for the Political Task 1 additional dataset.\\n'\n",
    "          '../code_relation_prediction/pytorch python run_classifier_ba.py  --task_name \"political-ru-topics\" --output_dir res/pol_ru_topics/crossval1 --do_cross_val --do_lower_case --num_train_epochs 5 --max_seq_length 256 --train_batch_size 12 --learning_rate 2e-5')\n",
    "    \n",
    "# Only run this cell, if the Political Task 2 additional training has been done\n",
    "if os.path.isdir(\"../pytorch/res/pol_as_topics\"):\n",
    "    # Load results\n",
    "    eval_results = pd.read_csv('../pytorch/res/pol_as_topics/eval_results.tsv', sep='\\t')\n",
    "    # Aggregate and display\n",
    "    print(\"Task 2:\")\n",
    "    results = eval_results.groupby(['_bert-model', '_num_epochs', '_batch_size','_gradient_acc' ,'_learning_rate'])['f1'].agg([np.mean, np.min, np.max, np.std])\n",
    "    display(results)\n",
    "else:\n",
    "    print('You have to first reproduce the results for the Political Task 2 additional dataset.\\n'\n",
    "          '../code_relation_prediction/pytorch python run_classifier_ba.py  --task_name \"political-as-topics\" --output_dir res/pol_as_topics/crossval1 --do_cross_val --do_lower_case --num_train_epochs 5 --max_seq_length 256 --train_batch_size 12 --learning_rate 2e-5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The additional results on the Agreement dataset. \n",
    "# Topic independent 80/20 train/test split\n",
    "\n",
    "# Parameters:\n",
    "# Fixed: input=both, seq_len=256, warmup_prop=0.1, seed=42\n",
    "# model=base-uncased, epochs=3, batch_size=12, lr=2e-5\n",
    "\n",
    "# Only run this cell, if the Political Task 2 additional training has been done\n",
    "if os.path.isdir(\"../pytorch/res/agreement_topics_new\"):\n",
    "    # Load results\n",
    "    eval_results = pd.read_csv('../pytorch/res/agreement_topics_new/eval_results.tsv', sep='\\t')\n",
    "    # Aggregate and display\n",
    "    results = eval_results.groupby(['_bert-model', '_num_epochs', '_batch_size','_gradient_acc' ,'_learning_rate'])['f1'].agg([np.mean, np.min, np.max, np.std])\n",
    "    display(results)\n",
    "else:\n",
    "    print('You have to first reproduce the results for the Agreement additional dataset.\\n'\n",
    "          '../code_relation_prediction/pytorch python run_classifier_ba.py  --task_name \"agreement-topics\" --output_dir res/agreement_topics_new/train_test1 --do_train --do_eval --do_lower_case --num_train_epochs 3 --max_seq_length 256 --train_batch_size 12 --learning_rate 2e-5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The additional only org results on the NoDE dataset. The network is always trained on the train dataset\n",
    "# and evaluated on the test dataset. \n",
    "\n",
    "# Parameters:\n",
    "# Fixed: input=org, seq_len=128, warmup_prop=0.1, seed=42-71\n",
    "# Tested: model=base-uncased, epochs=4,5, batch_size=12,16, lr=2e-5, 3e-5\n",
    "\n",
    "# Only run this cell, if the NoDE additional org training has been done\n",
    "if os.path.isdir(\"../pytorch/res/node_org\"):\n",
    "    # Read in all testing results\n",
    "    eval_results = pd.read_csv('../pytorch/res/node_org/eval_results.tsv', sep='\\t')\n",
    "    # Read in all testing predictions\n",
    "    eval_preds = pd.read_csv('../pytorch/res/node_org/eval_preds.csv')\n",
    "    # Group all runs with the same settings (different seeds) together\n",
    "    eval_results_grouped = eval_results.groupby(['_bert-model', '_num_epochs', '_batch_size','_gradient_acc' ,'_learning_rate' ])\n",
    "    # Aggregate mean, min, max, std and median\n",
    "    results = eval_results_grouped['acc'].agg([np.mean, np.min, np.max, np.std, np.median])\n",
    "    \n",
    "    # Print the statistics reported in the thesis\n",
    "    print(\"Best mean result with parameters and other statistics:\\n\", results.loc[results['mean'].idxmax()]) \n",
    "    print(\"\\nOverall mean:\\n\", eval_results['acc'].mean())\n",
    "    print(\"\\nWorst (single) run:\\n\", eval_results['acc'].min())\n",
    "    print(\"\\nBest (single) run:\\n\", eval_results['acc'].max())\n",
    "    \n",
    "    # Formats and shows the complete result table\n",
    "    results = results.reset_index()\n",
    "    results = results.rename(columns={element: element.replace(\"_\", \"-\") for element in results.columns.tolist()})\n",
    "    results = results.rename(columns={element: re.sub(\"^-\", \"\", element) for element in results.columns.tolist()})\n",
    "    results = results.replace({\"bert-base-uncased\": 'bbu', \"bert-large-uncased\": 'blu'})\n",
    "    results['batch-size'] = results['batch-size'] * results['gradient-acc']\n",
    "    results = results.drop(columns=[\"gradient-acc\"])\n",
    "    results.iloc[:,:9].to_csv('../data/thesis/node_onlyorg_results.csv', index=False)\n",
    "    print(\"\\nComplete results table:\")\n",
    "    display(results.iloc[:,:8])\n",
    "else:\n",
    "    print(\"You have to first reproduce the results for the NoDE org dataset.\\n\"\n",
    "          \"../code_relation_prediction/pytorch ./run_all_node.sh only\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The additional results on the Political dataset (Task 1+2). \n",
    "# 10-Fold stratified cross-validation is used \n",
    "\n",
    "# Parameters:\n",
    "# Fixed: input=org, seq_len=256, warmup_prop=0.1, seed=42\n",
    "# model=base-uncased, epochs=5, batch_size=12, lr=2e-5\n",
    "\n",
    "# Only run this cell, if the Political Task 1 additional org training has been done\n",
    "if os.path.isdir(\"../pytorch/res/pol_ru_org\"):\n",
    "    # Load results\n",
    "    eval_results = pd.read_csv('../pytorch/res/pol_ru_org/eval_results.tsv', sep='\\t')\n",
    "    # Aggregate and display\n",
    "    print(\"Task 1:\")\n",
    "    results = eval_results.groupby(['_bert-model', '_num_epochs', '_batch_size','_gradient_acc' ,'_learning_rate'])['f1'].agg([np.mean, np.min, np.max, np.std])\n",
    "    display(results)\n",
    "else:\n",
    "    print('You have to first reproduce the results for the Political Task 1 additional org dataset.\\n'\n",
    "          '../code_relation_prediction/pytorch python run_classifier_ba.py  --task_name \"political-as\" --output_dir res/pol_as_org/crossval1 --input_to_use \"org\" --do_cross_val --do_lower_case --num_train_epochs 5 --max_seq_length 256 --train_batch_size 12 --learning_rate 2e-5')\n",
    "# Only run this cell, if the Political Task 2 additional org training has been done\n",
    "if os.path.isdir(\"../pytorch/res/pol_as_org\"):\n",
    "    # Load results\n",
    "    eval_results = pd.read_csv('../pytorch/res/pol_as_org/eval_results.tsv', sep='\\t')\n",
    "    # Aggregate and display\n",
    "    print(\"Task 2:\")\n",
    "    results = eval_results.groupby(['_bert-model', '_num_epochs', '_batch_size','_gradient_acc' ,'_learning_rate'])['f1'].agg([np.mean, np.min, np.max, np.std])\n",
    "    display(results)\n",
    "else:\n",
    "    print('You have to first reproduce the results for the Political Task 2 additional org dataset.\\n'\n",
    "          '../code_relation_prediction/pytorch python run_classifier_ba.py  --task_name \"political-ru\" --output_dir res/pol_ru_org/crossval1 --input_to_use \"org\" --do_cross_val --do_lower_case --num_train_epochs 5 --max_seq_length 256 --train_batch_size 12 --learning_rate 2e-5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The additional only resp results on the NoDE dataset. The network is always trained on the train dataset\n",
    "# and evaluated on the test dataset. \n",
    "\n",
    "# Parameters:\n",
    "# Fixed: input=response, seq_len=128, warmup_prop=0.1, seed=42-71\n",
    "# Tested: model=base-uncased, epochs=4,5, batch_size=12,16, lr=2e-5, 3e-5\n",
    "\n",
    "# Only run this cell, if the NoDE additional org training has been done\n",
    "if os.path.isdir(\"../pytorch/res/node_resp\"):\n",
    "    # Read in all testing results\n",
    "    eval_results = pd.read_csv('../pytorch/res/node_resp/eval_results.tsv', sep='\\t')\n",
    "    # Read in all testing predictions\n",
    "    eval_preds = pd.read_csv('../pytorch/res/node_resp/eval_preds.csv')\n",
    "    # Group all runs with the same settings (different seeds) together\n",
    "    eval_results_grouped = eval_results.groupby(['_bert-model', '_num_epochs', '_batch_size','_gradient_acc' ,'_learning_rate' ])\n",
    "    # Aggregate mean, min, max, std and median\n",
    "    results = eval_results_grouped['acc'].agg([np.mean, np.min, np.max, np.std, np.median])\n",
    "    \n",
    "    # Print the statistics reported in the thesis\n",
    "    print(\"Best mean result with parameters and other statistics:\\n\", results.loc[results['mean'].idxmax()]) \n",
    "    print(\"\\nOverall mean:\\n\", eval_results['acc'].mean())\n",
    "    print(\"\\nWorst (single) run:\\n\", eval_results['acc'].min())\n",
    "    print(\"\\nBest (single) run:\\n\", eval_results['acc'].max())\n",
    "    \n",
    "    # Formats and shows the complete result table\n",
    "    results = results.reset_index()\n",
    "    results = results.rename(columns={element: element.replace(\"_\", \"-\") for element in results.columns.tolist()})\n",
    "    results = results.rename(columns={element: re.sub(\"^-\", \"\", element) for element in results.columns.tolist()})\n",
    "    results = results.replace({\"bert-base-uncased\": 'bbu', \"bert-large-uncased\": 'blu'})\n",
    "    results['batch-size'] = results['batch-size'] * results['gradient-acc']\n",
    "    results = results.drop(columns=[\"gradient-acc\"])\n",
    "    results.iloc[:,:9].to_csv('../data/thesis/node_onlyorg_results.csv', index=False)\n",
    "    print(\"\\nComplete results table:\")\n",
    "    display(results.iloc[:,:8])\n",
    "else:\n",
    "    print(\"You have to first reproduce the results for the NoDE resp dataset.\\n\"\n",
    "          \"../code_relation_prediction/pytorch ./run_all_node.sh only\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The additional results on the Political dataset (Task 1+2). \n",
    "# 10-Fold stratified cross-validation is used \n",
    "\n",
    "# Parameters:\n",
    "# Fixed: input=response, seq_len=256, warmup_prop=0.1, seed=42\n",
    "# model=base-uncased, epochs=5, batch_size=12, lr=2e-5\n",
    "\n",
    "# Only run this cell, if the Political Task 1 additional resp training has been done\n",
    "if os.path.isdir(\"../pytorch/res/pol_ru_resp\"):\n",
    "    # Load results\n",
    "    eval_results = pd.read_csv('../pytorch/res/pol_ru_resp/eval_results.tsv', sep='\\t')\n",
    "    # Aggregate and display\n",
    "    print(\"Task 1:\")\n",
    "    results = eval_results.groupby(['_bert-model', '_num_epochs', '_batch_size','_gradient_acc' ,'_learning_rate'])['f1'].agg([np.mean, np.min, np.max, np.std])\n",
    "    display(results)\n",
    "else:\n",
    "    print('You have to first reproduce the results for the Political Task 1 additional resp dataset.\\n'\n",
    "          '../code_relation_prediction/pytorch python run_classifier_ba.py  --task_name \"political-as\" --output_dir res/pol_as_resp/crossval1 --input_to_use \"response\" --do_cross_val --do_lower_case --num_train_epochs 5 --max_seq_length 256 --train_batch_size 12 --learning_rate 2e-5')\n",
    "# Only run this cell, if the Political Task 2 additional resp training has been done\n",
    "if os.path.isdir(\"../pytorch/res/pol_as_resp\"):\n",
    "    # Load results\n",
    "    eval_results = pd.read_csv('../pytorch/res/pol_as_resp/eval_results.tsv', sep='\\t')\n",
    "    # Aggregate and display\n",
    "    print(\"Task 2:\")\n",
    "    results = eval_results.groupby(['_bert-model', '_num_epochs', '_batch_size','_gradient_acc' ,'_learning_rate'])['f1'].agg([np.mean, np.min, np.max, np.std])\n",
    "    display(results)\n",
    "else:\n",
    "    print('You have to first reproduce the results for the Political Task 2 additional resp dataset.\\n'\n",
    "          '../code_relation_prediction/pytorch python run_classifier_ba.py  --task_name \"political-ru\" --output_dir res/pol_ru_resp/crossval1 --input_to_use \"response\" --do_cross_val --do_lower_case --num_train_epochs 5 --max_seq_length 256 --train_batch_size 12 --learning_rate 2e-5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The additional reversed results on the NoDE dataset. The network is always trained on the train dataset\n",
    "# and evaluated on the test dataset. \n",
    "\n",
    "# Parameters:\n",
    "# Fixed: input=resp-org, seq_len=128, warmup_prop=0.1, seed=42-71\n",
    "# Tested: model=base-uncased, epochs=4,5, batch_size=12,16, lr=2e-5, 3e-5\n",
    "\n",
    "# Only run this cell, if the NoDE additional org training has been done\n",
    "if os.path.isdir(\"../pytorch/res/node_both_reversed\"):\n",
    "    # Read in all testing results\n",
    "    eval_results = pd.read_csv('../pytorch/res/node_both_reversed/eval_results.tsv', sep='\\t')\n",
    "    # Read in all testing predictions\n",
    "    eval_preds = pd.read_csv('../pytorch/res/node_both_reversed/eval_preds.csv')\n",
    "    # Group all runs with the same settings (different seeds) together\n",
    "    eval_results_grouped = eval_results.groupby(['_bert-model', '_num_epochs', '_batch_size','_gradient_acc' ,'_learning_rate' ])\n",
    "    # Aggregate mean, min, max, std and median\n",
    "    results = eval_results_grouped['acc'].agg([np.mean, np.min, np.max, np.std, np.median])\n",
    "    \n",
    "    # Print the statistics reported in the thesis\n",
    "    print(\"Best mean result with parameters and other statistics:\\n\", results.loc[results['mean'].idxmax()]) \n",
    "    print(\"\\nOverall mean:\\n\", eval_results['acc'].mean())\n",
    "    print(\"\\nWorst (single) run:\\n\", eval_results['acc'].min())\n",
    "    print(\"\\nBest (single) run:\\n\", eval_results['acc'].max())\n",
    "    \n",
    "    # Formats and shows the complete result table\n",
    "    results = results.reset_index()\n",
    "    results = results.rename(columns={element: element.replace(\"_\", \"-\") for element in results.columns.tolist()})\n",
    "    results = results.rename(columns={element: re.sub(\"^-\", \"\", element) for element in results.columns.tolist()})\n",
    "    results = results.replace({\"bert-base-uncased\": 'bbu', \"bert-large-uncased\": 'blu'})\n",
    "    results['batch-size'] = results['batch-size'] * results['gradient-acc']\n",
    "    results = results.drop(columns=[\"gradient-acc\"])\n",
    "    results.iloc[:,:9].to_csv('../data/thesis/node_resporg_results.csv', index=False)\n",
    "    print(\"\\nComplete results table:\")\n",
    "    display(results.iloc[:,:8])\n",
    "else:\n",
    "    print(\"You have to first reproduce the results for the NoDE resp dataset.\\n\"\n",
    "          \"../code_relation_prediction/pytorch ./run_all_node.sh resporg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The additional results on the Political dataset (Task 1+2). \n",
    "# 10-Fold stratified cross-validation is used \n",
    "\n",
    "# Parameters:\n",
    "# Fixed: input=resp-org, seq_len=256, warmup_prop=0.1, seed=42\n",
    "# model=base-uncased, epochs=5, batch_size=12, lr=2e-5\n",
    "\n",
    "# Only run this cell, if the Political Task 1 additional resp-org training has been done\n",
    "if os.path.isdir(\"../pytorch/res/pol_ru_resporg\"):\n",
    "    # Load results\n",
    "    eval_results = pd.read_csv('../pytorch/res/pol_ru_resporg/eval_results.tsv', sep='\\t')\n",
    "    # Aggregate and display\n",
    "    print(\"Task 1:\")\n",
    "    results = eval_results.groupby(['_bert-model', '_num_epochs', '_batch_size','_gradient_acc' ,'_learning_rate'])['f1'].agg([np.mean, np.min, np.max, np.std])\n",
    "    display(results)\n",
    "else:\n",
    "    print('You have to first reproduce the results for the Political Task 1 additional resp-org dataset.\\n'\n",
    "          '../code_relation_prediction/pytorch python run_classifier_ba.py  --task_name \"political-as\" --output_dir res/pol_as_resporg/crossval1 --input_to_use \"response-org\" --do_cross_val --do_lower_case --num_train_epochs 5 --max_seq_length 256 --train_batch_size 12 --learning_rate 2e-5')\n",
    "# Only run this cell, if the Political Task 2 additional resp-org training has been done\n",
    "if os.path.isdir(\"../pytorch/res/pol_as_resporg\"):\n",
    "    # Load results\n",
    "    eval_results = pd.read_csv('../pytorch/res/pol_as_resporg/eval_results.tsv', sep='\\t')\n",
    "    # Aggregate and display\n",
    "    print(\"Task 2:\")\n",
    "    results = eval_results.groupby(['_bert-model', '_num_epochs', '_batch_size','_gradient_acc' ,'_learning_rate'])['f1'].agg([np.mean, np.min, np.max, np.std])\n",
    "    display(results)\n",
    "else:\n",
    "    print('You have to first reproduce the results for the Political Task 2 additional resp-org dataset.\\n'\n",
    "          '../code_relation_prediction/pytorch python run_classifier_ba.py  --task_name \"political-ru\" --output_dir res/pol_ru_resporg/crossval1 --input_to_use \"response-org\" --do_cross_val --do_lower_case --num_train_epochs 5 --max_seq_length 256 --train_batch_size 12 --learning_rate 2e-5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Critical Analysis  <a name=\"critana\"></a>\n",
    "\n",
    "Critical Analysis\n",
    "- Duplicates in Agreement\n",
    "- Author distributon + unique argument author\n",
    "- WordClouds Author\n",
    "- NoDe:\n",
    "    - Major\n",
    "    - Major by topic\n",
    "    - Sent 1+2\n",
    "    - Output in respect to the topics\n",
    "- Pol:\n",
    "    - RU:\n",
    "        - Major\n",
    "        - Major by topic\n",
    "        - Major by org\n",
    "        - Major by response\n",
    "        - Major by author\n",
    "        - Performance over topics (10-CV, 5-CV)\n",
    "    - AS:\n",
    "        - Major\n",
    "        - Major by topic\n",
    "        - Major by org\n",
    "        - Major by response\n",
    "        - Major by author\n",
    "        - Sent 1+2\n",
    "        - Performance over topics (10-CV, 5-CV)\n",
    "- Agrement:\n",
    "    - Major\n",
    "    - Major by topic\n",
    "    - Performance over topics (10-CV, train/test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

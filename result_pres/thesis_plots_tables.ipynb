{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Relation Prediction in Argument Mining With Pre-trained Deep Bidirectional Transformers - Core\n",
    "\n",
    "This notebook generates or shows all plots and tables used in the thesis. Some additional interesting material not mentioned or shown in the thesis, can be found in the other notebook. To run this file, first make sure that all requirements are satisified. The README has detailed information about how to satisfy the requirements. Not all training settings have to be retrained to run this notebook, the cells belonging to training settings not done will be simply skipped. The order is the same as in the thesis.\n",
    "\n",
    "## Table of Contents:\n",
    "1. [Materials and Methods](#methods)\n",
    "2. [Comparative Experiments](#results)\n",
    "3. [Additional Experiments](#addexps)\n",
    "4. [Critical Analysis](#critana)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "The following cells import and load the necessary data and libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Necessary imports and setups\n",
    "# Basic python imports\n",
    "import re\n",
    "import sys \n",
    "import os\n",
    "\n",
    "# For the sentiment baselines\n",
    "import nltk\n",
    "# Download Sentiment Lexicon\n",
    "nltk.download('vader_lexicon')\n",
    "\n",
    "# Datahandling and plotting\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Statistics and other stuff\n",
    "from sklearn.metrics import precision_recall_fscore_support, accuracy_score, classification_report\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from pytorch_pretrained_bert.tokenization import BertTokenizer\n",
    "\n",
    "\n",
    "# Get the exact splits of the data\n",
    "sys.path.append(os.path.abspath(\"../pytorch\"))\n",
    "from run_classifier_dataset_utils import processors\n",
    "\n",
    "# Settings\n",
    "# Do no hide rows in pandas\n",
    "pd.set_option('display.max_rows', 164)\n",
    "pd.set_option('precision', 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read all necessary data\n",
    "df = pd.read_csv('../data/complete_data.tsv', sep='\\t').astype({\"id\": str})\n",
    "data_stats_org = np.load('../data/stats/data_stats_org.npy', allow_pickle=True).item()\n",
    "data_stats_resp = np.load('../data/stats/data_stats_resp.npy', allow_pickle=True).item()\n",
    "data_stats_topic = np.load('../data/stats/data_stats_topic.npy', allow_pickle=True).item()\n",
    "data_stats_author = pd.read_csv('../data/stats/data_stats_author.tsv', sep='\\t')\n",
    "data_nix_ken = pd.read_csv('../data/stats/data_nix_ken.tsv', sep='\\t')\n",
    "\n",
    "# Init the data processors and get the correct labels\n",
    "node_pro = processors['node']('both')\n",
    "_, node_test_df = node_pro.get_dev_examples('../data')\n",
    "political_ru_pro = processors['political-ru']('both')\n",
    "pol_ru_test_df = pd.concat(np.array(political_ru_pro.get_splits('../data'))[:,3])\n",
    "political_as_pro = processors['political-as']('both')\n",
    "pol_as_test_df = pd.concat(np.array(political_as_pro.get_splits('../data'))[:,3])\n",
    "political_asu_pro = processors['political-asu']('both')\n",
    "pol_asu_test_df = pd.concat(np.array(political_asu_pro.get_splits('../data'))[:,3])\n",
    "agreement_pro = processors['agreement']('both')\n",
    "ag_test_df = pd.concat(np.array(agreement_pro.get_splits('../data'))[:,3])\n",
    "political_ru_topics_pro = processors['political-ru-topics']('both')\n",
    "pol_ru_to_test_df = pd.concat(np.array(political_ru_topics_pro.get_splits('../data'))[:,3])\n",
    "political_as_topics_pro = processors['political-as-topics']('both')\n",
    "pol_as_to_test_df = pd.concat(np.array(political_as_topics_pro.get_splits('../data'))[:,3])\n",
    "agreement_topics_pro = processors['agreement-topics']('both')\n",
    "_, ag_to_test_df = agreement_topics_pro.get_dev_examples('../data')\n",
    "\n",
    "# Init the sentiment analyzer\n",
    "sid = SentimentIntensityAnalyzer()\n",
    "\n",
    "# Define helper functions\n",
    "def get_major_acc(x, classes=['Unrelated', 'Attack/Disagreement', 'Support/Agreement']):\n",
    "    \"\"\"Returns the accuracy of the major class.\"\"\"\n",
    "    return np.divide(x[classes].max(), np.sum(x[classes]))\n",
    "\n",
    "def get_major_class(x, classes=['Unrelated', 'Attack/Disagreement', 'Support/Agreement']):\n",
    "    \"\"\"Returns the name of the major class.\"\"\"\n",
    "    return x[classes].astype('float64').idxmax()\n",
    "\n",
    "def disc_pol(x):\n",
    "    \"\"\"Discretize the float sentiment polarity.\"\"\"\n",
    "    if x >= 0.00:\n",
    "        return 'positive'\n",
    "    else:\n",
    "        return 'negative'\n",
    "    \n",
    "def count_values(x, labels):\n",
    "    \"\"\"Count how many rows in x have a label in labels.\"\"\"\n",
    "    return x['label'].loc[x['label'].isin(labels)].count()\n",
    "\n",
    "def convert_preds_topic(df_preds, df_res, replace_dict, count, print_clr=True):\n",
    "    \"\"\"Prints the classification reports for every topic.\"\"\"\n",
    "    labels = list(replace_dict.values())\n",
    "    preds = pd.Series()\n",
    "    for i, row in df_preds.iterrows():\n",
    "        preds = preds.append(pd.Series(row.values[~row.str.contains('bert*', na=False, regex=True)]))\n",
    "        if i == count:\n",
    "            break\n",
    "    preds = preds.dropna().astype(int)\n",
    "    df_res['preds'] = preds.values\n",
    "    df_res = df_res.replace(replace_dict)\n",
    "    \n",
    "    if print_clr:\n",
    "        for name, group in df_res.groupby('topic'):\n",
    "            print(\"Topic: {}\".format(name))\n",
    "            print(classification_report(group['label'], group['preds'], labels=labels))\n",
    "    \n",
    "    #display(pd.crosstab(df_res['topic'], [df_res['label'], df_res['preds']]))\n",
    "    results_topic = pd.crosstab(df_res['topic'], [df_res['label'], df_res['preds']])\n",
    "    results_topic['total'] = results_topic.agg([np.sum], axis=1)\n",
    "    results_topic['correctness'] = results_topic.apply(\n",
    "        lambda r: (r[labels[0], labels[0]]+r[labels[1], labels[1]])/r['total'], axis=1)\n",
    "    display(results_topic.sort_values(by='correctness'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Materials and Methods <a name=\"methods\"></a>\n",
    "\n",
    "Three datasets were used in this thesis. For every dataset a link where the dataset is available and a link to the describing paper is provided.\n",
    "- NoDE:\n",
    "    - Link: http://www-sop.inria.fr/NoDE/NoDE-xml.html\n",
    "    - Paper: https://pdfs.semanticscholar.org/16d1/6b8a37c5313fa8c8430fddc011f2a98d20c5.pdf\n",
    "- Political\n",
    "    - Link: https://dh.fbk.eu/resources/political-argumentation\n",
    "    - Paper: https://aaai.org/ocs/index.php/AAAI/AAAI18/paper/view/16393/16020\n",
    "- Agreement\n",
    "    - Link: https://dh.fbk.eu/resources/agreement-disagreement\n",
    "    - Paper: https://www.aclweb.org/anthology/C16-1232"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NoDE (consisting of debatepedia train and test and procon)\n",
    "# General statistics for the datasets\n",
    "pd.concat((data_stats_topic['debate_train'], data_stats_topic['debate_test'], data_stats_topic['procon']), keys=['train', 'test', 'procon'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example tokenization\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)\n",
    "text = \"Violent games make youth more agressive/violent.\"\n",
    "print(tokenizer.tokenize(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Histogram of ingoing and outgoing links for the arguments in the NoDe debatepedia test set\n",
    "fig, (ax1, ax2) = plt.subplots(1,2, figsize=(10,4), sharex=True)  # 1 row, 2 columns\n",
    "\n",
    "data_set = 'debate_test'\n",
    "df_plot = pd.concat((data_stats_org[data_set].iloc[:-1], \n",
    "                     data_stats_resp[data_set].iloc[:-1].rename(\n",
    "                         columns={'Attacks': 'Attacked', 'Supports': 'Supported'})), keys=['Original', 'Response'])\n",
    "df_plot.loc['Original'].hist(column=['Total pairs'], ax=ax1, bins=[1,2,3,4,5,6,7,8,9], align='left', ec='black')\n",
    "df_plot.loc['Response'].hist(column=['Total pairs'], ax=ax2, bins=[1,2,3,4,5,6,7,8,9], align='left', ec='black')\n",
    "\n",
    "ax1.set_title(\"Original\")\n",
    "ax2.set_title(\"Response\")\n",
    "ax1.set_ylabel(\"Count\")\n",
    "ax1.set_xlabel(\"Number of ingoing links\")\n",
    "ax2.set_xlabel(\"Number of outgoing links\")\n",
    "ax1.set_xticks([1,2,3,4,5,6,7,8])\n",
    "ax1.grid(False)\n",
    "ax2.set_xticks([1,2,3,4,5,6,7,8])\n",
    "ax2.grid(False)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../data/thesis/node_hist.pdf', bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Boxplot of the lengths of arguments in WordPiece-Tokens for the debatepedia train and test set\n",
    "fig, ax = plt.subplots(1,2, figsize=(10,4), sharey=True)  # 1 row, 2 columns\n",
    "\n",
    "#for data_set, ax in [('debate_train', ax1),('debate_test', ax2)]:\n",
    "df_plot = pd.concat((df[df['org_dataset'] == 'debate_train'], df[df['org_dataset'] == 'debate_test']))\n",
    "df_plot = df_plot.rename(columns={'org_len': 'Original', 'response_len': 'Response', 'complete_len': 'Combined'})\n",
    "df_plot = df_plot.replace({'debate_test': 'Test', 'debate_train': 'Train'})\n",
    "df_plot.groupby('org_dataset', sort=False).boxplot(ax=ax)\n",
    "ax[0].set_ylabel('Length in WordPiece tokens')\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.savefig('../data/thesis/node_length.pdf', bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Political\n",
    "# General statistics for the dataset\n",
    "data_stats_topic['political']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Histogram of ingoing and outgoing links for the arguments in the Political dataset\n",
    "fig, (ax1, ax2) = plt.subplots(1,2, figsize=(10,4), sharex=False, sharey=True)  # 1 row, 2 columns\n",
    "\n",
    "data_set = 'political'\n",
    "df_plot = pd.concat((data_stats_org[data_set].iloc[:-1], \n",
    "                     data_stats_resp[data_set].iloc[:-1].rename(\n",
    "                         columns={'Attacks': 'Attacked', 'Supports': 'Supported'})), keys=['Original', 'Response'])\n",
    "df_plot.loc['Original'].hist(column=['Total pairs'], bins=[1,2,3,4,5,6,7,8,9], ax=ax1, align='left', ec='black')\n",
    "df_plot.loc['Response'].hist(column=['Total pairs'], bins=[1,2,3,4,5,6,7,8,9], ax=ax2, align='left', ec='black')\n",
    "\n",
    "ax1.set_title(\"Original\")\n",
    "ax2.set_title(\"Response\")\n",
    "ax1.set_ylabel(\"Count\")\n",
    "ax1.set_xlabel(\"Number of ingoing links\")\n",
    "ax2.set_xlabel(\"Number of outgoing links\")\n",
    "ax1.set_xticks([1,2,3,4,5,6,7,8])\n",
    "ax1.grid(False)\n",
    "ax2.set_xticks([1,2,3,4,5,6,7,8])\n",
    "ax2.grid(False)\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.savefig('../data/thesis/political_hist.pdf', bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Boxplot of the lengths of arguments in WordPiece-Tokens for the Political dataset\n",
    "fig, ax = plt.subplots(1,1, figsize=(5,4), sharey=True)  # 1 row, 2 columns\n",
    "\n",
    "#for data_set, ax in [('debate_train', ax1),('debate_test', ax2)]:\n",
    "df_plot = df[df['org_dataset'] == 'political']\n",
    "df_plot = df_plot.rename(columns={'org_len': 'Original', 'response_len': 'Response', 'complete_len': 'Combined'})\n",
    "df_plot.groupby('org_dataset', sort=False).boxplot(ax=ax)\n",
    "ax.set_title(\"\")\n",
    "ax.set_ylabel('Length in WordPiece tokens')\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.savefig('../data/thesis/political_length.pdf', bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Agreement\n",
    "# General statistics\n",
    "data_stats_topic['agreement']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Boxplot of the lengths of arguments in WordPiece-Tokens for the Agreement dataset\n",
    "fig, ax = plt.subplots(1,1, figsize=(5,4), sharey=True)  # 1 row, 2 columns\n",
    "\n",
    "df_plot = df[df['org_dataset'] == 'agreement']\n",
    "df_plot = df_plot.rename(columns={'org_len': 'Argument 1', 'response_len': 'Argument 2', 'complete_len': 'Combined'})\n",
    "df_plot.groupby('org_dataset', sort=False).boxplot(ax=ax)\n",
    "ax.set_title(\"\")\n",
    "ax.set_ylabel('Length in WordPiece tokens')\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.savefig('../data/thesis/agreement_length.pdf', bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparative Experiments <a name=\"results\"></a>\n",
    "\n",
    "In the following, the results for the comparative experiments for the three datasets are presented."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The comparative results on the NoDE dataset. The network is always trained on the train dataset \n",
    "# and evaluated on the test dataset. \n",
    "\n",
    "# Parameters:\n",
    "# Fixed: input=both, seq_len=128, warmup_prop=0.1, seed=42-51\n",
    "# Tested: model=base-uncased,large-uncased, epochs=3,4,5, batch_size=8,12,16, lr=2e-5, 3e-5, 5e-5\n",
    "# Gradient accumulation: batch_size/4 for bert_large \n",
    "\n",
    "# Only run this cell, if the NoDE comparative training has been done\n",
    "if os.path.isdir(\"../pytorch/res/node_both_paper\"):\n",
    "    # Read in all testing results\n",
    "    eval_results = pd.read_csv('../pytorch/res/node_both_paper/eval_results.tsv', sep='\\t')\n",
    "    # Read in all testing predictions\n",
    "    eval_preds = pd.read_csv('../pytorch/res/node_both_paper/eval_preds.csv')\n",
    "    # Group all runs with the same settings (different seeds) together\n",
    "    eval_results_grouped = eval_results.groupby(['_bert-model', '_num_epochs', '_batch_size','_gradient_acc' ,'_learning_rate' ])\n",
    "    # Aggregate mean, min, max, std and median\n",
    "    results = eval_results_grouped['acc'].agg([np.mean, np.min, np.max, np.std, np.median])\n",
    "    \n",
    "    # Print the statistics reported in the thesis\n",
    "    print(\"Best mean result with parameters and other statistics:\\n\", results.loc[results['mean'].idxmax()]) \n",
    "    print(\"\\nOverall mean:\\n\", eval_results['acc'].mean())\n",
    "    print(\"\\nWorst (single) run:\\n\", eval_results['acc'].min())\n",
    "    print(\"\\nBest (single) run:\\n\", eval_results['acc'].max())\n",
    "    print(\"\\nResults grouped per model:\\n\", eval_results.groupby('_bert-model')['acc'].agg([np.mean])) \n",
    "    \n",
    "    # Precision and recall for the best mean setting\n",
    "    bmodel, bepochs, bb, bga, blr = results.loc[results['mean'].idxmax()].name\n",
    "    best_pred_ps = eval_results.loc[(eval_results['_bert-model'] == bmodel) & \n",
    "                           (eval_results['_num_epochs'] == bepochs) & (eval_results['_batch_size'] == bb) &\n",
    "                           (eval_results['_learning_rate'] == blr)].index\n",
    "    res = pd.concat([node_test_df.reset_index(drop=True), \n",
    "                     eval_preds.iloc[best_pred_ps,:-1].transpose().reset_index(drop=True)], axis=1)\n",
    "    res = res.replace({0: 'attack', 1: 'support'})\n",
    "    re_pre_dict = {'Precision support' : [], 'Precision attack': [], 'Recall support': [], 'Recall attack': []}\n",
    "    for ind in best_pred_ps:\n",
    "        pre, rec, _, _ = precision_recall_fscore_support(res['label'], res[ind], labels=['support', 'attack'])\n",
    "        re_pre_dict['Precision support'].append(pre[0])\n",
    "        re_pre_dict['Precision attack'].append(pre[1])\n",
    "        re_pre_dict['Recall support'].append(rec[0])\n",
    "        re_pre_dict['Recall attack'].append(rec[1])\n",
    "    \n",
    "    print(\"\\nPrecision and recall for the best mean setting:\")\n",
    "    display(pd.DataFrame(re_pre_dict).agg([np.mean, np.std]))\n",
    "    \n",
    "    # Formats and shows the complete result table\n",
    "    results = results.reset_index()\n",
    "    results = results.rename(columns={element: element.replace(\"_\", \"-\") for element in results.columns.tolist()})\n",
    "    results = results.rename(columns={element: re.sub(\"^-\", \"\", element) for element in results.columns.tolist()})\n",
    "    results = results.replace({\"bert-base-uncased\": 'bbu', \"bert-large-uncased\": 'blu'})\n",
    "    results['batch-size'] = results['batch-size'] * results['gradient-acc']\n",
    "    results = results.drop(columns=[\"gradient-acc\"])\n",
    "    results.iloc[:,:9].to_csv('../data/thesis/node_all_results.csv', index=False)\n",
    "    print(\"\\nComplete results table:\")\n",
    "    display(results.iloc[:,:9])\n",
    "else:\n",
    "    print(\"You have to first reproduce the results for the NoDE dataset.\\n\"\n",
    "          \"../code_relation_prediction/pytorch ./run_all_node.sh comp\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The comparative results on the Political dataset (Task 1: Related/Unrelated). \n",
    "# 10-Fold stratified cross-validation is used \n",
    "\n",
    "# Parameters:\n",
    "# Fixed: input=both, seq_len=256, warmup_prop=0.1, seed=42\n",
    "# model=base-uncased, epochs=5, batch_size=12, lr=2e-5\n",
    "\n",
    "# Only run this cell, if the Political Task 1 comparative training has been done\n",
    "if os.path.isdir(\"../pytorch/res/pol_ru\"):\n",
    "    # Read the results\n",
    "    eval_results = pd.read_csv('../pytorch/res/pol_ru/eval_results.tsv', sep='\\t')\n",
    "    # Read the predictions\n",
    "    eval_preds = pd.read_csv('../pytorch/res/pol_ru/eval_preds.csv')\n",
    "    \n",
    "    # Calculate precision, recall and F1 for all Folds\n",
    "    preds = pd.Series()\n",
    "    re_pre_f1_dict = {'Precision related' : [], 'Precision unrelated': [], \n",
    "                      'Recall related': [], 'Recall unrelated': [], 'F1 related': [], 'F1 unrelated': []}\n",
    "    scores_df = pd.DataFrame(re_pre_f1_dict)\n",
    "    count = 0\n",
    "    for i, row in eval_preds.iterrows():\n",
    "        preds_split_i = pd.Series(row.values[~row.str.contains('bert*', na=False, regex=True)]).dropna().astype('int8')    \n",
    "        preds_split_i = preds_split_i.replace({0: 'related', 1: 'unrelated'})\n",
    "        preds = preds.append(preds_split_i)\n",
    "        length = len(preds_split_i)\n",
    "        pre, rec, f1, support = precision_recall_fscore_support(pol_ru_test_df.iloc[count:count+length]['label'], preds_split_i, labels=['related', 'unrelated'])\n",
    "        count += length\n",
    "        scores_df.loc[i] = np.array((pre,rec,f1)).reshape((1,-1))[0]\n",
    "        if i == 9:\n",
    "            break\n",
    "    pol_ru_test_df['preds'] = preds.values\n",
    "\n",
    "    # Calculate the macro-averages\n",
    "    scores_df['Average Precision'] = (scores_df['Precision related']+scores_df['Precision unrelated'])/2\n",
    "    scores_df['Average Recall'] = (scores_df['Recall related']+scores_df['Recall unrelated'])/2\n",
    "    scores_df['Average F1'] = (scores_df['F1 related']+scores_df['F1 unrelated'])/2\n",
    "\n",
    "    print(\"Precision, Recall, F1, Macro-averages (Mean + std) for Task 1 on all folds:\")\n",
    "    display(scores_df.agg([np.mean, np.std, np.min, np.max]))\n",
    "    \n",
    "    print('Only calculate the scores on the complete predictions:')\n",
    "    print(classification_report(pol_ru_test_df['label'], pol_ru_test_df['preds'], labels=['related', 'unrelated']))\n",
    "\n",
    "else:\n",
    "    print('You have to first reproduce the results for the Polical dataset Task 1.\\n'\n",
    "          'python run_classifier_ba.py  --task_name \"political-ru\" --output_dir res/pol_ru/crossval1 --do_cross_val --do_lower_case --num_train_epochs 5 --max_seq_length 256 --train_batch_size 12 --learning_rate 2e-5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The comparative results on the Political dataset (Task 2: Attack/Support). \n",
    "# 10-Fold stratified cross-validation is used \n",
    "\n",
    "# Parameters:\n",
    "# Fixed: input=both, seq_len=256, warmup_prop=0.1, seed=42\n",
    "# model=base-uncased, epochs=5, batch_size=12, lr=2e-5\n",
    "\n",
    "# Only run this cell, if the Political Task 2 comparative training has been done\n",
    "if os.path.isdir(\"../pytorch/res/pol_as\"):\n",
    "    # Read the results\n",
    "    eval_results = pd.read_csv('../pytorch/res/pol_as/eval_results.tsv', sep='\\t')\n",
    "    # Read the predictions\n",
    "    eval_preds = pd.read_csv('../pytorch/res/pol_as/eval_preds.csv')\n",
    "    \n",
    "    # Calculate precision, recall and F1 for all Folds\n",
    "    preds = pd.Series()\n",
    "    re_pre_f1_dict = {'Precision attack' : [], 'Precision support': [], \n",
    "                  'Recall attack': [], 'Recall support': [], 'F1 attack': [], 'F1 support': []}\n",
    "    scores_df = pd.DataFrame(re_pre_f1_dict)\n",
    "    count = 0\n",
    "    for i, row in eval_preds.iterrows():\n",
    "        preds_split_i = pd.Series(row.values[~row.str.contains('bert*', na=False, regex=True)]).dropna().astype('int8')    \n",
    "        preds_split_i = preds_split_i.replace({0: 'attack', 1: 'support'})\n",
    "        preds = preds.append(preds_split_i)\n",
    "        length = len(preds_split_i)\n",
    "        pre, rec, f1, support = precision_recall_fscore_support(pol_as_test_df.iloc[count:count+length]['label'], preds_split_i, labels=['attack', 'support'])\n",
    "        count += length\n",
    "        scores_df.loc[i] = np.array((pre,rec,f1)).reshape((1,-1))[0]\n",
    "        if i == 9:\n",
    "            break\n",
    "    pol_as_test_df['preds'] = preds.values\n",
    "\n",
    "    # Calculate the macro-averages\n",
    "    scores_df['Average Precision'] = (scores_df['Precision attack']+scores_df['Precision support'])/2\n",
    "    scores_df['Average Recall'] = (scores_df['Recall attack']+scores_df['Recall support'])/2\n",
    "    scores_df['Average F1'] = (scores_df['F1 attack']+scores_df['F1 support'])/2\n",
    "\n",
    "    print(\"Precision, Recall, F1, Macro-averages (Mean + std) for Task 2 on all folds:\")\n",
    "    display(scores_df.agg([np.mean, np.std, np.min, np.max]))\n",
    "    \n",
    "    print('Only calculate the scores on the complete predictions:')\n",
    "    print(classification_report(pol_as_test_df['label'], pol_as_test_df['preds'], labels=['attack', 'support']))\n",
    "\n",
    "else:\n",
    "    print('You have to first reproduce the results for the Polical dataset Task 2.\\n'\n",
    "          'python run_classifier_ba.py  --task_name \"political-as\" --output_dir res/pol_as/crossval1 --do_cross_val --do_lower_case --num_train_epochs 5 --max_seq_length 256 --train_batch_size 12 --learning_rate 2e-5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The comparative results on the Political dataset (Task 3: Attack/Support/Unrelated). \n",
    "# 10-Fold stratified cross-validation is used \n",
    "\n",
    "# Parameters:\n",
    "# Fixed: input=both, seq_len=256, warmup_prop=0.1, seed=42\n",
    "# model=base-uncased, epochs=5, batch_size=12, lr=2e-5\n",
    "\n",
    "# Only run this cell, if the Political Task 3 comparative training has been done\n",
    "if os.path.isdir(\"../pytorch/res/pol_asu\"):\n",
    "    # Read the results\n",
    "    eval_results = pd.read_csv('../pytorch/res/pol_asu/eval_results.tsv', sep='\\t')\n",
    "    # Read the predictions\n",
    "    eval_preds = pd.read_csv('../pytorch/res/pol_asu/eval_preds.csv')\n",
    "    \n",
    "    # Calculate precision, recall and F1 for all Folds\n",
    "    preds = pd.Series()\n",
    "    re_pre_f1_dict = {'Precision attack' : [], 'Precision support': [], 'Precision unrelated': [],\n",
    "                  'Recall attack': [], 'Recall support': [], 'Recall unrelated': [],\n",
    "                  'F1 attack': [], 'F1 support': [], 'F1 unrelated': []}\n",
    "    scores_df = pd.DataFrame(re_pre_f1_dict)\n",
    "    count = 0\n",
    "    for i, row in eval_preds.iterrows():\n",
    "        preds_split_i = pd.Series(row.values[~row.str.contains('bert*', na=False, regex=True)]).dropna().astype('int8')    \n",
    "        preds_split_i = preds_split_i.replace({0: 'attack', 1: 'support', 2: 'unrelated'})\n",
    "        preds = preds.append(preds_split_i)\n",
    "        length = len(preds_split_i)\n",
    "        pre, rec, f1, support = precision_recall_fscore_support(pol_asu_test_df.iloc[count:count+length]['label'], preds_split_i, labels=['attack', 'support', 'unrelated'])\n",
    "        count += length\n",
    "        scores_df.loc[i] = np.array((pre,rec,f1)).reshape((1,-1))[0]\n",
    "        if i == 9:\n",
    "            break\n",
    "    pol_asu_test_df['preds'] = preds.values\n",
    "\n",
    "    # Calculate the macro-averages\n",
    "    scores_df['Average Precision'] = (scores_df['Precision attack']+scores_df['Precision support']+scores_df['Precision unrelated'])/3\n",
    "    scores_df['Average Recall'] = (scores_df['Recall attack']+scores_df['Recall support']+scores_df['Recall unrelated'])/3\n",
    "    scores_df['Average F1'] = (scores_df['F1 attack']+scores_df['F1 support']+scores_df['F1 unrelated'])/3\n",
    "\n",
    "    print(\"Precision, Recall, F1, Macro-averages (Mean + std) for Task 3 on all folds:\")\n",
    "    display(scores_df.agg([np.mean, np.std, np.min, np.max]))\n",
    "    \n",
    "    print('Only calculate the scores on the complete predictions:')\n",
    "    print(classification_report(pol_asu_test_df['label'], pol_asu_test_df['preds'], labels=['attack', 'support', 'unrelated']))\n",
    "    \n",
    "    print('Major class baseline:')\n",
    "    print(classification_report(pol_asu_test_df['label'], ['unrelated']*1460, labels=['attack', 'support', 'unrelated']))\n",
    "\n",
    "\n",
    "else:\n",
    "    print('You have to first reproduce the results for the Polical dataset Task 3.\\n'\n",
    "          'python run_classifier_ba.py  --task_name \"political-asu\" --output_dir res/pol_asu/crossval1 --do_cross_val --do_lower_case --num_train_epochs 5 --max_seq_length 256 --train_batch_size 12 --learning_rate 2e-5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The comparative results on the Agreement dataset. \n",
    "# 10-Fold stratified cross-validation is used \n",
    "\n",
    "# Parameters:\n",
    "# Fixed: input=both, seq_len=256, warmup_prop=0.1, seed=42\n",
    "# model=base-uncased, epochs=3, batch_size=12, lr=2e-5\n",
    "\n",
    "# Only run this cell, if the Agreement comparative training has been done\n",
    "if os.path.isdir(\"../pytorch/res/agreement_new\"):\n",
    "    # Read the results\n",
    "    eval_results = pd.read_csv('../pytorch/res/agreement_new/eval_results.tsv', sep='\\t')\n",
    "    # Read the predictions\n",
    "    eval_preds = pd.read_csv('../pytorch/res/agreement_new/eval_preds.csv')\n",
    "    \n",
    "    # Calculate precision, recall and F1 for all Folds\n",
    "    preds = pd.Series()\n",
    "    re_pre_f1_dict = {'Precision agreement' : [], 'Precision disagreement': [], \n",
    "                  'Recall agreement': [], 'Recall disagreement': [], 'F1 agreement': [], 'F1 disagreement': [], \"Accuracy\": []}\n",
    "    scores_df = pd.DataFrame(re_pre_f1_dict)\n",
    "    count = 0\n",
    "    for i, row in eval_preds.iterrows():\n",
    "        preds_split_i = pd.Series(row.values[~row.str.contains('bert*', na=False, regex=True)]).dropna().astype('int8')    \n",
    "        preds_split_i = preds_split_i.replace({0: 'agreement', 1: 'disagreement'})\n",
    "        preds = preds.append(preds_split_i)\n",
    "        length = len(preds_split_i)\n",
    "        pre, rec, f1, support = precision_recall_fscore_support(ag_test_df.iloc[count:count+length]['label'], preds_split_i, labels=['agreement', 'disagreement'])\n",
    "        acc = np.array([accuracy_score(ag_test_df.iloc[count:count+length]['label'], preds_split_i)])\n",
    "        count += length\n",
    "        scores_df.loc[i] = np.concatenate((pre,rec,f1,acc)).ravel()\n",
    "        if i == 9:\n",
    "            break\n",
    "    ag_test_df['preds'] = preds.values\n",
    "\n",
    "    print(\"Precision, Recall, F1, Accuracy (Mean + std) for Agreement on all folds:\")\n",
    "    display(scores_df.agg([np.mean, np.std, np.min, np.max]))\n",
    "\n",
    "\n",
    "else:\n",
    "    print('You have to first reproduce the results for the Agreement dataset.\\n'\n",
    "          'python run_classifier_ba.py  --task_name \"agreement\" --output_dir res/agreement_new/crossval1 --do_cross_val --do_lower_case --num_train_epochs 3 --max_seq_length 256 --train_batch_size 12 --learning_rate 2e-5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Additional Experiments <a name=\"addexps\"></a>\n",
    "\n",
    "In the following the results of the additional experiments are presented."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The additional procon results on the NoDE dataset. The network is always trained on the train dataset + procon \n",
    "# and evaluated on the test dataset. \n",
    "\n",
    "# Parameters:\n",
    "# Fixed: input=both, seq_len=128, warmup_prop=0.1, seed=42-71\n",
    "# Tested: model=base-uncased, epochs=4,5, batch_size=12,16, lr=2e-5, 3e-5\n",
    "\n",
    "# Only run this cell, if the NoDE additional procon training has been done\n",
    "if os.path.isdir(\"../pytorch/res/node_both_procon\"):\n",
    "    # Read in all testing results\n",
    "    eval_results = pd.read_csv('../pytorch/res/node_both_procon/eval_results.tsv', sep='\\t')\n",
    "    # Read in all testing predictions\n",
    "    eval_preds = pd.read_csv('../pytorch/res/node_both_procon/eval_preds.csv')\n",
    "    # Group all runs with the same settings (different seeds) together\n",
    "    eval_results_grouped = eval_results.groupby(['_bert-model', '_num_epochs', '_batch_size','_gradient_acc' ,'_learning_rate' ])\n",
    "    # Aggregate mean, min, max, std and median\n",
    "    results = eval_results_grouped['acc'].agg([np.mean, np.min, np.max, np.std, np.median])\n",
    "    \n",
    "    # Print the statistics reported in the thesis\n",
    "    print(\"Best mean result with parameters and other statistics:\\n\", results.loc[results['mean'].idxmax()]) \n",
    "    print(\"\\nOverall mean:\\n\", eval_results['acc'].mean())\n",
    "    print(\"\\nWorst (single) run:\\n\", eval_results['acc'].min())\n",
    "    print(\"\\nBest (single) run:\\n\", eval_results['acc'].max())\n",
    "    \n",
    "    # Formats and shows the complete result table\n",
    "    results = results.reset_index()\n",
    "    results = results.rename(columns={element: element.replace(\"_\", \"-\") for element in results.columns.tolist()})\n",
    "    results = results.rename(columns={element: re.sub(\"^-\", \"\", element) for element in results.columns.tolist()})\n",
    "    results = results.replace({\"bert-base-uncased\": 'bbu', \"bert-large-uncased\": 'blu'})\n",
    "    results['batch-size'] = results['batch-size'] * results['gradient-acc']\n",
    "    results = results.drop(columns=[\"gradient-acc\"])\n",
    "    results.iloc[:,:9].to_csv('../data/thesis/node_procon_results.csv', index=False)\n",
    "    print(\"\\nComplete results table:\")\n",
    "    display(results.iloc[:,:8])\n",
    "else:\n",
    "    print(\"You have to first reproduce the results for the NoDE procon dataset.\\n\"\n",
    "          \"../code_relation_prediction/pytorch ./run_all_node.sh procon\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The additional results on the Political dataset (Task 1+2). \n",
    "# 5-Fold leave-one-group-out (topics) cross-validation is used \n",
    "\n",
    "# Parameters:\n",
    "# Fixed: input=both, seq_len=256, warmup_prop=0.1, seed=42\n",
    "# model=base-uncased, epochs=5, batch_size=12, lr=2e-5\n",
    "\n",
    "# Only run this cell, if the Political Task 1 additional training has been done\n",
    "if os.path.isdir(\"../pytorch/res/pol_ru_topics\"):\n",
    "    # Load results\n",
    "    eval_results = pd.read_csv('../pytorch/res/pol_ru_topics/eval_results.tsv', sep='\\t')\n",
    "    # Aggregate and display\n",
    "    print(\"Task 1:\")\n",
    "    results = eval_results.groupby(['_bert-model', '_num_epochs', '_batch_size','_gradient_acc' ,'_learning_rate'])['f1'].agg([np.mean, np.min, np.max, np.std])\n",
    "    display(results)\n",
    "else:\n",
    "    print('You have to first reproduce the results for the Political Task 1 additional dataset.\\n'\n",
    "          '../code_relation_prediction/pytorch python run_classifier_ba.py  --task_name \"political-ru-topics\" --output_dir res/pol_ru_topics/crossval1 --do_cross_val --do_lower_case --num_train_epochs 5 --max_seq_length 256 --train_batch_size 12 --learning_rate 2e-5')\n",
    "    \n",
    "# Only run this cell, if the Political Task 2 additional training has been done\n",
    "if os.path.isdir(\"../pytorch/res/pol_as_topics\"):\n",
    "    # Load results\n",
    "    eval_results = pd.read_csv('../pytorch/res/pol_as_topics/eval_results.tsv', sep='\\t')\n",
    "    # Aggregate and display\n",
    "    print(\"Task 2:\")\n",
    "    results = eval_results.groupby(['_bert-model', '_num_epochs', '_batch_size','_gradient_acc' ,'_learning_rate'])['f1'].agg([np.mean, np.min, np.max, np.std])\n",
    "    display(results)\n",
    "else:\n",
    "    print('You have to first reproduce the results for the Political Task 2 additional dataset.\\n'\n",
    "          '../code_relation_prediction/pytorch python run_classifier_ba.py  --task_name \"political-as-topics\" --output_dir res/pol_as_topics/crossval1 --do_cross_val --do_lower_case --num_train_epochs 5 --max_seq_length 256 --train_batch_size 12 --learning_rate 2e-5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The additional results on the Agreement dataset. \n",
    "# Topic independent 80/20 train/test split\n",
    "\n",
    "# Parameters:\n",
    "# Fixed: input=both, seq_len=256, warmup_prop=0.1, seed=42\n",
    "# model=base-uncased, epochs=3, batch_size=12, lr=2e-5\n",
    "\n",
    "# Only run this cell, if the Political Task 2 additional training has been done\n",
    "if os.path.isdir(\"../pytorch/res/agreement_topics_new\"):\n",
    "    # Load results\n",
    "    eval_results = pd.read_csv('../pytorch/res/agreement_topics_new/eval_results.tsv', sep='\\t')\n",
    "    # Aggregate and display\n",
    "    results = eval_results.groupby(['_bert-model', '_num_epochs', '_batch_size','_gradient_acc' ,'_learning_rate'])['f1'].agg([np.mean, np.min, np.max, np.std])\n",
    "    display(results)\n",
    "else:\n",
    "    print('You have to first reproduce the results for the Agreement additional dataset.\\n'\n",
    "          '../code_relation_prediction/pytorch python run_classifier_ba.py  --task_name \"agreement-topics\" --output_dir res/agreement_topics_new/train_test1 --do_train --do_eval --do_lower_case --num_train_epochs 3 --max_seq_length 256 --train_batch_size 12 --learning_rate 2e-5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The additional only org results on the NoDE dataset. The network is always trained on the train dataset\n",
    "# and evaluated on the test dataset. \n",
    "\n",
    "# Parameters:\n",
    "# Fixed: input=org, seq_len=128, warmup_prop=0.1, seed=42-71\n",
    "# Tested: model=base-uncased, epochs=4,5, batch_size=12,16, lr=2e-5, 3e-5\n",
    "\n",
    "# Only run this cell, if the NoDE additional org training has been done\n",
    "if os.path.isdir(\"../pytorch/res/node_org\"):\n",
    "    # Read in all testing results\n",
    "    eval_results = pd.read_csv('../pytorch/res/node_org/eval_results.tsv', sep='\\t')\n",
    "    # Read in all testing predictions\n",
    "    eval_preds = pd.read_csv('../pytorch/res/node_org/eval_preds.csv')\n",
    "    # Group all runs with the same settings (different seeds) together\n",
    "    eval_results_grouped = eval_results.groupby(['_bert-model', '_num_epochs', '_batch_size','_gradient_acc' ,'_learning_rate' ])\n",
    "    # Aggregate mean, min, max, std and median\n",
    "    results = eval_results_grouped['acc'].agg([np.mean, np.min, np.max, np.std, np.median])\n",
    "    \n",
    "    # Print the statistics reported in the thesis\n",
    "    print(\"Best mean result with parameters and other statistics:\\n\", results.loc[results['mean'].idxmax()]) \n",
    "    print(\"\\nOverall mean:\\n\", eval_results['acc'].mean())\n",
    "    print(\"\\nWorst (single) run:\\n\", eval_results['acc'].min())\n",
    "    print(\"\\nBest (single) run:\\n\", eval_results['acc'].max())\n",
    "    \n",
    "    # Formats and shows the complete result table\n",
    "    results = results.reset_index()\n",
    "    results = results.rename(columns={element: element.replace(\"_\", \"-\") for element in results.columns.tolist()})\n",
    "    results = results.rename(columns={element: re.sub(\"^-\", \"\", element) for element in results.columns.tolist()})\n",
    "    results = results.replace({\"bert-base-uncased\": 'bbu', \"bert-large-uncased\": 'blu'})\n",
    "    results['batch-size'] = results['batch-size'] * results['gradient-acc']\n",
    "    results = results.drop(columns=[\"gradient-acc\"])\n",
    "    results.iloc[:,:9].to_csv('../data/thesis/node_onlyorg_results.csv', index=False)\n",
    "    print(\"\\nComplete results table:\")\n",
    "    display(results.iloc[:,:8])\n",
    "else:\n",
    "    print(\"You have to first reproduce the results for the NoDE org dataset.\\n\"\n",
    "          \"../code_relation_prediction/pytorch ./run_all_node.sh only\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The additional results on the Political dataset (Task 1+2). \n",
    "# 10-Fold stratified cross-validation is used \n",
    "\n",
    "# Parameters:\n",
    "# Fixed: input=org, seq_len=256, warmup_prop=0.1, seed=42\n",
    "# model=base-uncased, epochs=5, batch_size=12, lr=2e-5\n",
    "\n",
    "# Only run this cell, if the Political Task 1 additional org training has been done\n",
    "if os.path.isdir(\"../pytorch/res/pol_ru_org\"):\n",
    "    # Load results\n",
    "    eval_results = pd.read_csv('../pytorch/res/pol_ru_org/eval_results.tsv', sep='\\t')\n",
    "    # Aggregate and display\n",
    "    print(\"Task 1:\")\n",
    "    results = eval_results.groupby(['_bert-model', '_num_epochs', '_batch_size','_gradient_acc' ,'_learning_rate'])['f1'].agg([np.mean, np.min, np.max, np.std])\n",
    "    display(results)\n",
    "else:\n",
    "    print('You have to first reproduce the results for the Political Task 1 additional org dataset.\\n'\n",
    "          '../code_relation_prediction/pytorch python run_classifier_ba.py  --task_name \"political-as\" --output_dir res/pol_as_org/crossval1 --input_to_use \"org\" --do_cross_val --do_lower_case --num_train_epochs 5 --max_seq_length 256 --train_batch_size 12 --learning_rate 2e-5')\n",
    "# Only run this cell, if the Political Task 2 additional org training has been done\n",
    "if os.path.isdir(\"../pytorch/res/pol_as_org\"):\n",
    "    # Load results\n",
    "    eval_results = pd.read_csv('../pytorch/res/pol_as_org/eval_results.tsv', sep='\\t')\n",
    "    # Aggregate and display\n",
    "    print(\"Task 2:\")\n",
    "    results = eval_results.groupby(['_bert-model', '_num_epochs', '_batch_size','_gradient_acc' ,'_learning_rate'])['f1'].agg([np.mean, np.min, np.max, np.std])\n",
    "    display(results)\n",
    "else:\n",
    "    print('You have to first reproduce the results for the Political Task 2 additional org dataset.\\n'\n",
    "          '../code_relation_prediction/pytorch python run_classifier_ba.py  --task_name \"political-ru\" --output_dir res/pol_ru_org/crossval1 --input_to_use \"org\" --do_cross_val --do_lower_case --num_train_epochs 5 --max_seq_length 256 --train_batch_size 12 --learning_rate 2e-5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The additional only resp results on the NoDE dataset. The network is always trained on the train dataset\n",
    "# and evaluated on the test dataset. \n",
    "\n",
    "# Parameters:\n",
    "# Fixed: input=response, seq_len=128, warmup_prop=0.1, seed=42-71\n",
    "# Tested: model=base-uncased, epochs=4,5, batch_size=12,16, lr=2e-5, 3e-5\n",
    "\n",
    "# Only run this cell, if the NoDE additional org training has been done\n",
    "if os.path.isdir(\"../pytorch/res/node_resp\"):\n",
    "    # Read in all testing results\n",
    "    eval_results = pd.read_csv('../pytorch/res/node_resp/eval_results.tsv', sep='\\t')\n",
    "    # Read in all testing predictions\n",
    "    eval_preds = pd.read_csv('../pytorch/res/node_resp/eval_preds.csv')\n",
    "    # Group all runs with the same settings (different seeds) together\n",
    "    eval_results_grouped = eval_results.groupby(['_bert-model', '_num_epochs', '_batch_size','_gradient_acc' ,'_learning_rate' ])\n",
    "    # Aggregate mean, min, max, std and median\n",
    "    results = eval_results_grouped['acc'].agg([np.mean, np.min, np.max, np.std, np.median])\n",
    "    \n",
    "    # Print the statistics reported in the thesis\n",
    "    print(\"Best mean result with parameters and other statistics:\\n\", results.loc[results['mean'].idxmax()]) \n",
    "    print(\"\\nOverall mean:\\n\", eval_results['acc'].mean())\n",
    "    print(\"\\nWorst (single) run:\\n\", eval_results['acc'].min())\n",
    "    print(\"\\nBest (single) run:\\n\", eval_results['acc'].max())\n",
    "    \n",
    "    # Formats and shows the complete result table\n",
    "    results = results.reset_index()\n",
    "    results = results.rename(columns={element: element.replace(\"_\", \"-\") for element in results.columns.tolist()})\n",
    "    results = results.rename(columns={element: re.sub(\"^-\", \"\", element) for element in results.columns.tolist()})\n",
    "    results = results.replace({\"bert-base-uncased\": 'bbu', \"bert-large-uncased\": 'blu'})\n",
    "    results['batch-size'] = results['batch-size'] * results['gradient-acc']\n",
    "    results = results.drop(columns=[\"gradient-acc\"])\n",
    "    results.iloc[:,:9].to_csv('../data/thesis/node_onlyorg_results.csv', index=False)\n",
    "    print(\"\\nComplete results table:\")\n",
    "    display(results.iloc[:,:8])\n",
    "else:\n",
    "    print(\"You have to first reproduce the results for the NoDE resp dataset.\\n\"\n",
    "          \"../code_relation_prediction/pytorch ./run_all_node.sh only\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The additional results on the Political dataset (Task 1+2). \n",
    "# 10-Fold stratified cross-validation is used \n",
    "\n",
    "# Parameters:\n",
    "# Fixed: input=response, seq_len=256, warmup_prop=0.1, seed=42\n",
    "# model=base-uncased, epochs=5, batch_size=12, lr=2e-5\n",
    "\n",
    "# Only run this cell, if the Political Task 1 additional resp training has been done\n",
    "if os.path.isdir(\"../pytorch/res/pol_ru_resp\"):\n",
    "    # Load results\n",
    "    eval_results = pd.read_csv('../pytorch/res/pol_ru_resp/eval_results.tsv', sep='\\t')\n",
    "    # Aggregate and display\n",
    "    print(\"Task 1:\")\n",
    "    results = eval_results.groupby(['_bert-model', '_num_epochs', '_batch_size','_gradient_acc' ,'_learning_rate'])['f1'].agg([np.mean, np.min, np.max, np.std])\n",
    "    display(results)\n",
    "else:\n",
    "    print('You have to first reproduce the results for the Political Task 1 additional resp dataset.\\n'\n",
    "          '../code_relation_prediction/pytorch python run_classifier_ba.py  --task_name \"political-as\" --output_dir res/pol_as_resp/crossval1 --input_to_use \"response\" --do_cross_val --do_lower_case --num_train_epochs 5 --max_seq_length 256 --train_batch_size 12 --learning_rate 2e-5')\n",
    "# Only run this cell, if the Political Task 2 additional resp training has been done\n",
    "if os.path.isdir(\"../pytorch/res/pol_as_resp\"):\n",
    "    # Load results\n",
    "    eval_results = pd.read_csv('../pytorch/res/pol_as_resp/eval_results.tsv', sep='\\t')\n",
    "    # Aggregate and display\n",
    "    print(\"Task 2:\")\n",
    "    results = eval_results.groupby(['_bert-model', '_num_epochs', '_batch_size','_gradient_acc' ,'_learning_rate'])['f1'].agg([np.mean, np.min, np.max, np.std])\n",
    "    display(results)\n",
    "else:\n",
    "    print('You have to first reproduce the results for the Political Task 2 additional resp dataset.\\n'\n",
    "          '../code_relation_prediction/pytorch python run_classifier_ba.py  --task_name \"political-ru\" --output_dir res/pol_ru_resp/crossval1 --input_to_use \"response\" --do_cross_val --do_lower_case --num_train_epochs 5 --max_seq_length 256 --train_batch_size 12 --learning_rate 2e-5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The additional reversed results on the NoDE dataset. The network is always trained on the train dataset\n",
    "# and evaluated on the test dataset. \n",
    "\n",
    "# Parameters:\n",
    "# Fixed: input=resp-org, seq_len=128, warmup_prop=0.1, seed=42-71\n",
    "# Tested: model=base-uncased, epochs=4,5, batch_size=12,16, lr=2e-5, 3e-5\n",
    "\n",
    "# Only run this cell, if the NoDE additional org training has been done\n",
    "if os.path.isdir(\"../pytorch/res/node_both_reversed\"):\n",
    "    # Read in all testing results\n",
    "    eval_results = pd.read_csv('../pytorch/res/node_both_reversed/eval_results.tsv', sep='\\t')\n",
    "    # Read in all testing predictions\n",
    "    eval_preds = pd.read_csv('../pytorch/res/node_both_reversed/eval_preds.csv')\n",
    "    # Group all runs with the same settings (different seeds) together\n",
    "    eval_results_grouped = eval_results.groupby(['_bert-model', '_num_epochs', '_batch_size','_gradient_acc' ,'_learning_rate' ])\n",
    "    # Aggregate mean, min, max, std and median\n",
    "    results = eval_results_grouped['acc'].agg([np.mean, np.min, np.max, np.std, np.median])\n",
    "    \n",
    "    # Print the statistics reported in the thesis\n",
    "    print(\"Best mean result with parameters and other statistics:\\n\", results.loc[results['mean'].idxmax()]) \n",
    "    print(\"\\nOverall mean:\\n\", eval_results['acc'].mean())\n",
    "    print(\"\\nWorst (single) run:\\n\", eval_results['acc'].min())\n",
    "    print(\"\\nBest (single) run:\\n\", eval_results['acc'].max())\n",
    "    \n",
    "    # Formats and shows the complete result table\n",
    "    results = results.reset_index()\n",
    "    results = results.rename(columns={element: element.replace(\"_\", \"-\") for element in results.columns.tolist()})\n",
    "    results = results.rename(columns={element: re.sub(\"^-\", \"\", element) for element in results.columns.tolist()})\n",
    "    results = results.replace({\"bert-base-uncased\": 'bbu', \"bert-large-uncased\": 'blu'})\n",
    "    results['batch-size'] = results['batch-size'] * results['gradient-acc']\n",
    "    results = results.drop(columns=[\"gradient-acc\"])\n",
    "    results.iloc[:,:9].to_csv('../data/thesis/node_resporg_results.csv', index=False)\n",
    "    print(\"\\nComplete results table:\")\n",
    "    display(results.iloc[:,:8])\n",
    "else:\n",
    "    print(\"You have to first reproduce the results for the NoDE resp dataset.\\n\"\n",
    "          \"../code_relation_prediction/pytorch ./run_all_node.sh resporg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The additional reversed results on the Political dataset (Task 1+2). \n",
    "# 10-Fold stratified cross-validation is used \n",
    "\n",
    "# Parameters:\n",
    "# Fixed: input=resp-org, seq_len=256, warmup_prop=0.1, seed=42\n",
    "# model=base-uncased, epochs=5, batch_size=12, lr=2e-5\n",
    "\n",
    "# Only run this cell, if the Political Task 1 additional resp-org training has been done\n",
    "if os.path.isdir(\"../pytorch/res/pol_ru_resporg\"):\n",
    "    # Load results\n",
    "    eval_results = pd.read_csv('../pytorch/res/pol_ru_resporg/eval_results.tsv', sep='\\t')\n",
    "    # Aggregate and display\n",
    "    print(\"Task 1:\")\n",
    "    results = eval_results.groupby(['_bert-model', '_num_epochs', '_batch_size','_gradient_acc' ,'_learning_rate'])['f1'].agg([np.mean, np.min, np.max, np.std])\n",
    "    display(results)\n",
    "else:\n",
    "    print('You have to first reproduce the results for the Political Task 1 additional resp-org dataset.\\n'\n",
    "          '../code_relation_prediction/pytorch python run_classifier_ba.py  --task_name \"political-as\" --output_dir res/pol_as_resporg/crossval1 --input_to_use \"response-org\" --do_cross_val --do_lower_case --num_train_epochs 5 --max_seq_length 256 --train_batch_size 12 --learning_rate 2e-5')\n",
    "# Only run this cell, if the Political Task 2 additional resp-org training has been done\n",
    "if os.path.isdir(\"../pytorch/res/pol_as_resporg\"):\n",
    "    # Load results\n",
    "    eval_results = pd.read_csv('../pytorch/res/pol_as_resporg/eval_results.tsv', sep='\\t')\n",
    "    # Aggregate and display\n",
    "    print(\"Task 2:\")\n",
    "    results = eval_results.groupby(['_bert-model', '_num_epochs', '_batch_size','_gradient_acc' ,'_learning_rate'])['f1'].agg([np.mean, np.min, np.max, np.std])\n",
    "    display(results)\n",
    "else:\n",
    "    print('You have to first reproduce the results for the Political Task 2 additional resp-org dataset.\\n'\n",
    "          '../code_relation_prediction/pytorch python run_classifier_ba.py  --task_name \"political-ru\" --output_dir res/pol_ru_resporg/crossval1 --input_to_use \"response-org\" --do_cross_val --do_lower_case --num_train_epochs 5 --max_seq_length 256 --train_batch_size 12 --learning_rate 2e-5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Critical Analysis  <a name=\"critana\"></a>\n",
    "\n",
    "Critical Analysis\n",
    "- Duplicates in Agreement\n",
    "- Author distributon + unique argument author\n",
    "- WordClouds Author\n",
    "- NoDe:\n",
    "    - Major\n",
    "    - Major by topic\n",
    "    - Sent 1+2\n",
    "    - Output in respect to the topics\n",
    "- Pol:\n",
    "    - RU:\n",
    "        - Major\n",
    "        - Major by topic\n",
    "        - Major by org\n",
    "        - Major by response\n",
    "        - Major by author\n",
    "        - Performance over topics (10-CV, 5-CV)\n",
    "    - AS:\n",
    "        - Major\n",
    "        - Major by topic\n",
    "        - Major by org\n",
    "        - Major by response\n",
    "        - Major by author\n",
    "        - Sent 1+2\n",
    "        - Performance over topics (10-CV, 5-CV)\n",
    "- Agreement:\n",
    "    - Major\n",
    "    - Major by topic\n",
    "    - Performance over topics (10-CV, train/test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The first few duplicates in the agreement Dataset (~4000 arg1,arg2 pairs occur twice, sometimes in different topics)\n",
    "print(\"Agreement Duplicates:\")\n",
    "df_check = df[df['org_dataset'] == 'agreement']\n",
    "display(df_check[df_check.duplicated(subset=['org', 'response'], keep=False)].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Political by author\n",
    "# Same author mostly support each other\n",
    "# Different authors mostly attack each other\n",
    "# Dataset is heavily imbalanced in respect to the author, Kennedy occurs way more often\n",
    "print(\"Number of unique arguments per author:\")\n",
    "display(data_nix_ken.groupby(\"author\").nunique())\n",
    "print(\"Label distribution for the different author combinations:\")\n",
    "data_stats_author.iloc[:,:-2].to_csv('../data/thesis/author_imbalance.csv', index=False)\n",
    "display(data_stats_author.iloc[:,:-2].style.background_gradient(cmap='Blues'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wordclouds for Kennedy and for Nixon\n",
    "# Both often say the name of the other candidate, Nixon talks about Predisdent Eisenhower\n",
    "fig, (ax1,ax2) = plt.subplots(1,2, figsize=(12,10))  # 1 row, 2 columns\n",
    "\n",
    "stopwords = set(STOPWORDS)  \n",
    "wordcloud = WordCloud(\n",
    "    stopwords=stopwords, random_state=5).generate(\n",
    "    \" \".join(text for text in data_nix_ken.loc[data_nix_ken[\"author\"] == 'Nixon', 'text']))\n",
    "ax1.imshow(wordcloud, interpolation=\"bilinear\")\n",
    "ax1.set_title(\"Nixon WordCloud\")\n",
    "ax1.set_axis_off()\n",
    "wordcloud = WordCloud(\n",
    "    stopwords=stopwords, random_state=5).generate(\n",
    "    \" \".join(text for text in data_nix_ken.loc[data_nix_ken[\"author\"] == 'Kennedy', 'text']))\n",
    "ax2.imshow(wordcloud, interpolation=\"bilinear\")\n",
    "ax2.set_title(\"Kennedy WordCloud\")\n",
    "ax2.set_axis_off()\n",
    "\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../data/thesis/authors_wordcloud.pdf', bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All Baselines for the NoDE (test) dataset + Predictions on NoDE with respect to the topcis\n",
    "\n",
    "# Major class baseline \n",
    "data = data_stats_topic['debate_test']\n",
    "data['major_acc'] = data.apply(get_major_acc, args=[['Attack','Support']], axis=1)\n",
    "data['major_class'] = data.apply(get_major_class, args=[['Attack','Support']], axis=1)\n",
    "data = data.set_index('Topic')\n",
    "print(\"Major class baseline NoDe test:\")\n",
    "display(data.iloc[-1:])\n",
    "\n",
    "# Major class topic baseline\n",
    "print(\"Major class by topic NoDe test:\")\n",
    "display(data)\n",
    "print(\"Major by topic baseline overall NoDe test:\\n\")\n",
    "node_test_df['major_topic_pred'] = node_test_df['topic'].apply(\n",
    "    lambda r: data.loc[r, 'major_class']).replace({'Attack': 'attack', 'Support': 'support'})\n",
    "print(classification_report(node_test_df['label'], node_test_df['major_topic_pred']))\n",
    "\n",
    "# Sentiment baseliness\n",
    "node_test_df['org_polarity'] = node_test_df['org'].apply(lambda r: disc_pol(sid.polarity_scores(r)['compound']))\n",
    "node_test_df['resp_polarity'] = node_test_df['response'].apply(lambda r: disc_pol(sid.polarity_scores(r)['compound']))\n",
    "node_test_df['sent_both_baseline'] = node_test_df.apply(\n",
    "    lambda r: 'attack' if r['org_polarity'] != r['resp_polarity'] else 'support', axis=1)\n",
    "node_test_df['sent_resp_baseline'] = node_test_df.apply(\n",
    "    lambda r: 'attack' if r['resp_polarity'] == 'negative' else 'support', axis=1)\n",
    "print(\"\\nSentimen 1 baseline (both arguments same sent==support, otherwise==attack):\\n\")\n",
    "print(classification_report(node_test_df['label'], node_test_df['sent_both_baseline']))\n",
    "print(\"\\nSentiment 2 baseline (response negative sentiment==attack, otherwise==support):\\n\")\n",
    "print(classification_report(node_test_df['label'], node_test_df['sent_resp_baseline']))\n",
    "\n",
    "# Predictions with respect to the topics (only if NoDE comparative was trained)\n",
    "if os.path.isdir(\"../pytorch/res/node_both_paper\"):\n",
    "    # Load the data\n",
    "    eval_results = pd.read_csv('../pytorch/res/node_both_paper/eval_results.tsv', sep='\\t')\n",
    "    eval_results_grouped = eval_results.groupby(['_bert-model', '_num_epochs', '_batch_size','_gradient_acc' ,'_learning_rate' ])\n",
    "    eval_preds = pd.read_csv('../pytorch/res/node_both_paper/eval_preds.csv')\n",
    "    results = eval_results_grouped['acc'].agg([np.mean, np.min, np.max, np.std, np.median])\n",
    "    bmodel, bepochs, bb, bga, blr = results.loc[results['mean'].idxmax()].name\n",
    "    best_pred_ps = eval_results.loc[(eval_results['_bert-model'] == bmodel) & \n",
    "                           (eval_results['_num_epochs'] == bepochs) & (eval_results['_batch_size'] == bb) &\n",
    "                           (eval_results['_learning_rate'] == blr)].index\n",
    "    \n",
    "    # Take the rounded mean prediction for all runs of the best setting\n",
    "    res = pd.concat([node_test_df.reset_index(drop=True), eval_preds.iloc[best_pred_ps,:-1].transpose().reset_index(drop=True)], axis=1)\n",
    "    res['Mean prediction'] = res[list(best_pred_ps)].mean(axis=1).round().values\n",
    "    res = res.replace({0: 'attack', 1: 'support'})\n",
    "    res = res.rename(columns={'label': 'Label'})\n",
    "\n",
    "    print(\"\\nPredictions of the best setting (rounded) with respect to the topics:\")\n",
    "    pd.crosstab(res['topic'], [res['Label'],res['Mean prediction']]).to_csv('../data/thesis/node_topics_preds.csv', index=True)\n",
    "    display(pd.crosstab(res['topic'], [res['Label'],res['Mean prediction']]))\n",
    "else:\n",
    "    print(\"You have to first reproduce the results for the NoDE dataset.\\n\"\n",
    "          \"../code_relation_prediction/pytorch ./run_all_node.sh comp\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All Baselines for the Political (Task 1) dataset + Predictions with respect to the topcis\n",
    "\n",
    "# Major class baseline \n",
    "data = data_stats_topic['political']\n",
    "data['related'] = data.apply(lambda r: r['Attack'] + r['Support'], axis=1)\n",
    "data['major_acc'] = data.apply(get_major_acc, args=[['related', 'Unrelated']], axis=1)\n",
    "data['major_class'] = data.apply(get_major_class, args=[['related', 'Unrelated']], axis=1)\n",
    "data = data.set_index('Topic')\n",
    "pol_ru_test_df['major_class'] = 'related'\n",
    "print(\"Major class Political RU:\\n\")\n",
    "print(classification_report(pol_ru_test_df['label'], pol_ru_test_df['major_class']))\n",
    "\n",
    "# Major class topic baseline\n",
    "print(\"\\nMajor class by topic Political RU:\\n\")\n",
    "pol_ru_test_df['major_topic_pred'] = pol_ru_test_df['topic'].apply(\n",
    "    lambda r: data.loc[r, 'major_class']).replace({'Unrelated': 'unrelated'})\n",
    "print(classification_report(pol_ru_test_df['label'], pol_ru_test_df['major_topic_pred']))\n",
    "\n",
    "# Author major class baseline\n",
    "data = data_stats_author.copy()\n",
    "data['related'] = data.apply(lambda r: r['Attack'] + r['Support'], axis=1)\n",
    "data['major_acc'] = data.apply(get_major_acc, args=[['related', 'Unrelated']], axis=1)\n",
    "data['major_class'] = data.apply(get_major_class, args=[['related', 'Unrelated']], axis=1)\n",
    "data = data.set_index(['Author resp', 'Author org'])\n",
    "pol_ru_test_df['major_author'] = pol_ru_test_df.apply(\n",
    "    lambda r: data.loc[r['response_stance'], r['org_stance']]['major_class'], axis=1).replace({'Unrelated': 'unrelated'})\n",
    "print(\"\\nAuthor major class baseline RU:\\n\")\n",
    "print(classification_report(pol_ru_test_df['label'], pol_ru_test_df['major_author']))\n",
    "display(data)\n",
    "\n",
    "# Major class per original argument baseline (excluding all arguments only occurring once)\n",
    "df = pol_ru_test_df.copy()\n",
    "data = df.groupby('org').apply(\n",
    "    lambda r: pd.Series({'org': r['org'].iloc[0], 'related': count_values(r, ['related']),\n",
    "                         'unrelated': count_values(r, ['unrelated'])}))\n",
    "data['major_acc'] = data.apply(get_major_acc, args=[['related', 'unrelated']], axis=1)\n",
    "data['major_class'] = data.apply(get_major_class, args=[['related', 'unrelated']], axis=1)\n",
    "data = data.set_index('org')\n",
    "data['total'] = data['related'] + data['unrelated']\n",
    "# Drop all arguments only occurring once\n",
    "orgs_only_once = data.loc[data['total'] == 1].index.to_list()\n",
    "index = df[df['org'].isin(orgs_only_once)].index\n",
    "df = df.drop(index)\n",
    "df['major_org_pred'] = df['org'].apply(lambda r: data.loc[r, 'major_class'])\n",
    "print(\"\\nMajor class by original argument RU:\\n\")\n",
    "print(classification_report(df['label'], df['major_org_pred']))\n",
    "\n",
    "# Major class by response argument baseline (excluding all arguments only occurring once)\n",
    "df = pol_ru_test_df.copy()\n",
    "data = df.groupby('response').apply(\n",
    "    lambda r: pd.Series({'resp': r['response'].iloc[0], 'related': count_values(r, ['related']),\n",
    "                         'unrelated': count_values(r, ['unrelated'])}))\n",
    "data['major_acc'] = data.apply(get_major_acc, args=[['related', 'unrelated']], axis=1)\n",
    "data['major_class'] = data.apply(get_major_class, args=[['related', 'unrelated']], axis=1)\n",
    "data = data.set_index('resp')\n",
    "data['total'] = data['related'] + data['unrelated']\n",
    "# Drop all arguments only occurring once\n",
    "orgs_only_once = data.loc[data['total'] == 1].index.to_list()\n",
    "index = df[df['response'].isin(orgs_only_once)].index\n",
    "df = df.drop(index)\n",
    "df['major_resp_pred'] = df['response'].apply(lambda r: data.loc[r, 'major_class'])\n",
    "print(\"\\nMajor class by response argument RU:\\n\")\n",
    "print(classification_report(df['label'], df['major_resp_pred']))\n",
    "\n",
    "# Predictions with respect to the topics (only if Political Task1 comparative was trained)\n",
    "if os.path.isdir(\"../pytorch/res/pol_ru\"):\n",
    "    # Load the data\n",
    "    eval_preds = pd.read_csv('../pytorch/res/pol_ru/eval_preds.csv')\n",
    "    print(\"\\nPredictions with respect to the topics RU:\\n\")\n",
    "    convert_preds_topic(eval_preds, pol_ru_test_df, {0: 'related', 1: 'unrelated'}, 9)\n",
    "else:\n",
    "    print('You have to first reproduce the results for the Political Task 1 dataset.\\n'\n",
    "          'python run_classifier_ba.py  --task_name \"political-ru\" --output_dir res/pol_ru/crossval1 --do_cross_val --do_lower_case --num_train_epochs 5 --max_seq_length 256 --train_batch_size 12 --learning_rate 2e-5')    \n",
    "\n",
    "# Predictions with respect to the topics (only if Political Task1 additional was trained)\n",
    "if os.path.isdir(\"../pytorch/res/pol_ru_topics\"):\n",
    "    # Load the data\n",
    "    eval_preds = pd.read_csv('../pytorch/res/pol_ru_topics/eval_preds.csv', names=list(range(0,315))).iloc[1:]\n",
    "    print(\"\\nPredictions with respect to the topics RU (indepedent version):\\n\")\n",
    "    convert_preds_topic(eval_preds, pol_ru_to_test_df, {0: 'related', 1: 'unrelated'}, 5)\n",
    "else:\n",
    "    print('You have to first reproduce the results for the Political Task 1 (topic independent) dataset.\\n'\n",
    "          '../code_relation_prediction/pytorch python run_classifier_ba.py  --task_name \"political-ru-topics\" --output_dir res/pol_ru_topics/crossval1 --do_cross_val --do_lower_case --num_train_epochs 5 --max_seq_length 256 --train_batch_size 12 --learning_rate 2e-5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All Baselines for the Political (Task 2) dataset + Predictions with respect to the topcis\n",
    "\n",
    "# Major class baseline \n",
    "data = data_stats_topic['political']\n",
    "data['major_acc'] = data.apply(get_major_acc, args=[['Attack', 'Support']], axis=1)\n",
    "data['major_class'] = data.apply(get_major_class, args=[['Attack', 'Support']], axis=1)\n",
    "data = data.set_index('Topic')\n",
    "pol_as_test_df['major_class'] = 'attack'\n",
    "print(\"Major class Political As:\\n\")\n",
    "print(classification_report(pol_as_test_df['label'], pol_as_test_df['major_class']))\n",
    "\n",
    "# Major class topic baseline\n",
    "print(\"\\nMajor class by topic Political AS:\\n\")\n",
    "pol_as_test_df['major_topic_pred'] = pol_as_test_df['topic'].apply(\n",
    "    lambda r: data.loc[r, 'major_class']).replace({'Attack': 'attack', 'Support': 'support'})\n",
    "print(classification_report(pol_as_test_df['label'], pol_as_test_df['major_topic_pred']))\n",
    "\n",
    "# Author major class baseline\n",
    "data = data_stats_author.copy()\n",
    "data['major_acc'] = data.apply(get_major_acc, args=[['Attack', 'Support']], axis=1)\n",
    "data['major_class'] = data.apply(get_major_class, args=[['Attack', 'Support']], axis=1)\n",
    "data = data.set_index(['Author resp', 'Author org'])\n",
    "pol_as_test_df['major_author'] = pol_as_test_df.apply(\n",
    "    lambda r: data.loc[r['response_stance'], r['org_stance']]['major_class'], axis=1).replace(\n",
    "    {'Attack': 'attack', 'Support': 'support'})\n",
    "print(\"\\nAuthor major class baseline AS:\\n\")\n",
    "print(classification_report(pol_as_test_df['label'], pol_as_test_df['major_author']))\n",
    "display(data)\n",
    "\n",
    "# Major class per original argument baseline (excluding all arguments only occurring once)\n",
    "df = pol_as_test_df.copy()\n",
    "data = df.groupby('org').apply(\n",
    "    lambda r: pd.Series({'org': r['org'].iloc[0], 'attack': count_values(r, ['attack']),\n",
    "                         'support': count_values(r, ['support'])}))\n",
    "data['major_acc'] = data.apply(get_major_acc, args=[['attack', 'support']], axis=1)\n",
    "data['major_class'] = data.apply(get_major_class, args=[['attack', 'support']], axis=1)\n",
    "data = data.set_index('org')\n",
    "data['total'] = data['attack'] + data['support']\n",
    "# Drop all arguments only occurring once\n",
    "orgs_only_once = data.loc[data['total'] == 1].index.to_list()\n",
    "index = df[df['org'].isin(orgs_only_once)].index\n",
    "df = df.drop(index)\n",
    "df['major_org_pred'] = df['org'].apply(lambda r: data.loc[r, 'major_class'])\n",
    "print(\"\\nMajor class by original argument AS:\\n\")\n",
    "print(classification_report(df['label'], df['major_org_pred']))\n",
    "\n",
    "# Majro class by response argument baseline (excluding all arguments only occurring once)\n",
    "df = pol_as_test_df.copy()\n",
    "data = df.groupby('response').apply(\n",
    "    lambda r: pd.Series({'resp': r['response'].iloc[0], 'attack': count_values(r, ['attack']),\n",
    "                         'support': count_values(r, ['support'])}))\n",
    "data['major_acc'] = data.apply(get_major_acc, args=[['attack', 'support']], axis=1)\n",
    "data['major_class'] = data.apply(get_major_class, args=[['attack', 'support']], axis=1)\n",
    "data = data.set_index('resp')\n",
    "data['total'] = data['attack'] + data['support']\n",
    "# Drop all arguments only occurring once\n",
    "orgs_only_once = data.loc[data['total'] == 1].index.to_list()\n",
    "index = df[df['response'].isin(orgs_only_once)].index\n",
    "df = df.drop(index)\n",
    "df['major_resp_pred'] = df['response'].apply(lambda r: data.loc[r, 'major_class'])\n",
    "print(\"\\nMajor class by response argument AS:\\n\")\n",
    "print(classification_report(df['label'], df['major_resp_pred']))\n",
    "\n",
    "# Sentiment baselines\n",
    "df = pol_as_test_df.copy()\n",
    "df['org_polarity'] = df['org'].apply(lambda r: disc_pol(sid.polarity_scores(r)['compound']))\n",
    "df['resp_polarity'] = df['response'].apply(lambda r: disc_pol(sid.polarity_scores(r)['compound']))\n",
    "df['sent_both_baseline'] = df.apply(\n",
    "    lambda r: 'attack' if r['org_polarity'] != r['resp_polarity'] else 'support', axis=1)\n",
    "df['sent_resp_baseline'] = df.apply(\n",
    "    lambda r: 'attack' if r['resp_polarity'] == 'negative' else 'support', axis=1)\n",
    "print(\"\\nSentimen 1 baseline (both arguments same sent==support, otherwise==attack):\\n\")\n",
    "print(classification_report(df['label'], df['sent_both_baseline']))\n",
    "print(\"\\nSentiment 2 baseline (response negative sentiment==attack, otherwise==support):\\n\")\n",
    "print(classification_report(df['label'], df['sent_resp_baseline']))\n",
    "\n",
    "# Predictions with respect to the topics (only if Political Task1 comparative was trained)\n",
    "if os.path.isdir(\"../pytorch/res/pol_as\"):\n",
    "    # Load the data\n",
    "    eval_preds = pd.read_csv('../pytorch/res/pol_as/eval_preds.csv')\n",
    "    print(\"\\nPredictions with respect to the topics AS:\\n\")\n",
    "    convert_preds_topic(eval_preds, pol_as_test_df, {0: 'attack', 1: 'support'}, 9)\n",
    "else:\n",
    "    print('You have to first reproduce the results for the Political Task 2 dataset.\\n'\n",
    "          'python run_classifier_ba.py  --task_name \"political-as\" --output_dir res/pol_as/crossval1 --do_cross_val --do_lower_case --num_train_epochs 5 --max_seq_length 256 --train_batch_size 12 --learning_rate 2e-5')    \n",
    "\n",
    "# Predictions with respect to the topics (only if Political Task1 additional was trained)\n",
    "if os.path.isdir(\"../pytorch/res/pol_as_topics\"):\n",
    "    # Load the data\n",
    "    eval_preds = pd.read_csv('../pytorch/res/pol_as_topics/eval_preds.csv', names=list(range(0,315))).iloc[1:]\n",
    "    print(\"\\nPredictions with respect to the topics AS (indepedent version):\\n\")\n",
    "    convert_preds_topic(eval_preds, pol_as_to_test_df, {0: 'attack', 1: 'support'}, 5)\n",
    "else:\n",
    "    print('You have to first reproduce the results for the Political Task 2 (topic independent) dataset.\\n'\n",
    "          '../code_relation_prediction/pytorch python run_classifier_ba.py  --task_name \"political-as-topics\" --output_dir res/pol_as_topics/crossval1 --do_cross_val --do_lower_case --num_train_epochs 5 --max_seq_length 256 --train_batch_size 12 --learning_rate 2e-5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All Baselines for the Agreement dataset + Predictions with respect to the topcis\n",
    "\n",
    "# Major class baseline \n",
    "data = data_stats_topic['agreement']\n",
    "data['major_acc'] = data.apply(get_major_acc, args=[['Agreement', 'Disagreement']], axis=1)\n",
    "data['major_class'] = data.apply(get_major_class, args=[['Agreement', 'Disagreement']], axis=1)\n",
    "data = data.set_index('Topic')\n",
    "ag_test_df['major_class'] = 'disagreement'\n",
    "print(\"Major class Agreement:\\n\")\n",
    "print(classification_report(ag_test_df['label'], ag_test_df['major_class']))\n",
    "\n",
    "# Major class topic baseline\n",
    "print(\"\\nMajor class by topic Agreement:\\n\")\n",
    "ag_test_df['major_topic_pred'] = ag_test_df['topic'].apply(\n",
    "    lambda r: data.loc[r, 'major_class']).replace({'Agreement': 'agreement', 'Disagreement': 'disagreement'})\n",
    "print(classification_report(ag_test_df['label'], ag_test_df['major_topic_pred']))\n",
    "\n",
    "\n",
    "# Predictions with respect to the topics (only if Agreement comparative was trained)\n",
    "if os.path.isdir(\"../pytorch/res/agreement_new\"):\n",
    "    # Load the data\n",
    "    eval_preds = pd.read_csv('../pytorch/res/agreement_new/eval_preds.csv')\n",
    "    print(\"\\nPredictions with respect to the topics:\\n\")\n",
    "    convert_preds_topic(eval_preds, ag_test_df, {0: 'agreement', 1: 'disagreement'}, 9, False)\n",
    "else:\n",
    "    print('You have to first reproduce the results for the Agreement dataset.\\n'\n",
    "          'python run_classifier_ba.py  --task_name \"agreement\" --output_dir res/agreement_new/crossval1 --do_cross_val --do_lower_case --num_train_epochs 3 --max_seq_length 256 --train_batch_size 12 --learning_rate 2e-5')    \n",
    "\n",
    "# Predictions with respect to the topics (only if Agreement additional was trained)\n",
    "if os.path.isdir(\"../pytorch/res/agreement_topics_new\"):\n",
    "    # Load the data\n",
    "    eval_preds = pd.read_csv('../pytorch/res/agreement_topics_new/eval_preds.csv')\n",
    "    print(\"\\nPredictions with respect to the topics (indepedent version):\\n\")\n",
    "    convert_preds_topic(eval_preds, ag_to_test_df, {0: 'agreement', 1: 'disagreement'}, 1, False)\n",
    "else:\n",
    "    print('You have to first reproduce the results for the Political Task 2 (topic independent) dataset.\\n'\n",
    "          '../code_relation_prediction/pytorch python run_classifier_ba.py  --task_name \"agreement_topics\" --output_dir res/agreement_topics_new/crossval1 --do_cross_val --do_lower_case --num_train_epochs 3 --max_seq_length 256 --train_batch_size 12 --learning_rate 2e-5')"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

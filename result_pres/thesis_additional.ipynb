{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Relation Prediction in Argument Mining With Pre-trained Deep Bidirectional Transformers - Additional\n",
    "\n",
    "This notebook shows or generates some additional results and visualizations not included in the thesis.\n",
    "\n",
    "- Overall stats overview\n",
    "- Attack/Support ratios\n",
    "- Lime visualizations\n",
    "- Scattertext visualizations \n",
    "- Acc distributions for NoDE + Procon\n",
    "- (Visualizations) for predictions by org/response\n",
    "- Agreement sentiment "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Necessary imports and setups\n",
    "# Basic python imports\n",
    "import re\n",
    "import sys \n",
    "import os\n",
    "\n",
    "# For the sentiment baselines\n",
    "import nltk\n",
    "# Download Sentiment Lexicon\n",
    "nltk.download('vader_lexicon')\n",
    "\n",
    "# Datahandling and plotting\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Statistics and other stuff\n",
    "from sklearn.metrics import precision_recall_fscore_support, accuracy_score, classification_report\n",
    "from IPython.display import IFrame, HTML\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "\n",
    "# Get the exact splits of the data\n",
    "sys.path.append(os.path.abspath(\"../pytorch\"))\n",
    "from run_classifier_dataset_utils import processors\n",
    "\n",
    "# Settings\n",
    "# Do no hide rows in pandas\n",
    "pd.set_option('display.max_rows', 999)\n",
    "# Print everything with precision 2\n",
    "pd.set_option('precision', 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read all necessary data\n",
    "df = pd.read_csv('../data/complete_data.tsv', sep='\\t').astype({\"id\": str})\n",
    "data_stats_org = np.load('../data/stats/data_stats_org.npy', allow_pickle=True).item()\n",
    "data_stats_resp = np.load('../data/stats/data_stats_resp.npy', allow_pickle=True).item()\n",
    "data_stats_total = pd.read_csv('../data/stats/data_stats_total.tsv', sep='\\t')\n",
    "\n",
    "# Init the sentiment analyzer\n",
    "sid = SentimentIntensityAnalyzer()\n",
    "\n",
    "# Helper functions\n",
    "def disc_pol(x):\n",
    "    \"\"\"Discretize the float sentiment polarity.\"\"\"\n",
    "    if x >= 0.00:\n",
    "        return 'positive'\n",
    "    else:\n",
    "        return 'negative'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Overall stats overview\n",
    "display(data_stats_total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Attack/Support ratios\n",
    "# Plot how many arguments attack an original argument (attack-ratio)\n",
    "# Most arguments are only attacked or only supported (interesting for detecting arguments likely to be attacked/supported)\n",
    "# If we disregard every argument, which is only answered to once more arguments have an attack-ratio of 0.5\n",
    "# For political Task 2 (Attack/Support is regarded)\n",
    "\n",
    "print(\"Attack ratio for the original arguments in debate train and political as:\")\n",
    "fig, (ax1,ax2) = plt.subplots(2,2, figsize=(10,4))  # 2 rows, 2 columns\n",
    "for data_set, ax in [('debate_train', ax1), ('political',ax2)]:\n",
    "    data_stats_org[data_set][\"Total pairs nu\"] = data_stats_org[data_set]['Attacked'] + data_stats_org[data_set]['Supported']\n",
    "    df_plot = data_stats_org[data_set].iloc[:-1].apply(\n",
    "        lambda r: pd.Series({\"Attack-ratio\": r[\"Attacked\"] / r[\"Total pairs nu\"],\n",
    "                             \"Attack-ratio (exluding arguments only attacked/supported once)\": np.nan if r[\"Total pairs nu\"] == 1 else r[\"Attacked\"] / r[\"Total pairs nu\"]}),\n",
    "        axis=1)\n",
    "    df_plot.hist(density=False, ax=ax)\n",
    "    ax[0].set_ylabel(data_set, rotation=0)\n",
    "    \n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lime + Scattertexts\n",
    "- LIME visualization of two example sentences, model trained using **only_response** (rest default options)\n",
    "    - Details about LIME [here](https://github.com/marcotcr/lime)\n",
    "    - To create LIME visualizations add the `--do_visualization`-flag to `run_classifier_ba.py`, it will then create a file called `lime.html` in the `output_dir` specified.\n",
    "- The scattertexts are produced by running `python datasets_plots.py`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lime Visualization\n",
    "# Some of the words play an influence as expected, e.g. are and not (attack), play, and alcohol (support)\n",
    "# Others do not play the expected influence, e.g. china (attack and not support as expected)\n",
    "# Overall, all weights are really small and the removal/replacement with UNK of a single word \n",
    "# does not change the prediction\n",
    "HTML(filename='./LIME/lime.html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lime\n",
    "# All words have a very small impact\n",
    "HTML(filename='./LIME/lime_pol.html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scattertext of the responses in debate_train\n",
    "# No special \"attacking\" or \"supporting\" words easily recognizable\n",
    "# The words are either topic specific, e.g. China (in topic Chinaonechildpolicy there are more supports than attacks)\n",
    "# Or they seem to be there by chance (small dataset), e.g. he, does\n",
    "if os.path.isdir(\"../data/plots\"):\n",
    "    display(IFrame(src='../data/plots/scattertext_attack_supportdebate_train.html', width=950, height=500))\n",
    "else:\n",
    "    print(\"Run cd ../data; python datatsets_plots.py\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scattertext\n",
    "# Scattertext of the authors in political\n",
    "# The word usage of Nixon and Kennedy is quite different\n",
    "if os.path.isdir(\"../data/plots\"):\n",
    "    display(IFrame(src='../data/plots/scattertext_nixon_kennedy.html', width=950, height=500))\n",
    "else:\n",
    "     print(\"Run cd ../data; python datatsets_plots.py\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot distributions of accuracy of all runs (30) for one setting\n",
    "\n",
    "if os.path.isdir(\"../pytorch/res/node_both_procon\"):\n",
    "    print(\"Accuracy for 30 runs each of the 8 settings for NoDE+procon:\")\n",
    "    fig, ax = plt.subplots(4,2, figsize=(10,10), sharey=True, sharex=True)  # 4 rows, 2 columns\n",
    "    eval_results = pd.read_csv('../pytorch/res/node_both_procon/eval_results.tsv', sep='\\t')\n",
    "    eval_results_grouped = eval_results.groupby(['_bert-model', '_num_epochs', '_batch_size','_gradient_acc' ,'_learning_rate' ])\n",
    "    for i, (name, group) in enumerate(eval_results_grouped):\n",
    "        group.hist(column='acc', ax=ax[i//2, i%2])\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"You have to first reproduce the results for the NoDE procon dataset.\\n\"\n",
    "          \"../code_relation_prediction/pytorch ./run_all_node.sh procon\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NoDE Predictions with respect to the original arguments\n",
    "# Does one original argument always get the same label or does it depend on the paired response argument?\n",
    "\n",
    "# Load the data\n",
    "if os.path.isdir(\"../pytorch/res/node_both_paper/\"):\n",
    "    node_pro = processors['node']('both')\n",
    "    _, node_test_df = node_pro.get_dev_examples('../data')\n",
    "    eval_preds = pd.read_csv('../pytorch/res/node_both_paper/eval_preds.csv')\n",
    "    results = eval_results_grouped['acc'].agg([np.mean, np.min, np.max, np.std, np.median])\n",
    "    bmodel, bepochs, bb, bga, blr = results.loc[results['mean'].idxmax()].name\n",
    "    best_pred_ps = eval_results.loc[(eval_results['_bert-model'] == bmodel) & \n",
    "                           (eval_results['_num_epochs'] == bepochs) & (eval_results['_batch_size'] == bb) &\n",
    "                           (eval_results['_learning_rate'] == blr)].index\n",
    "    # Only predictions from best setting \n",
    "    res = pd.concat([node_test_df.reset_index(drop=True), eval_preds.iloc[best_pred_ps,:-1].transpose().reset_index(drop=True)], axis=1)\n",
    "    res['Mean prediction'] = res[list(best_pred_ps)].mean(axis=1).round().values\n",
    "    res = res.replace({0: 'attack', 1: 'support'})\n",
    "    res = res.rename(columns={'label': 'Label'})\n",
    "    preds_orgs = pd.crosstab(res['org'], [res['Mean prediction'],res['Label']])\n",
    "    preds_orgs['total'] = preds_orgs.agg([np.sum], axis=1)\n",
    "    # Original arguments with many responses \n",
    "    print(\"Original arguments with many answers (Only for the second argument two different classes are predicted)\")\n",
    "    display(preds_orgs.loc[preds_orgs['total'] > 1].sort_values(by='total', ascending=False).head())\n",
    "else:\n",
    "    print(\"You have to first reproduce the results for the NoDE dataset.\\n\"\n",
    "          \"../code_relation_prediction/pytorch ./run_all_node.sh comp\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Political Task 2 (Attack/Support) Predictions with respect to the original arguments and response arguments\n",
    "# Does one original argument always get the same label or does it depend on the paired response argument?\n",
    "# Does one response argument always get the same label or does it depend on the paired original argument?\n",
    "\n",
    "# Load the data\n",
    "if os.path.isdir(\"../pytorch/res/pol_as/\"):\n",
    "    pol_pro = processors['political-as']('both')\n",
    "    pol_test_df = pd.concat(np.array(pol_pro.get_splits('../data'))[:,3])\n",
    "    eval_preds = pd.read_csv('../pytorch/res/pol_as/eval_preds.csv')\n",
    "    preds = pd.Series()\n",
    "    for i, row in eval_preds.iterrows():\n",
    "        preds = preds.append(pd.Series(row.values[~row.str.contains('bert*', na=False, regex=True)]))\n",
    "        if i == 9:\n",
    "            break\n",
    "    preds = preds.dropna().astype(int)\n",
    "    pol_test_df['preds'] = preds.values\n",
    "    pol_test_df = pol_test_df.replace({0: 'attack', 1: 'support'})\n",
    "    # Original arguments with many responses \n",
    "    preds_orgs = pd.crosstab(pol_test_df['org'], [pol_test_df['preds'], pol_test_df['label']])\n",
    "    preds_orgs['total'] = preds_orgs.agg([np.sum], axis=1)\n",
    "    print(\"Original arguments with many answers (Only for the thirds and fifth argument two different classes\" \n",
    "          \"are predicted) \\n But the \")\n",
    "    display(preds_orgs.loc[preds_orgs['total'] > 1].sort_values(by='total', ascending=False).head())\n",
    "    # Original arguments with many responses \n",
    "    preds_resp = pd.crosstab(pol_test_df['response'], [pol_test_df['preds'], pol_test_df['label']])\n",
    "    preds_resp['total'] = preds_resp.agg([np.sum], axis=1)\n",
    "    print(\"Response arguments with many originals (Only for the third argument two different classes are predicted)\")\n",
    "    display(preds_resp.loc[preds_resp['total'] > 1].sort_values(by='total', ascending=False).head())\n",
    "else:\n",
    "    print('You have to first reproduce the results for the Polical dataset Task 2.\\n'\n",
    "          'python run_classifier_ba.py  --task_name \"political-as\" --output_dir res/pol_as/crossval1 --do_cross_val --do_lower_case --num_train_epochs 5 --max_seq_length 256 --train_batch_size 12 --learning_rate 2e-5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Agreement Sentiment Baselines\n",
    "# (About as good as a random guess)\n",
    "\n",
    "agreement_pro = processors['agreement']('both')\n",
    "splits_data = np.array(agreement_pro.get_splits('../data'))\n",
    "ag_test_df = pd.concat(splits_data[:,3])\n",
    "ag_test_df['org_polarity'] = ag_test_df['org'].apply(lambda r: disc_pol(sid.polarity_scores(r)['compound']))\n",
    "ag_test_df['resp_polarity'] = ag_test_df['response'].apply(lambda r: disc_pol(sid.polarity_scores(r)['compound']))\n",
    "ag_test_df['sent_both_baseline'] = ag_test_df.apply(lambda r: 'disagreement' if r['org_polarity'] != r['resp_polarity'] else 'agreement', axis=1)\n",
    "ag_test_df['sent_resp_baseline'] = ag_test_df.apply(lambda r: 'disagreement' if r['resp_polarity'] == 'negative' else 'agreement', axis=1)\n",
    "\n",
    "print(\"Agreement Sentiment 1 baseline:\\n\")\n",
    "print(classification_report(ag_test_df['label'], ag_test_df['sent_both_baseline']))\n",
    "print(\"\\nAgreement Sentiment 2 baseline:\\n\")\n",
    "print(classification_report(ag_test_df['label'], ag_test_df['sent_resp_baseline']))"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
